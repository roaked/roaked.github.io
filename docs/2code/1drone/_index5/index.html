<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Computer Vision for Obstacle Detection # 1 Description # In preparation for the Red Bull drone race, a comprehensive dataset was compiled, consisting of images capturing gates from diverse angles, distances, and lighting conditions. These images showcase the gates amidst various backgrounds, providing a range of scenarios that drones might encounter during the race. The intention behind gathering such a varied dataset is to enable the development of an algorithm capable of accurately identifying these gates despite changes in perspective, distance, lighting, and background colors, facilitating precise navigation for the drones during the high-paced race."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:title" content="Drone Computer Vision"><meta property="og:description" content="Computer Vision for Obstacle Detection # 1 Description # In preparation for the Red Bull drone race, a comprehensive dataset was compiled, consisting of images capturing gates from diverse angles, distances, and lighting conditions. These images showcase the gates amidst various backgrounds, providing a range of scenarios that drones might encounter during the race. The intention behind gathering such a varied dataset is to enable the development of an algorithm capable of accurately identifying these gates despite changes in perspective, distance, lighting, and background colors, facilitating precise navigation for the drones during the high-paced race."><meta property="og:type" content="article"><meta property="og:url" content="https://xsleaks.dev/docs/2code/1drone/_index5/"><meta property="article:section" content="docs"><title>Drone Computer Vision | Ricardo Chin</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png><link rel=stylesheet href=/book.min.33a48f5432973b8ff9a82679d9e45d67f2c15d4399bd2829269455cfe390b5e8.css integrity="sha256-M6SPVDKXO4/5qCZ52eRdZ/LBXUOZvSgpJpRVz+OQteg=" crossorigin=anonymous><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.51ee484d1667fcbe693cf8fc0fd5d3a820a722043da50f3131edefac709ad93f.js integrity="sha256-Ue5ITRZn/L5pPPj8D9XTqCCnIgQ9pQ8xMe3vrHCa2T8=" crossorigin=anonymous></script><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>Ricardo Chin</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li class=book-section-flat><a href=/docs/1design/>Design Portfolio</a><ul><li><a href=/docs/1design/1mest/>Structural Mechanics</a><ul></ul></li><li><a href=/docs/1design/2thermo/>ThermoCup</a><ul></ul></li><li><a href=/docs/1design/3pmec/>Mechanical Design</a><ul></ul></li><li><a href=/docs/1design/4mcomp/>Finite Element Model Formulation</a><ul></ul></li><li><a href=/docs/1design/5om/>Compound Cylindrical Gear Train</a><ul></ul></li></ul></li><li class=book-section-flat><a href=/docs/2code/>Coding Portfolio</a><ul><li><a href=/docs/2code/1drone/>RedBull Racing Competition</a><ul><li><a href=/docs/2code/1drone/_index2/>Drone Dynamics</a></li><li><a href=/docs/2code/1drone/_index3/>Drone Controller Design Pt. 1</a></li><li><a href=/docs/2code/1drone/_index4/>Drone Controller Design Pt. 2</a></li><li><a href=/docs/2code/1drone/_index5/ class=active>Drone Computer Vision</a></li></ul></li><li><a href=/docs/2code/3sint/>Deep Learning on Fake News</a><ul></ul></li><li><a href=/docs/2code/5od/>Evolutionary Computation</a><ul><li><a href=/docs/2code/5od/_index2/>Genetic Algorithm</a></li><li><a href=/docs/2code/5od/_index3/>Particle Swarm Optimization</a></li></ul></li><li><a href=/docs/2code/6auto/>Industrial Automation</a><ul></ul></li><li><a href=/docs/2code/7rman/>Manipulator Robotics: JACO</a><ul></ul></li></ul></li><li class=book-section-flat><a href=/docs/3research/>Research</a><ul><li><a href=/docs/3research/1ebeam/>Electron Beam Welding</a><ul></ul></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Drone Computer Vision</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#21-colorspace-region-segmentation>2.1 Colorspace Region Segmentation</a></li><li><a href=#22-black-and-white-segmentation>2.2 Black and White Segmentation</a></li><li><a href=#23-morphological-operations>2.3 Morphological Operations</a></li><li><a href=#24-edge-detection>2.4 Edge Detection</a></li><li><a href=#25-hough-transform>2.5 Hough Transform</a></li><li><a href=#26-extreme-points-and-centroid>2.6 Extreme Points and Centroid</a></li><li><a href=#27-image-enhancement>2.7 Image Enhancement</a></li></ul></nav></aside></header><article class=markdown><h1 id=computer-vision-for-obstacle-detection><strong>Computer Vision for Obstacle Detection</strong>
<a class=anchor href=#computer-vision-for-obstacle-detection>#</a></h1><h1 id=1-description>1 Description
<a class=anchor href=#1-description>#</a></h1><p>In preparation for the Red Bull drone race, a comprehensive dataset was compiled, consisting of images capturing gates from diverse angles, distances, and lighting conditions. These images showcase the gates amidst various backgrounds, providing a range of scenarios that drones might encounter during the race. The intention behind gathering such a varied dataset is to enable the development of an algorithm capable of accurately identifying these gates despite changes in perspective, distance, lighting, and background colors, facilitating precise navigation for the drones during the high-paced race.</p><h1 id=2-gate-characterization>2 Gate Characterization
<a class=anchor href=#2-gate-characterization>#</a></h1><p>The analysis of the images reveals that the gates used for detection exhibit a shallow square shape, predominantly appearing in shades of blue along their sides and darker tones overall. Each gate features white letters and small squares within them, affixed to a metal structure supported by two ground-based supports. While the gates primarily share a base color, variations in lighting conditions result in perceptible differences. For instance, exposure to red light causes the white small squares to take on a red hue, as follows:</p><p><img src=https://live.staticflickr.com/65535/53348683291_d2191e7265.jpg alt=nodafafnlfasr></p><h2 id=21-colorspace-region-segmentation>2.1 Colorspace Region Segmentation
<a class=anchor href=#21-colorspace-region-segmentation>#</a></h2><p>The initial phase involves image segmentation, commencing with a test set of images showcase below. These specific images were chosen due to their similarity in appearance, presenting a frontal view to the camera with minimal distortion and consistent colors unaffected by varying lighting conditions. This selection aims to establish a baseline for segmentation, allowing for a focused and controlled analysis of gate recognition and isolation within these images.</p><p><img src=https://live.staticflickr.com/65535/53348683301_ccf7290b14.jpg alt=ntetete></p><p>The strategy involved an examination of different color spaces to identify potential relationships between them. It was done by observing the RGB color space aiming to discern any possible connections or patterns among the color channels within the images.</p><p><img src=https://live.staticflickr.com/65535/53347810927_ffbb57983a_z.jpg alt=ntetesaddate></p><p>In the image, it is seen the original RGB components as they are, without any changes. It&rsquo;s hard to spot the gate distinctly from the background in any of these components. None of them make the gate stand out separately. So, it looks like using these RGB components isn&rsquo;t the best for separating the gate from the rest. Next up was trying out the HSV color representation.</p><p><img src=https://live.staticflickr.com/65535/53349006219_f418323e63.jpg alt=ntetsdsdaete></p><p>In the HSV representation, an adjustment was made to the original images specifically for the Hue component. The process involved converting the image to the HSV color space, modifying its Saturation and Value components to a saturation of 1, and then re-converting the image back to the RGB color space. This resulted in a representation highlighting the genuine colors of the image, with only the Hue component &ldquo;enabled&rdquo;.</p><p>When the image is observed, the gate is distinctly noticeable from the background in the Hue component. The dark and blue tones from the original picture are reflected as dark blue in the Hue representation, while the small white squares take on a magenta hue. The background spans a range from light blue to green and red in this representation. While the Saturation and Value components alone do not facilitate gate identification.</p><p><img src=https://live.staticflickr.com/65535/53348700101_af30a3ae2e_c.jpg alt=ntetsdsdaeasate></p><p><img src=https://live.staticflickr.com/65535/53349023269_7ae02dbe61_c.jpg alt=ntetsdsssssdaeasate></p><p>The Lab and YCbCr representations didn&rsquo;t yield satisfactory results as the distinction between the background and the gate was hardly noticeable. Consequently, the HSV color space emerged as the most effective choice for segmenting the gate in the images.</p><p>To execute the segmentation, the images from the test dataset were examined in their HSV representations. Through visual inspection and some trial and error, specific conditions (thresholds) for the HSV values were established to pinpoint the gate&rsquo;s location.</p><p>This process generated a black and white image where all the points meeting the conditions were highlighted. However, restricting the HSV values alone proved insufficient. Some parts of the background shared similar HSV values with the gate, leading to noise in the resulting images.</p><h2 id=22-black-and-white-segmentation>2.2 Black and White Segmentation
<a class=anchor href=#22-black-and-white-segmentation>#</a></h2><p>Applying thresholds to the HSV values, a first approximation of the gate can be found.</p><p><img src=https://live.staticflickr.com/65535/53349023254_37cce63789.jpg alt=ntetsdss3223sssdaeasate></p><p>To refine the image, several morphological operations need to be applied. In exploring various techniques, one solution considered was to initiate the noise reduction process by cropping the initial image. Utilizing a cropping function facilitated the selection of a specific region of interest (ROI). An illustration of this method is depicted in the subsequent figure.</p><p><img src=https://live.staticflickr.com/65535/53349023284_ae24b7b46a.jpg alt=ntetsdss3223sss2ssdaeasate></p><p>While the outcomes appeared acceptable for the selected images in the testing set where the gate was in proximity to the camera, this method was ultimately dismissed. Its limitation became apparent with images where the gate was situated farther away, rendering the approach less effective. Moreover, in real-life scenarios involving drones that sway considerably from side to side, cropping the image and narrowing the field of vision was deemed risky. This approach could potentially result in the drone losing track of the gate entirely, which is an undesirable outcome.</p><h2 id=23-morphological-operations>2.3 Morphological Operations
<a class=anchor href=#23-morphological-operations>#</a></h2><p>To isolate the gate and eliminate unwanted noise, a shift in approach was necessary, focusing on morphological operations. For the test image set, various scenarios were taken into account based on the image conditions. In the general case, the following morphological operations were applied:</p><ul><li><strong>1.</strong> Small dilation operation: Utilizing the <a href=https://www.mathworks.com/help/images/ref/imdilate.html>MATLAB function <em>&lsquo;imdilate&rsquo;</em></a>, this operation aimed to connect isolated pixels located at the corners of the gate with the rest of the gate. The &lsquo;cube&rsquo; structure with a magnitude of 2 was employed for this purpose.</li></ul><p><img src=https://live.staticflickr.com/65535/53349067404_f2a8d9eded.jpg alt=ntetsdss3223sssss2ssdaeasate></p><ul><li><strong>2.</strong> Retaining the largest area: This step involved employing the <a href=https://www.mathworks.com/help/images/ref/bwareafilt.html>MATLAB function <em>&lsquo;bwareafilt&rsquo;</em></a> to retain solely the gate in the black and white image. Initially, all connected components within a specified range were extracted, followed by preserving solely the largest area. This process became necessary due to potential noise accumulation and formation of objects in the image caused by the dilation process in certain cases.</li></ul><p><img src=https://live.staticflickr.com/65535/53348976298_3cd07df4ea.jpg alt=ntetsdss3223sssss2ssdaeas22ate></p><ul><li><strong>3.</strong> Closing small holes: The <a href=https://www.mathworks.com/help/images/ref/imclose.html>MATLAB function <em>&lsquo;imclose&rsquo;</em></a> function was employed to close only the small holes that form part of the gate shape. This operation aimed to fill these small gaps within the gate structure. A closing operation with a magnitude of 80 was applied using the &lsquo;cube&rsquo; structure to ensure closure when the gate shape wasn&rsquo;t entirely closed (e.g., when the rectangle wasn&rsquo;t fully closed). It&rsquo;s worth noting that the <a href=https://www.mathworks.com/help/images/ref/imfill.html>MATLAB function <em>&lsquo;imfill&rsquo;</em></a> function was avoided to prevent inadvertently filling the actual opening of the gate where drones navigate through.</li></ul><p><img src=https://live.staticflickr.com/65535/53349194590_8878fdac83.jpg alt=ntetsdss3223sssss2ssdae11as22ate></p><ul><li><strong>4.</strong> Smoothing the region: To further refine the image and eliminate residual noise, an <a href=https://www.mathworks.com/help/images/ref/imopen.html>MATLAB function <em>&lsquo;imopen&rsquo;</em></a> operation was executed using the &lsquo;cube&rsquo; structure with a magnitude of 20. This process aids in cleaning up any remaining artifacts or noise in the image. Notably, employing a higher magnitude might risk reopening the gate&rsquo;s rectangular structure itself, hence the careful choice of parameters for this operation.</li></ul><p><img src=https://live.staticflickr.com/65535/53348744306_80780ae17a.jpg alt=ntetsdss3223ssss221s2ssdae11as22ate></p><ul><li><strong>5.</strong> Reapplying the mask derived from the obtained black and white image: This was achieved by performing a boolean logic multiplication between the black and white image (acting as a mask) and the original image. The result was a segmented image where the gate was isolated and distinctly visible.</li></ul><p><img src=https://live.staticflickr.com/65535/53348744311_051decab50.jpg alt=nte11tsdss3223sss2ssdaeasate></p><p>Another method explored was the superpixels technique which involves segmenting the image. It is a relatively state-of-the-art technique that often couples numerous superpixels with clustering to enhance image recognition.<a href=https://cran.r-project.org/web/packages/OpenImageR/vignettes/Image_segmentation_superpixels_clustering.html>There are recent papers being published, but a particularly interesting one can be seen here.</a>. The divided image contain many superpixels and each superpixel is then represented by the average color value within its region. The intention behind this approach was to implement the superpixels segmentation preceding the HSV segmentation and assess whether this sequential method could yield improved results. The goal was to evaluate if the combination of superpixels segmentation followed by HSV segmentation could enhance the accuracy of isolating the gate in the images.</p><p><img src=https://live.staticflickr.com/65535/53349194580_d144f5c7a4.jpg alt=ntetsdss3223sss2ssdaeasate111></p><p>This method helps reducing the number of different colors in the image which could help the segmentation process in some cases.</p><p><img src=https://live.staticflickr.com/65535/53349067274_d76b061191.jpg alt=ntetsdss3223sss2ss52daeasate111></p><p><img src=https://live.staticflickr.com/65535/53347872712_3deecf9434.jpg alt=ntetsdss3223sss2ss52d4121aeasate111></p><p>This method was found to be functional but lacked reliability for consistent segmentation. A lower count of superpixels resulted in poorer outcomes, while a higher count led to slower segmentation without notable improvement. Moreover, in some instances, an increased count made segmentation more challenging as the gate and background merged together.</p><p>The segmentation algorithm underwent testing on various images. It worked well for images with similar background colors and lighting conditions, often necessitating minor adjustments to the thresholds for optimization. However, for other images, a new set of thresholds had to be determined.</p><p>To facilitate threshold selection, two new parameters were introduced to group images with similar colors. One parameter, denoted as &lsquo;c&rsquo;, signifies the center of mass of the histogram obtained using the <a href=https://de.mathworks.com/help/images/ref/imhist.html>MATLAB <em>&lsquo;imhist&rsquo;</em> function</a>. This parameter helped isolate images with pink or red backgrounds, requiring unique thresholds due to their distinct characteristics. Additionally, two images featuring a large red and black banner in the background were also isolated as they demanded slight threshold adjustments.</p><p>The second parameter was established to represent the x-coordinate with the maximum value in the histogram obtained from <a href=https://de.mathworks.com/help/images/ref/imhist.html>MATLAB <em>&lsquo;imhist&rsquo;</em> function</a>. This parameter primarily aided in segregating an image with a blue background. While this image had a &lsquo;c&rsquo; parameter similar to others in the testing set, it necessitated different thresholds owing to the blending of the blue background with the gate&rsquo;s blue.</p><p>Images with distinct thresholds also required minor modifications to the order of morphological operations.</p><p><img src=https://live.staticflickr.com/65535/53349078904_b07f388b95_c.jpg alt=tes24141241t></p><p><strong>Segmented</strong></p><p><img src=https://live.staticflickr.com/65535/53349206255_be9c9f9446_c.jpg alt=tes123142t></p><h2 id=24-edge-detection>2.4 Edge Detection
<a class=anchor href=#24-edge-detection>#</a></h2><p>The next phase involved edge detection to outline the gate&rsquo;s boundaries using various methods of the <a href=https://de.mathworks.com/help/images/ref/edge.html>MATLAB function <em>&rsquo;edge&rsquo;</em></a>. The previously obtained segmented image was utilized due to its superior performance in noise removal and clear delineation of the gate.</p><p>Three distinct methods were employed:</p><ul><li><strong>Canny Method</strong>: This technique identifies edges by seeking local maximums of the gradient of the image. The gradient is derived from a Gaussian filter. It uses two thresholds—one for strong edges and another for weak edges. Weak edges are included in the output only if they are connected to strong edges.</li></ul><p><img src=https://live.staticflickr.com/65535/53349009603_1155d7734b_c.jpg alt=ntetsdss322214143sss2ss52d4121aeasate111></p><ul><li><strong>Sobel Operator</strong>: This method convolves the image with a small, separable, and integer-valued filter along the horizontal and vertical directions. While computationally inexpensive, its gradient approximation can be relatively coarse, especially for high-frequency variations in the image.</li></ul><p><img src=https://live.staticflickr.com/65535/53348778536_65dbe9efb7_b.jpg alt=ntetsdss322214143sss2ss52412d4121aeasate111></p><ul><li><strong>Laplacian Filters</strong>: These filters are derivative-based and designed to detect areas of rapid change (edges) in images. As derivative filters are sensitive to noise, it&rsquo;s common practice to smooth the image (e.g., using a Gaussian filter) before applying the Laplacian. This combined process is known as the Laplacian of Gaussian (LoG) operation:</li></ul><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\[L(x, y) = \nabla^2 f(x, y) = \frac{\partial^2 f(x, y)}{\partial x^2} + \frac{\partial^2 f(x, y)}{\partial y^2}\]</span><p>The representation of the 2D LoG (Laplacian of Gaussian) function centered on zero and with a Gaussian standard deviation <span>\(\sigma\)
</span>is given by:</p><span>\[\text{LoG}(x, y) = -\frac{1}{\pi \sigma^4} \left(1 - \frac{x^2 + y^2}{2\sigma^2}\right) e^{-\frac{x^2 + y^2}{2\sigma^2}}\]</span><p><img src=https://live.staticflickr.com/65535/53349228340_7bfd0cb588_c.jpg alt=ntetsdss322214143sss142142ss52412d4121aeasate111></p><p>Edge detection was carried out on two types of images: the BW mask acquired during the segmentation process and the segmented images. Conducting edge detection on the original images was dismissed due to the excessive background information it would provide.</p><p>When performing edge detection on the BW mask, the expected outcome was solely the contours of the mask, encompassing both inner and outer rectangles. Conversely, edge detection on the segmented image aimed to extract additional features such as letters and squares, with the option to adjust the number of extracted features using a threshold.</p><p>The results displayed two images: edge detection on the BW mask was shown on the left, while edge detection on the segmented image was depicted on the right. The superiority of the Canny method&rsquo;s results was attributed mainly to the adjustability of its two thresholds.</p><h2 id=25-hough-transform>2.5 Hough Transform
<a class=anchor href=#25-hough-transform>#</a></h2><p>The Hough Transform involves identifying lines within an image by grouping edge points into object candidates, which undergo a voting procedure. This process occurs in a parameter space where the object candidates are considered local maxima. Each point in the Hough Space is represented by two parameters, Rho (<span>
\(\rho\)
</span>) and Theta (<span>
\(\theta\)
</span>). Rho represents the perpendicular distance from the origin to the line, while theta denotes the angle between the x-axis and this vector.</p><p>The MATLAB implementation comprises a series of functions. The <a href=https://de.mathworks.com/help/images/ref/hough.html>MATLAB function <em>&lsquo;hough&rsquo;</em></a> computes the Standard Hough Transform (SHT) of a binary image, producing outputs of rho, theta, and H, a matrix representing the parameter space. Subsequently, the <a href=https://de.mathworks.com/help/images/ref/houghpeaks.html>MATLAB function <em>&lsquo;houghpeaks&rsquo;</em></a> locates peaks in the Hough Transform Matrix, typically selecting 20 peaks. Finally, the <a href=https://de.mathworks.com/help/images/ref/houghlines.html>MATLAB function <em>&lsquo;houghlines&rsquo;</em></a> function identifies line segments within the image, returning a structure array that represents the merged line segments.</p><p>Hough lines serve as an alternative method for line detection. In this case, the Hough lines were applied to the BW mask generated from the Canny edges, as the Canny method displayed the most promising results. This application aimed to enhance the lines extracted from the edges, particularly emphasizing the extremes of the gate. The results are depicted below.</p><p><img src=https://live.staticflickr.com/65535/53349037313_05364df81a_c.jpg alt=hough></p><h2 id=26-extreme-points-and-centroid>2.6 Extreme Points and Centroid
<a class=anchor href=#26-extreme-points-and-centroid>#</a></h2><p>The <a href=https://github.com/roaked/redbull-drone-computer-vision/blob/main/CornerDetec.m>function <em>&lsquo;CornerDetec&rsquo;</em></a> was created to calculate the extreme points of the gate image post edge detection. As the gate consistently maintains a square shape, the extreme points can be defined by a sequence of maximum and minimum x and y coordinates. For instance, the top-right corner would be the point where the sum of the x and y coordinates is maximized.</p><p>Determining the centroid involved using the <a href=https://www.mathworks.com/help/images/ref/regionprops.html>MATLAB function <em>&lsquo;regionprops&rsquo;</em></a>. Given the gate&rsquo;s square and axisymmetric shape, the centroid could be extracted using either the inner or outer square of the gate.</p><p>Despite prior morphological operations aimed at refining the segmentation quality, residual noise persisted within the images. To reduce the error between the actual centroid and the estimated centroid, the inner square of the gate, less susceptible to edge detection errors, was utilized. In cases where the inner square wasn&rsquo;t entirely identified, the centroid of the outer square was plotted instead.</p><p><img src=https://live.staticflickr.com/65535/53349255010_8a1bb87556.jpg alt=linear213312></p><details><summary><strong>Results:</strong> Identification of centroid on a different image - (click to expand)</summary><div class=markdown-inner><img src=https://live.staticflickr.com/65535/53349255005_a19d3154d9_c.jpg alt=li2434near></div></details><details><summary><strong>Results:</strong> Identification of centroid on all images - (click to expand)</summary><div class=markdown-inner><img src=https://live.staticflickr.com/65535/53348804846_3f38a86795_c.jpg alt=li2434nea41421r></div></details><details><summary><strong>Results:</strong> Identification of hough lines on all images - (click to expand)</summary><div class=markdown-inner><img src=https://live.staticflickr.com/65535/53349254980_7e1375c0ff_c.jpg alt=li2434n123ea41421r></div></details><h2 id=27-image-enhancement>2.7 Image Enhancement
<a class=anchor href=#27-image-enhancement>#</a></h2><p>To accentuate the gate within the image, the region corresponding to the gate was emphasized. This was achieved by inverting the mask obtained from the segmentation process. Inverting the mask allowed for the removal of the gate from the original image and isolating it for highlighting purposes. To accomplish this, each color channel of the inverted mask was isolated and subsequently combined to create a true color RGB image. Additionally, the lines detected from the extreme points were plotted on this highlighted gate region to further outline its boundaries.</p><p><img src=https://live.staticflickr.com/65535/53347932412_12209e04fd_z.jpg alt=imageenhance></p><p>The conversion from RGB to HSV can be applied to this image. It&rsquo;s important to note that the green lines visible in the figure are not inherent to the image; they are plotted lines and, consequently, do not manifest in the HSV representation.</p><p><img src=https://live.staticflickr.com/65535/53347932407_d2b4da0dd6_c.jpg alt=imageenh42141ance></p><p>Upon observing the HSV components, it becomes apparent that the gate is distinctly highlighted in the Saturation and Value images. For all the image datasets:</p><p><img src=https://live.staticflickr.com/65535/53347932402_ef0bf9b1d0_z.jpg alt=imageenh42141ance></p><h1 id=3-section-end>3 Section End
<a class=anchor href=#3-section-end>#</a></h1><p>It became evident that arriving at a solution wasn&rsquo;t straightforward, and while the obtained solution wasn&rsquo;t perfect, it showcased potential for optimization with an expanded dataset. Additional images would enhance the algorithm&rsquo;s ability to recognize the gate under various lighting conditions and backgrounds. Nonetheless, color segmentation emerged as an effective means to segment the gate.</p><p>Among edge detection methods, the Canny approach stood out as the most effective, although other methods also demonstrated reasonable performance. The Hough transform offered an alternative method to identify lines, proving to be a decent performer.</p><p>Despite minor imperfections resulting from imperfect segmentation, the detection of extreme points and centroids was generally successful.</p><p>Ultimately, in this section the work done successfully achieved its goal by effectively isolating and enhancing the gate. Employing various solutions contributed to solving the challenge of gate identification for drone competitions.</p><h1 id=4-next-steps>4 Next Steps
<a class=anchor href=#4-next-steps>#</a></h1><p>The next step could involve exploring the integration of machine learning techniques to further enhance the gate identification process, potentially leading to real-time detection capabilities. Machine learning models, such as convolutional neural networks (CNNs) or other deep learning architectures, could be trained on an expanded dataset to improve gate recognition across diverse conditions. This adaptation could pave the way for more robust and efficient gate identification systems, especially in real-time scenarios during drone competitions.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/roaked/roaked.github.io/edit/main/content/content/docs/2code/1drone/_index5.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#21-colorspace-region-segmentation>2.1 Colorspace Region Segmentation</a></li><li><a href=#22-black-and-white-segmentation>2.2 Black and White Segmentation</a></li><li><a href=#23-morphological-operations>2.3 Morphological Operations</a></li><li><a href=#24-edge-detection>2.4 Edge Detection</a></li><li><a href=#25-hough-transform>2.5 Hough Transform</a></li><li><a href=#26-extreme-points-and-centroid>2.6 Extreme Points and Centroid</a></li><li><a href=#27-image-enhancement>2.7 Image Enhancement</a></li></ul></nav></div></aside></main></body></html>