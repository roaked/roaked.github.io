<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Notes on Bayesian Optimization
  #


  
    
      
        
      tip
    
git clone -> cd
pip install -r requirements.txt (jax.numpy, scipy, botorch, optuna)


  


  Optimization Under Expensive Evaluations
  #


Bayesian Optimization (BO) solves black-box optimization problems where each evaluation is expensive:





  \[x^\star = \arg \max_{x \in \mathcal{X}} f(x)\]




assumptions:

gradients are unavailable or too noisy
objective evaluations are costly (training loops, simulations, experiments)
query budget is limited, so every new point matters



BO minimizes regret by combining:"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://xsleaks.dev/docs/lectures/bayesian-optimization/bayesopt/"><meta property="og:site_name" content="Ricardo Chin"><meta property="og:title" content="Bayesian Optimization"><meta property="og:description" content="Notes on Bayesian Optimization # tip
git clone -> cd pip install -r requirements.txt (jax.numpy, scipy, botorch, optuna) Optimization Under Expensive Evaluations # Bayesian Optimization (BO) solves black-box optimization problems where each evaluation is expensive: \[x^\star = \arg \max_{x \in \mathcal{X}} f(x)\] assumptions:
gradients are unavailable or too noisy objective evaluations are costly (training loops, simulations, experiments) query budget is limited, so every new point matters BO minimizes regret by combining:"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2026-02-26T15:40:16+01:00"><title>Bayesian Optimization | Ricardo Chin</title><link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png><link rel=stylesheet href=/book.min.b1824eea1873de6fd466b2e1d8cc26a9f8e6e7aaf317af5779752ac8f255d4c4.css integrity="sha256-sYJO6hhz3m/UZrLh2Mwmqfjm56rzF69XeXUqyPJV1MQ=" crossorigin=anonymous><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>Ricardo Chin</span></a></h2><ul><li class=book-section-flat><a href=/docs/design/>Engineering Repository</a><ul><li><a href=/docs/design/finite-element-method-development/>FEM Package Development</a><ul></ul></li></ul></li><li class=book-section-flat><a href=/docs/code/>System Repository</a><ul><li><a href=/docs/code/agv/>AGV: Stochastic Identification</a><ul></ul></li><li><a href=/docs/code/uav/>UAV: Red Bull Air Racing</a><ul><li><a href=/docs/code/uav/dynamics/>Drone System Dynamics</a></li><li><a href=/docs/code/uav/continuous-controller-design/>Continuous Controller</a></li><li><a href=/docs/code/uav/discrete-controller-design/>Discrete Controller</a></li><li><a href=/docs/code/uav/computer-vision/>Gate Sense: Computer Vision</a></li></ul></li><li><a href=/docs/code/robotics/>Robotics: Kin, Dynamics & Control</a><ul></ul></li><li><a href=/docs/code/micromouse/>Micromouse: Flood Fill and A* (2D)</a><ul></ul></li><li><a href=/docs/code/bin-packing/>EC Optimization: Space & Time</a><ul><li><a href=/docs/code/bin-packing/genetic-algorithm/>Genetic Algorithm</a></li><li><a href=/docs/code/bin-packing/particle-swarm-optimization/>Particle Swarm Optimization</a></li></ul></li><li><a href=/docs/code/snake-game/>Snake: Reinforcement Learning</a><ul><li><a href=/docs/code/snake-game/deepqnetwork/>BaseRL, GA-RL & Hamiltonian</a></li><li><a href=/docs/code/snake-game/adversarial/>Adversarial Multi-Agent RL, BattleMode</a></li></ul></li><li><a href=/docs/code/deep-learning-fake-news/>Fake News: Inference & Clusters</a><ul></ul></li></ul></li><li class=book-section-flat><a href=/docs/lectures/>My Notes and Lectures</a><ul><li><a href=/docs/lectures/bayesian-machine-learning/bayes/>Bayesian Inference</a></li><li><a href=/docs/lectures/bayesian-optimization/bayesopt/ class=active>Bayesian Optimization</a></li><li><a href=/docs/lectures/nonlinear-programming/nonlinear/>Nonlinear Programming</a></li><li><a href=/docs/lectures/hamiltonian-graphs/hamiltonian/>Hamiltonian Graphs & Linked Lists</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Bayesian Optimization</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><a href=#optimization-under-expensive-evaluations><strong>Optimization Under Expensive Evaluations</strong></a></li><li><a href=#surrogate-modelling-with-gaussian-processes><strong>Surrogate Modelling with Gaussian Processes</strong></a></li><li><a href=#acquisition-functions><strong>Acquisition Functions</strong></a></li><li><a href=#canonical-bo-algorithm><strong>Canonical BO Algorithm</strong></a></li><li><a href=#practical-engineering-notes><strong>Practical Engineering Notes</strong></a></li><li><a href=#how-optuna-framework-works><strong>How Optuna Framework Works</strong></a></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=notes-on-bayesian-optimization><strong>Notes on Bayesian Optimization</strong>
<a class=anchor href=#notes-on-bayesian-optimization>#</a></h1><blockquote class="book-hint2 tip"><p class="hint-title tip"><svg class="book-icon"><use href="/svg/hint-icons.svg#tip-notice"/></svg><span>tip</span></p><ul><li><strong>git clone -> cd</strong></li><li><strong>pip install -r requirements.txt</strong> (jax.numpy, scipy, botorch, optuna)</li></ul></blockquote><h3 id=optimization-under-expensive-evaluations><strong>Optimization Under Expensive Evaluations</strong>
<a class=anchor href=#optimization-under-expensive-evaluations>#</a></h3><ul><li>Bayesian Optimization (BO) solves black-box optimization problems where each evaluation is expensive:</li></ul><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\[x^\star = \arg \max_{x \in \mathcal{X}} f(x)\]</span><ul><li><p>assumptions:</p><ul><li>gradients are unavailable or too noisy</li><li>objective evaluations are costly (training loops, simulations, experiments)</li><li>query budget is limited, so every new point matters</li></ul></li><li><p>BO minimizes regret by combining:</p><ul><li>a probabilistic surrogate of <span>\(f(x)\)</span></li><li>an acquisition strategy that controls exploration vs exploitation</li></ul></li></ul><h3 id=surrogate-modelling-with-gaussian-processes><strong>Surrogate Modelling with Gaussian Processes</strong>
<a class=anchor href=#surrogate-modelling-with-gaussian-processes>#</a></h3><ul><li>BO commonly models the unknown function as:</li></ul><span>\[f(x) \sim \mathcal{GP}(m(x), k(x, x'))\]</span><ul><li>after observing data <span>\(\mathcal{D}_t = \{(x_i, y_i)\}_{i=1}^{t}\)
</span>, prediction at <span>\(x\)
</span>is:</li></ul><span>\[f(x) \mid \mathcal{D}_t \sim \mathcal{N}(\mu_t(x), \sigma_t^2(x))\]</span><ul><li><p>interpretation:</p><ul><li><span>\(\mu_t(x)\)
</span>is the current estimate of performance</li><li><span>\(\sigma_t(x)\)
</span>captures epistemic uncertainty</li></ul></li><li><p>this posterior is the foundation for deciding where to sample next</p></li></ul><h3 id=acquisition-functions><strong>Acquisition Functions</strong>
<a class=anchor href=#acquisition-functions>#</a></h3><ul><li>acquisition functions convert posterior mean/uncertainty into an optimization signal:</li></ul><span>\[x_{t+1} = \arg \max_{x \in \mathcal{X}} \alpha_t(x)\]</span><ul><li>common choices:<ul><li><strong>Upper Confidence Bound (UCB)</strong>
<span>\[\alpha_{\text{UCB}}(x) = \mu_t(x) + \kappa \sigma_t(x)\]</span></li><li><strong>Probability of Improvement (PI)</strong>
<span>\[\alpha_{\text{PI}}(x) = \Phi\left(\frac{\mu_t(x)-f(x^+)-\xi}{\sigma_t(x)}\right)\]</span></li><li><strong>Expected Improvement (EI)</strong>
<span>\[\alpha_{\text{EI}}(x)= (\mu_t(x)-f(x^+)-\xi)\Phi(z)+\sigma_t(x)\phi(z), \quad z=\frac{\mu_t(x)-f(x^+)-\xi}{\sigma_t(x)}\]</span></li></ul></li></ul><blockquote class="book-hint2 important"><p class="hint-title important"><svg class="book-icon"><use href="/svg/hint-icons.svg#important-notice"/></svg><span>important</span></p><ul><li>BO performance depends heavily on sensible kernel choices, robust hyperparameter fitting, and stable optimization of the acquisition objective.</li></ul></blockquote><h3 id=canonical-bo-algorithm><strong>Canonical BO Algorithm</strong>
<a class=anchor href=#canonical-bo-algorithm>#</a></h3><ol><li>define bounded search space <span>\(\mathcal{X}\)</span></li><li>collect initial points (random or Latin hypercube)</li><li>fit/update the GP surrogate with current observations</li><li>maximize acquisition function to propose <span>\(x_{t+1}\)</span></li><li>evaluate expensive objective at <span>\(x_{t+1}\)</span></li><li>append <span>\((x_{t+1}, y_{t+1})\)
</span>and repeat until budget is exhausted</li></ol><ul><li>practical stop rules:<ul><li>fixed evaluation budget</li><li>small improvement over last N steps</li><li>hitting a target metric</li></ul></li></ul><h3 id=practical-engineering-notes><strong>Practical Engineering Notes</strong>
<a class=anchor href=#practical-engineering-notes>#</a></h3><ul><li>normalize inputs to similar scales before fitting the surrogate</li><li>standardize outputs to improve numerical conditioning</li><li>represent known observation noise in the likelihood</li><li>use log-scaled domains for parameters spanning multiple orders of magnitude</li><li>for higher dimensions, combine BO with trust-region methods or structured priors</li></ul><h3 id=how-optuna-framework-works><strong>How Optuna Framework Works</strong>
<a class=anchor href=#how-optuna-framework-works>#</a></h3><ul><li><p>Optuna provides a flexible orchestration layer for hyperparameter optimization, including Bayesian-style samplers and early stopping</p></li><li><p>building blocks:</p><ul><li><strong>Study</strong>: full optimization process and trial history</li><li><strong>Trial</strong>: one objective evaluation with suggested parameters</li><li><strong>Sampler</strong>: chooses parameters (default: TPE; alternatives: random, CMA-ES, NSGA-II)</li><li><strong>Pruner</strong>: stops weak trials early based on intermediate reports</li></ul></li><li><p>internal loop:</p><ol><li>user defines an objective function</li><li><code>study.optimize</code> starts repeated trials</li><li>sampler proposes parameter values (<code>trial.suggest_*</code>)</li><li>objective returns metric (or reports intermediate steps)</li><li>pruner optionally interrupts underperforming trials</li><li>study updates best params and complete trial database</li></ol></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> optuna
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>objective</span>(trial):
</span></span><span style=display:flex><span>    learning_rate <span style=color:#f92672>=</span> trial<span style=color:#f92672>.</span>suggest_float(<span style=color:#e6db74>&#34;learning_rate&#34;</span>, <span style=color:#ae81ff>1e-5</span>, <span style=color:#ae81ff>1e-1</span>, log<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    depth <span style=color:#f92672>=</span> trial<span style=color:#f92672>.</span>suggest_int(<span style=color:#e6db74>&#34;depth&#34;</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>12</span>)
</span></span><span style=display:flex><span>    dropout <span style=color:#f92672>=</span> trial<span style=color:#f92672>.</span>suggest_float(<span style=color:#e6db74>&#34;dropout&#34;</span>, <span style=color:#ae81ff>0.0</span>, <span style=color:#ae81ff>0.5</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    score <span style=color:#f92672>=</span> train_and_validate_model(
</span></span><span style=display:flex><span>        learning_rate<span style=color:#f92672>=</span>learning_rate, depth<span style=color:#f92672>=</span>depth, dropout<span style=color:#f92672>=</span>dropout
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> score  <span style=color:#75715e># maximize here</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>study <span style=color:#f92672>=</span> optuna<span style=color:#f92672>.</span>create_study(direction<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;maximize&#34;</span>)
</span></span><span style=display:flex><span>study<span style=color:#f92672>.</span>optimize(objective, n_trials<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(study<span style=color:#f92672>.</span>best_trial<span style=color:#f92672>.</span>number)
</span></span><span style=display:flex><span>print(study<span style=color:#f92672>.</span>best_params)
</span></span><span style=display:flex><span>print(study<span style=color:#f92672>.</span>best_value)
</span></span></code></pre></div><blockquote class="book-hint2 note"><p class="hint-title note"><svg class="book-icon"><use href="/svg/hint-icons.svg#note-notice"/></svg><span>note</span></p><ul><li>Optuna can persist studies in SQLite/PostgreSQL and run distributed workers, making it practical for real training pipelines.</li></ul></blockquote></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/roaked/roaked.github.io/commit/dc572594d4a993f11697ca0fd71f20c80c600a07 title='Last modified by roaked | February 26, 2026' target=_blank rel=noopener><img src=/svg/calendar.svg class=book-icon alt=Calendar>
<span>February 26, 2026</span></a></div><div><a class="flex align-center" href=https://github.com/roaked/roaked.github.io/edit/main/content/content/docs/lectures/bayesian-optimization/bayesopt.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Ricardo Chin</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#optimization-under-expensive-evaluations><strong>Optimization Under Expensive Evaluations</strong></a></li><li><a href=#surrogate-modelling-with-gaussian-processes><strong>Surrogate Modelling with Gaussian Processes</strong></a></li><li><a href=#acquisition-functions><strong>Acquisition Functions</strong></a></li><li><a href=#canonical-bo-algorithm><strong>Canonical BO Algorithm</strong></a></li><li><a href=#practical-engineering-notes><strong>Practical Engineering Notes</strong></a></li><li><a href=#how-optuna-framework-works><strong>How Optuna Framework Works</strong></a></li></ul></li></ul></nav></div></aside></main></body></html>