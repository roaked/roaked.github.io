<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Notes on Bayesian Inference # tip
git clone -> cd pip install -r requirements.txt (scipy, jax.numpy) Understanding Uncertainty via Probabilities # sum rule: P(X) = P(X,Y) + P(X, \(\neg\) Y) product rule: P(X,Y) = P(X) \(\cdot\) P(Y | X) bayes theorem on data D: \[\underbrace{P(X | D)}_{\text{posterior of X given D}} \hspace{.1cm} = \frac{\overbrace{P(D | X)}^{\text{likelihood of X under D}} \hspace{.1cm} \cdot \hspace{.1cm} \overbrace{P(X)}^{\text{prior of X}}}{\underbrace{P(D)}_{\text{marginalization or evidence of the model}}}\] &ldquo;discrete domain is just a subset of the continuous domain&rdquo; this extends deductive reasoning (statistics) to plausible reasoning (probabilities) Exponential Families and Conjugate Priors # random variable X taking x values \(\subset \R^n\) probability distribution for X with pdf of the following form:"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:title" content="Bayesian Inference"><meta property="og:description" content="Notes on Bayesian Inference # tip
git clone -> cd pip install -r requirements.txt (scipy, jax.numpy) Understanding Uncertainty via Probabilities # sum rule: P(X) = P(X,Y) + P(X, \(\neg\) Y) product rule: P(X,Y) = P(X) \(\cdot\) P(Y | X) bayes theorem on data D: \[\underbrace{P(X | D)}_{\text{posterior of X given D}} \hspace{.1cm} = \frac{\overbrace{P(D | X)}^{\text{likelihood of X under D}} \hspace{.1cm} \cdot \hspace{.1cm} \overbrace{P(X)}^{\text{prior of X}}}{\underbrace{P(D)}_{\text{marginalization or evidence of the model}}}\] &ldquo;discrete domain is just a subset of the continuous domain&rdquo; this extends deductive reasoning (statistics) to plausible reasoning (probabilities) Exponential Families and Conjugate Priors # random variable X taking x values \(\subset \R^n\) probability distribution for X with pdf of the following form:"><meta property="og:type" content="article"><meta property="og:url" content="https://xsleaks.dev/docs/lectures/bayesian-machine-learning/bayes/"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2024-03-04T21:24:08+01:00"><title>Bayesian Inference | Ricardo Chin</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png><link rel=stylesheet href=/book.min.b74e85bd7803de00c09a320dcf09ae0d7e37702a9918995f5fe9d1c71c55a223.css integrity="sha256-t06FvXgD3gDAmjINzwmuDX43cCqZGJlfX+nRxxxVoiM=" crossorigin=anonymous><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>Ricardo Chin</span></a></h2><ul><li class=book-section-flat><a href=/docs/design/>Design Repository</a><ul><li><a href=/docs/design/fea-beam-simulation/>FEA Beam Instability Modes</a><ul></ul></li><li><a href=/docs/design/industrial-crane-design/>Industrial Girder Crane</a><ul></ul></li><li><a href=/docs/design/finite-element-method-development/>FEM Package Development</a><ul></ul></li><li><a href=/docs/design/manual-transmission-design/>Gear Train Transmission</a><ul></ul></li></ul></li><li class=book-section-flat><a href=/docs/code/>System Repository</a><ul><li><a href=/docs/code/agv/>AGV: Stochastic Identification</a><ul></ul></li><li><a href=/docs/code/uav/>UAV: Red Bull Air Racing</a><ul><li><a href=/docs/code/uav/dynamics/>Drone System Dynamics</a></li><li><a href=/docs/code/uav/continuous-controller-design/>Continuous Controller</a></li><li><a href=/docs/code/uav/discrete-controller-design/>Discrete Controller</a></li><li><a href=/docs/code/uav/computer-vision/>Gate Sense: Computer Vision</a></li></ul></li><li><a href=/docs/code/robotics/>Robotics: Kin, Dynamics & Control</a><ul></ul></li><li><a href=/docs/code/deep-learning-fake-news/>Fake News: Inference & Clusters</a><ul></ul></li><li><a href=/docs/code/bin-packing/>EC Optimization: Space & Time</a><ul><li><a href=/docs/code/bin-packing/genetic-algorithm/>Genetic Algorithm</a></li><li><a href=/docs/code/bin-packing/particle-swarm-optimization/>Particle Swarm Optimization</a></li></ul></li><li><a href=/docs/code/snake-game/>Snake: Reinforcement Learning</a><ul><li><a href=/docs/code/snake-game/deepqnetwork/>Off Policy RL & Neuroevolution</a></li><li><a href=/docs/code/snake-game/adversarial/>Adversarial Multi-Agent RL</a></li></ul></li></ul></li><li class=book-section-flat><a href=/docs/lectures/>My Notes and Lectures</a><ul><li><a href=/docs/lectures/bayesian-machine-learning/bayes/ class=active>Bayesian Inference</a></li><li><a href=/docs/lectures/bayesian-optimization/bayesopt/>Bayesian Optimization</a></li><li><a href=/docs/lectures/hamiltonian-graphs/hamiltonian/>Fortran: Linked Lists</a></li></ul></li><li class=book-section-flat><a href=/docs/competitions/>Academic Competitions</a><ul><li><a href=/docs/competitions/fst/>Formula Student Lisbon</a></li><li><a href=/docs/competitions/thermocup/>ThermoCup</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Bayesian Inference</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><a href=#understanding-uncertainty-via-probabilities><strong>Understanding Uncertainty via Probabilities</strong></a></li><li><a href=#exponential-families-and-conjugate-priors><strong>Exponential Families and Conjugate Priors</strong></a></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=notes-on-bayesian-inference><strong>Notes on Bayesian Inference</strong>
<a class=anchor href=#notes-on-bayesian-inference>#</a></h1><blockquote class="book-hint2 tip"><p class="hint-title tip"><svg class="book-icon"><use href="/svg/hint-icons.svg#tip-notice"/></svg><span>tip</span></p><strong>git clone -> cd</strong>
<strong>pip install -r requirements.txt</strong> (scipy, jax.numpy)</blockquote><h3 id=understanding-uncertainty-via-probabilities><strong>Understanding Uncertainty via Probabilities</strong>
<a class=anchor href=#understanding-uncertainty-via-probabilities>#</a></h3><ul><li>sum rule: P(X) = P(X,Y) + P(X,
<link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\(\neg\)
</span>Y)</li><li>product rule: P(X,Y) = P(X) <span>\(\cdot\)
</span>P(Y | X)</li><li>bayes theorem on data D:</li></ul><span>\[\underbrace{P(X | D)}_{\text{posterior of X given D}} \hspace{.1cm} = \frac{\overbrace{P(D | X)}^{\text{likelihood of X under D}} \hspace{.1cm} \cdot \hspace{.1cm} \overbrace{P(X)}^{\text{prior of X}}}{\underbrace{P(D)}_{\text{marginalization or evidence of the model}}}\]</span><ul><li><em>&ldquo;discrete domain is just a subset of the continuous domain&rdquo;</em></li><li>this extends deductive reasoning (statistics) to plausible reasoning (probabilities)</li></ul><h3 id=exponential-families-and-conjugate-priors><strong>Exponential Families and Conjugate Priors</strong>
<a class=anchor href=#exponential-families-and-conjugate-priors>#</a></h3><ul><li><p>random variable X taking x values <span>\(\subset \R^n\)</span></p></li><li><p>probability distribution for X with pdf of the following form:</p></li></ul><span>\[p_{w}(x) = \overbrace{h(x)}^{\text{base measure}} \, \text{exp} \left( \overbrace{\phi(x)^T}^{\text{sufficient statistics}} \underbrace{w}_{\text{natural parameters}} - \text{log} \, \overbrace{Z(w)}^{\text{partition function}} \right)\]
</span><span>\[\Rightlongarrow \frac{h(x)}{Z(w)} e^{\phi(x)^T w} \hspace{.05cm} = \hspace{.05cm} p(x | w)\]</span><ul><li><p>for notational convenience, reparametrize natural parameters w := <span>\(\eta(\theta)\)
</span>in terms of canonical parameters <span>\(\theta\)</span></p></li><li><p>exponential families <span>\((h(x), \phi(x))\)
</span>as the model for some data <strong>x</strong> guarantee automatic existence of conjugate priors, although not always tractable</p></li><li><p>conjugate priors allow analytic Bayesian inference of probabilistic models, if we can compute the partition function Z(w) of the likelihood and the one for the conjugate prior F(<span>
\(\alpha, \nu\)
</span>)</p></li><li><p>biggest challenge is finding the normalization constant</p></li><li><p>reduce Bayesian inference to:</p><ul><li>modelling: computing sufficient statistics <span>\(\phi(x)\)
</span>and partition function Z(w)</li><li>evaluating posterior: assessing log partition function F of the conjugate prior</li></ul></li><li><p>if F is not tractable <span>\(\Longrightarrow\)
</span>use Laplace approximations:</p><ul><li>find the mode <span>\(ŵ\)
</span>of the posterior, by solving root-finding problems</li></ul><span>\[ \nabla_{w} \hspace{.05cm} \text{log} \hspace{.05cm} p (w | x) = \frac{\alpha + \sum_{i=1}^n \phi(x_{i})}{\nu + n}
\]</span><ul><li><p>evaluate the Hessian <span>\(\Psi = \nabla_w \nabla_w^T \hspace{.05cm} \text{log} \hspace{.05cm} p(w|x)\)
</span>at the mode ŵ</p></li><li><p>approximate posterior as <span>\(\mathcal{N}(w;ŵ, -\Psi^{-1}) \)
</span>and the conjugate log partition function as:</p><span>\[ F(\alpha', \nu') \approx \sqrt{(2\pi)^d \hspace{.05cm} \text{det}(-\Psi^{-1})} \cdot \text{exp}[ ŵ^T \cdot \alpha' - \hspace{.01cm} \text{log} \hspace{.01cm} Z(ŵ)^T \hspace{.01cm} \nu' ]
\]</span></li></ul></li></ul><blockquote class="book-hint2 important"><p class="hint-title important"><svg class="book-icon"><use href="/svg/hint-icons.svg#important-notice"/></svg><span>important</span></p>Laplace approximations reveal that Bayesian inference prioritizes capturing the geometry of the likelihood function around its peak (mode), rather than solely focusing on the prior distribution. In this context, uncertainty is better understood as encompassing the multitude of potential solutions simultaneously, rather than fixating on a single point estimate. It&rsquo;s about monitoring the breadth of plausible solutions. This means it is about observing the volume of possibilities rather than pinpointing individual points</blockquote></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/roaked/roaked.github.io/commit/33b5da388441a0123c0524699367687f40a68362 title='Last modified by roaked | March 4, 2024' target=_blank rel=noopener><img src=/svg/calendar.svg class=book-icon alt=Calendar>
<span>March 4, 2024</span></a></div><div><a class="flex align-center" href=https://github.com/roaked/roaked.github.io/edit/main/content/content/docs/lectures/bayesian-machine-learning/bayes.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Ricardo Chin</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#understanding-uncertainty-via-probabilities><strong>Understanding Uncertainty via Probabilities</strong></a></li><li><a href=#exponential-families-and-conjugate-priors><strong>Exponential Families and Conjugate Priors</strong></a></li></ul></li></ul></nav></div></aside></main></body></html>