<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Notes on Bayesian Inference # tip
git clone -> cd pip install -r requirements.txt (scipy, jax.numpy) Understanding Uncertainty via Probabilities # sum rule: P(X) = P(X,Y) + P(X, \(\neg\) Y) product rule: P(X,Y) = P(X) \(\cdot\) P(Y | X) bayes theorem on data D: \[\underbrace{P(X | D)}_{\text{posterior of X given D}} \hspace{.1cm} = \frac{\overbrace{P(D | X)}^{\text{likelihood of X under D}} \hspace{.1cm} \cdot \hspace{.1cm} \overbrace{P(X)}^{\text{prior of X}}}{\underbrace{P(D)}_{\text{marginalization or evidence of the model}}}\] &ldquo;discrete domain is just a subset of the continuous domain&rdquo; this extends deductive reasoning (statistics) to plausible reasoning (probabilities) Exponential Families and Conjugate Priors # random variable X taking x values \(\subset \R^n\) probability distribution for X with pdf of the following form:"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:title" content="Bayesian Inference"><meta property="og:description" content="Notes on Bayesian Inference # tip
git clone -> cd pip install -r requirements.txt (scipy, jax.numpy) Understanding Uncertainty via Probabilities # sum rule: P(X) = P(X,Y) + P(X, \(\neg\) Y) product rule: P(X,Y) = P(X) \(\cdot\) P(Y | X) bayes theorem on data D: \[\underbrace{P(X | D)}_{\text{posterior of X given D}} \hspace{.1cm} = \frac{\overbrace{P(D | X)}^{\text{likelihood of X under D}} \hspace{.1cm} \cdot \hspace{.1cm} \overbrace{P(X)}^{\text{prior of X}}}{\underbrace{P(D)}_{\text{marginalization or evidence of the model}}}\] &ldquo;discrete domain is just a subset of the continuous domain&rdquo; this extends deductive reasoning (statistics) to plausible reasoning (probabilities) Exponential Families and Conjugate Priors # random variable X taking x values \(\subset \R^n\) probability distribution for X with pdf of the following form:"><meta property="og:type" content="article"><meta property="og:url" content="https://xsleaks.dev/docs/lectures/bayesian-machine-learning/bayes/"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2024-03-14T18:58:12+01:00"><title>Bayesian Inference | Ricardo Chin</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png><link rel=stylesheet href=/book.min.b74e85bd7803de00c09a320dcf09ae0d7e37702a9918995f5fe9d1c71c55a223.css integrity="sha256-t06FvXgD3gDAmjINzwmuDX43cCqZGJlfX+nRxxxVoiM=" crossorigin=anonymous><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>Ricardo Chin</span></a></h2><ul><li class=book-section-flat><a href=/docs/design/>Design Repository</a><ul><li><a href=/docs/design/fea-beam-simulation/>FEA Beam Instability Modes</a><ul></ul></li><li><a href=/docs/design/industrial-crane-design/>Industrial Girder Crane</a><ul></ul></li><li><a href=/docs/design/finite-element-method-development/>FEM Package Development</a><ul></ul></li><li><a href=/docs/design/manual-transmission-design/>Gear Train Transmission</a><ul></ul></li></ul></li><li class=book-section-flat><a href=/docs/code/>System Repository</a><ul><li><a href=/docs/code/agv/>AGV: Stochastic Identification</a><ul></ul></li><li><a href=/docs/code/uav/>UAV: Red Bull Air Racing</a><ul><li><a href=/docs/code/uav/dynamics/>Drone System Dynamics</a></li><li><a href=/docs/code/uav/continuous-controller-design/>Continuous Controller</a></li><li><a href=/docs/code/uav/discrete-controller-design/>Discrete Controller</a></li><li><a href=/docs/code/uav/computer-vision/>Gate Sense: Computer Vision</a></li></ul></li><li><a href=/docs/code/robotics/>Robotics: Kin, Dynamics & Control</a><ul></ul></li><li><a href=/docs/code/deep-learning-fake-news/>Fake News: Inference & Clusters</a><ul></ul></li><li><a href=/docs/code/bin-packing/>EC Optimization: Space & Time</a><ul><li><a href=/docs/code/bin-packing/genetic-algorithm/>Genetic Algorithm</a></li><li><a href=/docs/code/bin-packing/particle-swarm-optimization/>Particle Swarm Optimization</a></li></ul></li><li><a href=/docs/code/snake-game/>Snake: Reinforcement Learning</a><ul><li><a href=/docs/code/snake-game/deepqnetwork/>Off Policy RL & Neuroevolution</a></li><li><a href=/docs/code/snake-game/adversarial/>Adversarial Multi-Agent RL</a></li></ul></li></ul></li><li class=book-section-flat><a href=/docs/lectures/>My Notes and Lectures</a><ul><li><a href=/docs/lectures/bayesian-machine-learning/bayes/ class=active>Bayesian Inference</a></li><li><a href=/docs/lectures/bayesian-optimization/bayesopt/>Bayesian Optimization</a></li><li><a href=/docs/lectures/nonlinear-pro/nonlinear/>Nonlinear Programming</a></li></ul></li><li class=book-section-flat><a href=/docs/competitions/>Academic Competitions</a><ul><li><a href=/docs/competitions/fst/>Formula Student Lisbon</a></li><li><a href=/docs/competitions/thermocup/>ThermoCup</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Bayesian Inference</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><a href=#understanding-uncertainty-via-probabilities><strong>Understanding Uncertainty via Probabilities</strong></a></li><li><a href=#exponential-families-and-conjugate-priors><strong>Exponential Families and Conjugate Priors</strong></a></li><li><a href=#gaussians><strong>Gaussians</strong></a></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=notes-on-bayesian-inference><strong>Notes on Bayesian Inference</strong>
<a class=anchor href=#notes-on-bayesian-inference>#</a></h1><blockquote class="book-hint2 tip"><p class="hint-title tip"><svg class="book-icon"><use href="/svg/hint-icons.svg#tip-notice"/></svg><span>tip</span></p><ul><li><strong>git clone -> cd</strong></li><li><strong>pip install -r requirements.txt</strong> (scipy, jax.numpy)</li></ul></blockquote><h3 id=understanding-uncertainty-via-probabilities><strong>Understanding Uncertainty via Probabilities</strong>
<a class=anchor href=#understanding-uncertainty-via-probabilities>#</a></h3><ul><li>sum rule: P(X) = P(X,Y) + P(X,
<link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\(\neg\)
</span>Y)</li><li>product rule: P(X,Y) = P(X) <span>\(\cdot\)
</span>P(Y | X)</li><li>bayes theorem on data D:</li></ul><span>\[\underbrace{P(X | D)}_{\text{posterior of X given D}} \hspace{.1cm} = \frac{\overbrace{P(D | X)}^{\text{likelihood of X under D}} \hspace{.1cm} \cdot \hspace{.1cm} \overbrace{P(X)}^{\text{prior of X}}}{\underbrace{P(D)}_{\text{marginalization or evidence of the model}}}\]</span><ul><li><em>&ldquo;discrete domain is just a subset of the continuous domain&rdquo;</em></li><li>this extends deductive reasoning (statistics) to plausible reasoning (probabilities)</li></ul><h3 id=exponential-families-and-conjugate-priors><strong>Exponential Families and Conjugate Priors</strong>
<a class=anchor href=#exponential-families-and-conjugate-priors>#</a></h3><ul><li><p>random variable X taking x values <span>\(\subset \R^n\)</span></p></li><li><p>probability distribution for X with <em>pdf</em> of the following form:</p></li></ul><span>\[p_{w}(x) = \overbrace{h(x)}^{\text{base measure}} \, \text{exp} \left( \overbrace{\phi(x)^T}^{\text{sufficient statistics}} \cdot \underbrace{w}_{\text{natural parameters}} - \text{log} \, \overbrace{Z(w)}^{\text{partition function}} \right)\]
</span><span>\[= \frac{h(x)}{Z(w)} e^{\phi(x)^T w} = p(x | w)\]</span><ul><li><p>for notational convenience, reparametrize natural parameters w := <span>\(\eta(\theta)\)
</span>in terms of canonical parameters <span>\(\theta\)</span></p></li><li><p>exponential families <span>\((h(x), \phi(x))\)
</span>as the model for some data <strong>x</strong> guarantee automatic existence of conjugate priors, although not always tractable</p></li><li><p>conjugate priors allow analytic Bayesian inference of probabilistic models, if we can compute the partition function Z(w) of the likelihood and the one for the conjugate prior F(<span>
\(\alpha, \nu\)
</span>)</p></li><li><p>biggest challenge is finding the normalization constant</p></li><li><p>reduce Bayesian inference to:</p><ul><li>modelling: computing sufficient statistics <span>\(\phi(x)\)
</span>and partition function Z(w)</li><li>evaluating posterior: assessing log partition function F of the conjugate prior</li></ul></li><li><p>if F is not tractable <span>\(\Longrightarrow\)
</span>use Laplace approximations:</p><ul><li>find the mode <span>\(Åµ\)
</span>of the posterior, by solving root-finding problems</li></ul><span>\[ \nabla_{w} \hspace{.05cm} \text{log} \hspace{.05cm} p (w | x) = \frac{\alpha + \sum_{i=1}^n \phi(x_{i})}{\nu + n}
\]</span><ul><li><p>evaluate the Hessian <span>\(\Psi = \nabla_w \nabla_w^T \hspace{.05cm} \text{log} \hspace{.05cm} p(w|x)\)
</span>at the mode Åµ</p></li><li><p>approximate posterior as <span>\(\mathcal{N}(w;Åµ, -\Psi^{-1}) \)
</span>and the conjugate log partition function as:</p><span>\[ F(\alpha', \nu') \approx \sqrt{(2\pi)^d \hspace{.05cm} \text{det}(-\Psi^{-1})} \cdot \text{exp}[ Åµ^T \cdot \alpha' - \hspace{.01cm} \text{log} \hspace{.01cm} Z(Åµ)^T \hspace{.01cm} \nu' ]
\]</span></li></ul></li></ul><blockquote class="book-hint2 important"><p class="hint-title important"><svg class="book-icon"><use href="/svg/hint-icons.svg#important-notice"/></svg><span>important</span></p><ul><li><p>Laplace approximations reveal that Bayesian inference prioritizes capturing the geometry of the likelihood function around its peak (mode), rather than solely focusing on the prior distribution</p></li><li><p>Uncertainty is better understood as encompassing the multitude of potential solutions simultaneously, rather than fixating on a single point estimate. It&rsquo;s about monitoring the breadth of plausible solutions. This means observing the volume of possibilities rather than pinpointing individual points</p></li></ul></blockquote><h3 id=gaussians><strong>Gaussians</strong>
<a class=anchor href=#gaussians>#</a></h3><ul><li><p>Gaussian inference is linear algebra at its core</p><ul><li>products of Gaussians are Gaussians</li></ul><p><span>\[ \mathcal{N}(x;a,A) \mathcal{N}(x;b,B) = \mathcal{N}(x;c,C) \mathcal{N}(a;b, A+B)
\]
</span><span>\[ C = (A^{-1} + B^{-1})^{-1}, \quad c = C(A^{-1}a + B^{-1}b)
\]</span></p><ul><li>linear maps/projections of Gaussians variables are Gaussian variables
<span>\[ p(z) = \mathcal{N}(z; \mu, \Sigma) \Longrightarrow p(Az) = \mathcal{N}(Az, A\mu, A\Sigma A^T)
\]</span></li><li>marginals of Gaussians are Gaussians
<span>\[ \int \mathcal{N} \left[ \begin{array}{c} x \\ y \end{array}; \begin{bmatrix} \mu_x \\ \mu_y \end{bmatrix}, \begin{bmatrix} \Sigma_{xx} & \Sigma_{xy} \\ \Sigma_{yx} & \Sigma_{yy} \end{bmatrix} \right] dy = \mathcal{N}(x;\mu_x, \Sigma_{xx})
\]</span></li><li>linear conditionals of Gaussians are Gaussians
<span>\[ p(x | y) = \frac{p(x,y)}{p(y)} = \mathcal{N}(x; \mu_x + \Sigma_{xy}\Sigma_{yy}^{-1}(y - \mu_y),\Sigma_{xx}-\Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx})
\]</span></li></ul></li><li><p>if Gaussian prior over a random variable and observations are linearly related, then all conditionals, joints and marginals are Gaussian with means and covariances computable by linear algebra expressions &ndash; <strong>Bayesian inference becomes linear algebra</strong></p></li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/roaked/roaked.github.io/commit/e75123d3cd84bebead88e637d5282bb9c2674771 title='Last modified by roaked | March 14, 2024' target=_blank rel=noopener><img src=/svg/calendar.svg class=book-icon alt=Calendar>
<span>March 14, 2024</span></a></div><div><a class="flex align-center" href=https://github.com/roaked/roaked.github.io/edit/main/content/content/docs/lectures/bayesian-machine-learning/bayes.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Ricardo Chin</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#understanding-uncertainty-via-probabilities><strong>Understanding Uncertainty via Probabilities</strong></a></li><li><a href=#exponential-families-and-conjugate-priors><strong>Exponential Families and Conjugate Priors</strong></a></li><li><a href=#gaussians><strong>Gaussians</strong></a></li></ul></li></ul></nav></div></aside></main></body></html>