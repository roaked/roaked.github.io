<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Genetic Tuning for Smarter Deep Q-Learning # Undergoing dev // Documented ASAP!! # warning
Fitness function optimization needed. Disclaimer - used the freeCodeCamp.org base code for the Snake Game, with a couple tweaks for my Genetic Algorithm implementation. 1. Snake Game # The Snake game has served as a fundamental project for programming novices due to its simplicity and versatility. In this work, Pygame is used, a Python library designed for game development, to create a Snake game environment."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:title" content="Snake Game: Genetic RL-DQN"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://xsleaks.dev/docs/code/snake-game/"><title>Snake Game: Genetic RL-DQN | Ricardo Chin</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png><link rel=stylesheet href=/book.min.b74e85bd7803de00c09a320dcf09ae0d7e37702a9918995f5fe9d1c71c55a223.css integrity="sha256-t06FvXgD3gDAmjINzwmuDX43cCqZGJlfX+nRxxxVoiM=" crossorigin=anonymous><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-10WQY47KS2"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-10WQY47KS2",{anonymize_ip:!1})}</script><link rel=alternate type=application/rss+xml href=https://xsleaks.dev/docs/code/snake-game/index.xml title="Ricardo Chin"></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>Ricardo Chin</span></a></h2><ul><li class=book-section-flat><a href=/docs/design/>Design Repository</a><ul><li><a href=/docs/design/fea-beam-simulation/>FEA Beam Instability Modes</a><ul></ul></li><li><a href=/docs/design/industrial-crane-design/>Industrial Girder Crane</a><ul></ul></li><li><a href=/docs/design/finite-element-method-development/>FEM Package Development</a><ul></ul></li><li><a href=/docs/design/manual-transmission-design/>Gear Train Transmission</a><ul></ul></li><li><a href=/docs/design/electron-beam-tech/>Electron Beam Technology</a><ul></ul></li></ul></li><li class=book-section-flat><a href=/docs/code/>Coding Repository</a><ul><li><a href=/docs/code/uav/>UAV Red Bull Air Racing</a><ul><li><a href=/docs/code/uav/dynamics/>Drone Dynamics</a></li><li><a href=/docs/code/uav/continuous-controller-design/>Drone Continuous Controller</a></li><li><a href=/docs/code/uav/discrete-controller-design/>Drone Discrete Controller</a></li><li><a href=/docs/code/uav/computer-vision/>Drone Computer Vision</a></li></ul></li><li><a href=/docs/code/agv/>AGV: System Identification</a><ul></ul></li><li><a href=/docs/code/deep-learning-fake-news/>Deep Learning on Fake News</a><ul></ul></li><li><a href=/docs/code/hamiltonian-graphs/hamiltonian/>Hamiltonian Graphs: Linked Lists</a></li><li><a href=/docs/code/bin-packing/>EC Optimization: Space & Time</a><ul><li><a href=/docs/code/bin-packing/genetic-algorithm/>Genetic Algorithm</a></li><li><a href=/docs/code/bin-packing/particle-swarm-optimization/>Particle Swarm Optimization</a></li></ul></li><li><a href=/docs/code/snake-game/ class=active>Snake Game: Genetic RL-DQN</a><ul></ul></li><li><a href=/docs/code/micromouse/>Micromouse: Flood Fill to A*</a><ul></ul></li><li><a href=/docs/code/pybamm-ml-battery/>PyBaMM-ML EV Battery Status</a><ul></ul></li></ul></li><li class=book-section-flat><a href=/docs/competitions/>Academic Competitions</a><ul><li><a href=/docs/competitions/fst/>Formula Student Lisbon</a></li><li><a href=/docs/competitions/thermocup/>ThermoCup</a></li></ul></li><li class=book-section-flat><a href=/docs/leet/>My LeetCode Solutions</a><ul></ul></li><li class=book-section-flat><a href=/docs/mod/>Website Modifications</a><ul></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Snake Game: Genetic RL-DQN</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#1-snake-game>1. Snake Game</a></li><li><a href=#2-user-and-ai-controlled-snake>2. User and AI Controlled Snake</a></li><li><a href=#3-reinforcement-deep-q-network-model>3. Reinforcement Deep Q-Network Model</a><ul><li><a href=#31-linearqnet-and-qtrainer-classes>3.1. LinearQNet and QTrainer Classes</a></li><li><a href=#32-deploying-a-reinforcement-learning-agent>3.2. Deploying a Reinforcement Learning Agent</a></li><li><a href=#33-max-replay-buffer-and-target-network>3.3. Max Replay Buffer and Target Network</a></li></ul></li><li><a href=#4-genetic-optimization-of-a-rl-deep-q-network>4. Genetic Optimization of a RL Deep-Q-Network</a><ul><li><a href=#41-theoretical-background>4.1. Theoretical Background</a></li></ul></li><li><a href=#5-results>5. Results</a></li><li><a href=#6-outcomes>6. Outcomes</a></li></ul></nav></aside></header><article class=markdown><h1 id=genetic-tuning-for-smarter-deep-q-learning><strong>Genetic Tuning for Smarter Deep Q-Learning</strong>
<a class=anchor href=#genetic-tuning-for-smarter-deep-q-learning>#</a></h1><p><img src=https://miro.medium.com/v2/resize:fit:2800/1*zRZ46MeFZMd5F52CHM6EYA.png alt=213d></p><h1 id=undergoing-dev--documented-asap>Undergoing dev // Documented ASAP!!
<a class=anchor href=#undergoing-dev--documented-asap>#</a></h1><blockquote class="book-hint2 warning"><p class="hint-title warning"><svg class="book-icon"><use href="/svg/hint-icons.svg#warning-notice"/></svg><span>warning</span></p><ul><li>Fitness function optimization needed.</li><li><strong>Disclaimer</strong> - used the <a href=https://www.freecodecamp.org/news/train-an-ai-to-play-a-snake-game-using-python/>freeCodeCamp.org</a> base code for the Snake Game, with a couple tweaks for my Genetic Algorithm implementation.</li></ul></blockquote><div class="book-columns flex flex-wrap"><div class="flex-even markdown-inner"><img src=https://s5.gifyu.com/images/SiDzT.gif alt=123></div><div class="flex-even markdown-inner"><img src=https://s5.gifyu.com/images/SiDzw.gif alt=123019></div></div><h2 id=1-snake-game>1. Snake Game
<a class=anchor href=#1-snake-game>#</a></h2><p>The Snake game has served as a fundamental project for programming novices due to its simplicity and versatility. In this work, Pygame is used, a Python library designed for game development, to create a Snake game environment. The core motivation is to provide a controlled and adaptable setting for AI development and reinforcement learning combined with metaheuristics.</p><h2 id=2-user-and-ai-controlled-snake>2. User and AI Controlled Snake
<a class=anchor href=#2-user-and-ai-controlled-snake>#</a></h2><blockquote class="book-hint2 important"><p class="hint-title important"><svg class="book-icon"><use href="/svg/hint-icons.svg#important-notice"/></svg><span>important</span></p>Most of the code for the game implementation was adapted using Pygame for providing a robust framework for game development in Python. &ndash; <a href=pygame.org/docs>Pygame Documentation</a></blockquote><p>The code begins with initializing Pygame and setting up essential game parameters such as window size, colors, and game speed. Pygame&rsquo;s functionalities are leveraged for window creation, event handling, and display rendering.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> random<span style=color:#f92672>,</span> pygame<span style=color:#f92672>,</span> sys<span style=color:#f92672>,</span> time
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Pygame initialization and window setup</span>
</span></span><span style=display:flex><span>check_errors <span style=color:#f92672>=</span> pygame<span style=color:#f92672>.</span>init()
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> check_errors[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;[!] Had </span><span style=color:#e6db74>{</span>check_errors[<span style=color:#ae81ff>1</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74> errors when initializing game, exiting...&#39;</span>)
</span></span><span style=display:flex><span>    sys<span style=color:#f92672>.</span>exit(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;[+] Game successfully initialized&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Defined other variables here, adapt fontsize, window size, colors, speed, </span>
</span></span><span style=display:flex><span><span style=color:#75715e># block size, etc...</span>
</span></span></code></pre></div><p>In addition, <code>SnakeGameUser</code> and <code>SnakeGameAI</code> classes are defined which encapsulate the game logic, managing the game state, snake movement, collision detection, and food placement.</p><blockquote class="book-hint2 important"><p class="hint-title important"><svg class="book-icon"><use href="/svg/hint-icons.svg#important-notice"/></svg><span>important</span></p><p>Vital functions encompass:</p><ul><li><code>def __init__</code> &ndash; Initializing game window and start up the game state</li><li><code>def _init_game</code> &ndash; Initialize game state variables (snake position, score, food)</li><li><code>def _place_food</code> &ndash; Places food randomly within the game window, avoiding snake collision</li><li><code>def _move</code> &ndash; Handling snake movement based on user or AI action</li><li><code>def play_step </code>&ndash; Process each step of the game based on user (and later AI actions). (in addition, collects user input, move snake, check collision, update score, etc.)</li></ul></blockquote><p>Given <code>__init__</code> and <code>__init__game</code> functions, the snake is initialized with a starting position, consisting of three body parts (<code>self.head</code> and two segments) positioned horizontally to the right - this is normally executed in good practice for developing a snake game, including representing the <code>self.head</code> at the center of the window. This setup essentially creates an initial length for the snake, allowing it to start the game with a visible length and direction. The snake initially consists of these three segments, and as the game progresses and the snake moves, additional segments will be added or removed based on its movement, food consumption, and collision detection.</p><blockquote class="book-hint2 tip"><p class="hint-title tip"><svg class="book-icon"><use href="/svg/hint-icons.svg#tip-notice"/></svg><span>tip</span></p>Having an initial length provides a visible presence of the snake on the screen, allowing the player or AI agent to see its position and direction from the beginning of the game.</blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_init_game</span>(self):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Game state variables (snake position, score, food)</span>
</span></span><span style=display:flex><span>    start_x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>width <span style=color:#f92672>//</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>    start_y <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>height <span style=color:#f92672>//</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>direction <span style=color:#f92672>=</span> Direction<span style=color:#f92672>.</span>RIGHT
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>head <span style=color:#f92672>=</span> Point(start_x, start_y)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>snake <span style=color:#f92672>=</span> [self<span style=color:#f92672>.</span>head]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Create initial instance of the snake with three blocks</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>snake<span style=color:#f92672>.</span>append(Point(self<span style=color:#f92672>.</span>head<span style=color:#f92672>.</span>x <span style=color:#f92672>-</span> i <span style=color:#f92672>*</span> BLOCK_SIZE, start_y))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>score <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>food <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>frame_iteration <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>_place_food()
</span></span></code></pre></div><p>After snake initialization, the <code>_move</code> function adjusts the snake&rsquo;s direction based on the received action (from user or AI). This is done using a list <code>clockwise_directions</code> that represents the directional movement of the snake - right, down, left, up (clockwise). Moreover, it determines the intended change in direction: no change, right turn, or left turn. Calculating the new direction index by locating the current direction within the <code>clockwise_directions</code> list and adjusting it based on the determined change, it then updates the snake&rsquo;s direction accordingly.</p><blockquote class="book-hint2 note"><p class="hint-title note"><svg class="book-icon"><use href="/svg/hint-icons.svg#note-notice"/></svg><span>note</span></p>Case scenarios:
If the action indicates no change [1, 0, 0] -> snake maintains its current direction.
If the action indicates a right turn [0, 1, 0] -> snake updates the direction to the next index in the clockwise order.
If the action indicates a left turn [0, 0, 1] -> snake updates the direction to the previous index in the clockwise order (effectively, anti-clockwise).</blockquote><p>Furthermore, using a dictionary <code>movement_adjustments</code> that maps directions foor movement adjustments (changes in <code>x</code> and <code>y</code> coordinates), it applies the corresponding adjustment to the snake&rsquo;s head position, effectively moving it in the intended direction by a distance specified by the <code>BLOCK_SIZE</code>, aligning its position with the game grid.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_move</span>(self, action):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Define clockwise directions</span>
</span></span><span style=display:flex><span>        clockwise_directions <span style=color:#f92672>=</span> [Direction<span style=color:#f92672>.</span>RIGHT, Direction<span style=color:#f92672>.</span>DOWN,
</span></span><span style=display:flex><span>                                 Direction<span style=color:#f92672>.</span>LEFT, Direction<span style=color:#f92672>.</span>UP]
</span></span><span style=display:flex><span>        current_direction <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>direction
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Define action codes for each movement direction</span>
</span></span><span style=display:flex><span>        no_change_action <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        right_turn_action <span style=color:#f92672>=</span> [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        left_turn_action <span style=color:#f92672>=</span> [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Determine the change in direction based on action</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> action <span style=color:#f92672>==</span> no_change_action:
</span></span><span style=display:flex><span>            direction_change <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>elif</span> action <span style=color:#f92672>==</span> right_turn_action:
</span></span><span style=display:flex><span>            direction_change <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>elif</span> action <span style=color:#f92672>==</span> left_turn_action:
</span></span><span style=display:flex><span>            direction_change <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Calculate the new direction index</span>
</span></span><span style=display:flex><span>        current_index <span style=color:#f92672>=</span> clockwise_directions<span style=color:#f92672>.</span>index(current_direction)
</span></span><span style=display:flex><span>        new_index <span style=color:#f92672>=</span> (current_index <span style=color:#f92672>+</span> direction_change) <span style=color:#f92672>%</span> <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>        new_direction <span style=color:#f92672>=</span> clockwise_directions[new_index]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Update the direction</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>direction <span style=color:#f92672>=</span> new_direction
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Define movement adjustments for each direction</span>
</span></span><span style=display:flex><span>        movement_adjustments <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>            Direction<span style=color:#f92672>.</span>RIGHT: (BLOCK_SIZE, <span style=color:#ae81ff>0</span>),
</span></span><span style=display:flex><span>            Direction<span style=color:#f92672>.</span>LEFT: (<span style=color:#f92672>-</span>BLOCK_SIZE, <span style=color:#ae81ff>0</span>),
</span></span><span style=display:flex><span>            Direction<span style=color:#f92672>.</span>DOWN: (<span style=color:#ae81ff>0</span>, BLOCK_SIZE),
</span></span><span style=display:flex><span>            Direction<span style=color:#f92672>.</span>UP: (<span style=color:#ae81ff>0</span>, <span style=color:#f92672>-</span>BLOCK_SIZE)
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Apply movement adjustments to update the head position</span>
</span></span><span style=display:flex><span>        move_x, move_y <span style=color:#f92672>=</span> movement_adjustments[self<span style=color:#f92672>.</span>direction]
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>head <span style=color:#f92672>=</span> Point(self<span style=color:#f92672>.</span>head<span style=color:#f92672>.</span>x <span style=color:#f92672>+</span> move_x, self<span style=color:#f92672>.</span>head<span style=color:#f92672>.</span>y <span style=color:#f92672>+</span> move_y)
</span></span></code></pre></div><p>Afterwards, the <code>_place_food</code> selects a random position within the game grid to place the food for the snake. Initially creating a set containing all current positions occupied by the snake segments, it enters a loop that generates random x and y coordinates within the game boundaries, scaled by the block size, effectively aligning with the grid. These coordinates form a potential new food position. The loop continues generating random positions until it finds a position that doesn&rsquo;t coincide with any segment of the snake. Once a suitable position is found, it assigns this new position as the food&rsquo;s location, effectively placing the food at an unoccupied spot within the game grid, ensuring it does not overlap with the snake&rsquo;s current positions.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_place_food</span>(self):
</span></span><span style=display:flex><span>    snake_positions <span style=color:#f92672>=</span> {point <span style=color:#66d9ef>for</span> point <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>snake}  <span style=color:#75715e># set of snake positions</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Generate random positions until an available one is found</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, (self<span style=color:#f92672>.</span>width <span style=color:#f92672>-</span> BLOCK_SIZE) <span style=color:#f92672>//</span> BLOCK_SIZE) <span style=color:#f92672>*</span> BLOCK_SIZE
</span></span><span style=display:flex><span>        y <span style=color:#f92672>=</span> random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, (self<span style=color:#f92672>.</span>height <span style=color:#f92672>-</span> BLOCK_SIZE) <span style=color:#f92672>//</span> BLOCK_SIZE) <span style=color:#f92672>*</span> BLOCK_SIZE
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        new_food_position <span style=color:#f92672>=</span> Point(x, y)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> new_food_position <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> snake_positions:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>food <span style=color:#f92672>=</span> new_food_position
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span></code></pre></div><p>For defining the game over condition, the <code>is_collision</code> function determines if a collision occurs at a given point within the game. Initially checking the point&rsquo;s existence, defaulting to the snake&rsquo;s head if not provided, it then examines whether the point lies beyond the game&rsquo;s boundaries, assessing if its coordinates exceed the game area&rsquo;s width or height. Additionally, it verifies if the point coincides with any part of the snake&rsquo;s body, excluding the head. If the point is outside the game boundaries or matches a segment of the snake&rsquo;s body, the function returns <code>True</code>, indicating a collision has occurred. Otherwise, it returns <code>False</code>, signifying no collision at that specific point in the game.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>is_collision</span>(self, pt<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> pt <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            pt <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>head
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Boundary collision check</span>
</span></span><span style=display:flex><span>        collides_with_boundary <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>            pt<span style=color:#f92672>.</span>x <span style=color:#f92672>&gt;</span> self<span style=color:#f92672>.</span>width <span style=color:#f92672>-</span> BLOCK_SIZE <span style=color:#f92672>or</span>
</span></span><span style=display:flex><span>            pt<span style=color:#f92672>.</span>x <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>or</span>
</span></span><span style=display:flex><span>            pt<span style=color:#f92672>.</span>y <span style=color:#f92672>&gt;</span> self<span style=color:#f92672>.</span>height <span style=color:#f92672>-</span> BLOCK_SIZE <span style=color:#f92672>or</span>
</span></span><span style=display:flex><span>            pt<span style=color:#f92672>.</span>y <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Snake body collision check</span>
</span></span><span style=display:flex><span>        collides_with_snake <span style=color:#f92672>=</span> pt <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>snake[<span style=color:#ae81ff>1</span>:]
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> collides_with_boundary <span style=color:#f92672>or</span> collides_with_snake
</span></span></code></pre></div><p>Lastly, the <code>play_step</code> function manages a single step within the game loop. It increments the frame count to track game progress and handles events like quitting the game. It updates the snake&rsquo;s movement based on the received action, adding a new head position to the snake&rsquo;s body. In the case of a user-controlled snake AI game, it must take as input, the respective key that is being pressed at the time and adjust the action based on that specific task. Otherwise, <code>play_step</code> will take the action as an input value (in case of AI controlled).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>play_step</span>(self):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>frame_iteration <span style=color:#f92672>+=</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> event <span style=color:#f92672>in</span> pygame<span style=color:#f92672>.</span>event<span style=color:#f92672>.</span>get():
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> event<span style=color:#f92672>.</span>type <span style=color:#f92672>==</span> pygame<span style=color:#f92672>.</span>QUIT:
</span></span><span style=display:flex><span>                pygame<span style=color:#f92672>.</span>quit()
</span></span><span style=display:flex><span>                quit()
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> event<span style=color:#f92672>.</span>type <span style=color:#f92672>==</span> pygame<span style=color:#f92672>.</span>KEYDOWN:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> event<span style=color:#f92672>.</span>key <span style=color:#f92672>==</span> pygame<span style=color:#f92672>.</span>K_LEFT:
</span></span><span style=display:flex><span>                    self<span style=color:#f92672>.</span>direction <span style=color:#f92672>=</span> Direction<span style=color:#f92672>.</span>LEFT
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>elif</span> event<span style=color:#f92672>.</span>key <span style=color:#f92672>==</span> pygame<span style=color:#f92672>.</span>K_RIGHT:
</span></span><span style=display:flex><span>                    self<span style=color:#f92672>.</span>direction <span style=color:#f92672>=</span> Direction<span style=color:#f92672>.</span>RIGHT
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>elif</span> event<span style=color:#f92672>.</span>key <span style=color:#f92672>==</span> pygame<span style=color:#f92672>.</span>K_UP:
</span></span><span style=display:flex><span>                    self<span style=color:#f92672>.</span>direction <span style=color:#f92672>=</span> Direction<span style=color:#f92672>.</span>UP
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>elif</span> event<span style=color:#f92672>.</span>key <span style=color:#f92672>==</span> pygame<span style=color:#f92672>.</span>K_DOWN:
</span></span><span style=display:flex><span>                    self<span style=color:#f92672>.</span>direction <span style=color:#f92672>=</span> Direction<span style=color:#f92672>.</span>DOWN
</span></span></code></pre></div><p>Moreover, the function checks for game-ending conditions — such as collision with itself or exceeding a frame limit—and sets the game over status accordingly, applying penalties if necessary. If the snake consumes the food, it increments the score and updates the food position. After these actions, it refreshes the game display and controls the game&rsquo;s frame rate before returning the reward earned, the game over status, and the current score, providing essential information for the game loop to proceed.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>play_step</span>(self, action):
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>frame_iteration <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> event <span style=color:#f92672>in</span> pygame<span style=color:#f92672>.</span>event<span style=color:#f92672>.</span>get():
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> event<span style=color:#f92672>.</span>type <span style=color:#f92672>==</span> pygame<span style=color:#f92672>.</span>QUIT:
</span></span><span style=display:flex><span>                pygame<span style=color:#f92672>.</span>quit()
</span></span><span style=display:flex><span>                quit()
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Move the snake</span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>_move(action)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>snake<span style=color:#f92672>.</span>insert(<span style=color:#ae81ff>0</span>, self<span style=color:#f92672>.</span>head)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Check for game over conditions</span>
</span></span><span style=display:flex><span>    game_over <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>    reward <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    collision_or_frame_limit <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>is_collision() <span style=color:#f92672>or</span> self<span style=color:#f92672>.</span>frame_iteration <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>100</span> <span style=color:#f92672>*</span> len(self<span style=color:#f92672>.</span>snake)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> collision_or_frame_limit:
</span></span><span style=display:flex><span>        game_over <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>        reward <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> reward, game_over, self<span style=color:#f92672>.</span>score
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Check if the snake has eaten the food</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>head <span style=color:#f92672>==</span> self<span style=color:#f92672>.</span>food:
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>score <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        reward <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>_place_food()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>snake<span style=color:#f92672>.</span>pop()
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Update UI and clock</span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>_update_ui()
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>clock<span style=color:#f92672>.</span>tick(SPEED)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Return game status and score</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> reward, game_over, self<span style=color:#f92672>.</span>score
</span></span></code></pre></div><p>For visualization, the <code>_update_ui</code> function serves as the visual engine of the game, orchestrating the graphical elements displayed to the player. It begins by wiping the display clean with a dark grey background, then proceeds to render the snake&rsquo;s body as a sequence of blue rectangles, each block outlined by a smaller blue rectangle, creating a bordered appearance. Next, it draws the red food block at its designated position within the game grid. Additionally, it generates the textual representation of the player&rsquo;s score, showcasing it prominently in the top-left corner of the screen. Finally, by updating the display, it ensures that all these visual elements accurately reflect the ongoing state of the game, providing players with a real-time view of the snake&rsquo;s movement, the position of the food, and their current score as they play.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_update_ui</span>(self):
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>display<span style=color:#f92672>.</span>fill(GREY)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Draw the snake</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> pt <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>snake:
</span></span><span style=display:flex><span>        snake_rect <span style=color:#f92672>=</span> pygame<span style=color:#f92672>.</span>Rect(pt<span style=color:#f92672>.</span>x, pt<span style=color:#f92672>.</span>y, BLOCK_SIZE, BLOCK_SIZE)
</span></span><span style=display:flex><span>        pygame<span style=color:#f92672>.</span>draw<span style=color:#f92672>.</span>rect(self<span style=color:#f92672>.</span>display, BLUE1, snake_rect)
</span></span><span style=display:flex><span>        <span style=color:#75715e># Inflate the rectangle</span>
</span></span><span style=display:flex><span>        pygame<span style=color:#f92672>.</span>draw<span style=color:#f92672>.</span>rect(self<span style=color:#f92672>.</span>display, BLUE2, snake_rect<span style=color:#f92672>.</span>inflate(<span style=color:#f92672>-</span><span style=color:#ae81ff>8</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>8</span>)) 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Draw the food</span>
</span></span><span style=display:flex><span>    food_rect <span style=color:#f92672>=</span> pygame<span style=color:#f92672>.</span>Rect(self<span style=color:#f92672>.</span>food<span style=color:#f92672>.</span>x, self<span style=color:#f92672>.</span>food<span style=color:#f92672>.</span>y, BLOCK_SIZE, BLOCK_SIZE)
</span></span><span style=display:flex><span>    pygame<span style=color:#f92672>.</span>draw<span style=color:#f92672>.</span>rect(self<span style=color:#f92672>.</span>display, RED, food_rect)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Render the score text</span>
</span></span><span style=display:flex><span>    score_text <span style=color:#f92672>=</span> font<span style=color:#f92672>.</span>render(<span style=color:#e6db74>&#34;Score: &#34;</span> <span style=color:#f92672>+</span> str(self<span style=color:#f92672>.</span>score), <span style=color:#66d9ef>True</span>, WHITE)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>display<span style=color:#f92672>.</span>blit(score_text, (<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Update the display</span>
</span></span><span style=display:flex><span>    pygame<span style=color:#f92672>.</span>display<span style=color:#f92672>.</span>flip()
</span></span></code></pre></div><blockquote class="book-hint2 warning"><p class="hint-title warning"><svg class="book-icon"><use href="/svg/hint-icons.svg#warning-notice"/></svg><span>warning</span></p><p>The game itself is still under development and some of the functions might have been changed already, but not updated here. Nevertheless, current tasks to further enhance the game environment and codebase consist of:</p><ul><li>Complete User Input Handling: Implement the get_user_input method to capture user actions, enabling manual control of the snake.</li><li>Display Rendering: Develop the _update_display method to visually represent the game state using Pygame&rsquo;s rendering capabilities.</li><li>Enhancements: Explore additional features such as different game difficulty levels, multiplayer functionalities, or advanced AI algorithms to enrich the gaming experience.</li></ul></blockquote><p><img src=https://s5.gifyu.com/images/SiDzT.gif alt=123></p><h2 id=3-reinforcement-deep-q-network-model>3. Reinforcement Deep Q-Network Model
<a class=anchor href=#3-reinforcement-deep-q-network-model>#</a></h2><h3 id=31-linearqnet-and-qtrainer-classes>3.1. LinearQNet and QTrainer Classes
<a class=anchor href=#31-linearqnet-and-qtrainer-classes>#</a></h3><p>The <a href=https://github.com/roaked/snake-q-learning-genetic-algorithm/blob/main/model.py>LinearQNet</a> class represents a simple neural network architecture tailored for Q-value approximation in reinforcement learning. It consists of two linear layers initialized during instantiation, with the first layer transforming input features to a hidden layer and the second layer producing Q-values for different actions. Additionally, it sets up other optional components, such as dropout regularization or weight initialization techniques, aiming to enhance the network&rsquo;s learning process and generalization ability.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> __init__(self, input_size, hidden_size, output_size):
</span></span><span style=display:flex><span>    super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>linear1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(input_size, hidden_size)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>linear2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(hidden_size, output_size)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>dropout <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dropout(<span style=color:#ae81ff>0.2</span>)  <span style=color:#75715e># Example: Adding dropout with p=0.2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Initialize weights using Xavier initialization</span>
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>xavier_uniform_(self<span style=color:#f92672>.</span>linear1<span style=color:#f92672>.</span>weight)
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>xavier_uniform_(self<span style=color:#f92672>.</span>linear2<span style=color:#f92672>.</span>weight)
</span></span></code></pre></div><p>The forward method defines the forward pass, applying a rectified linear unit (ReLU) activation to the hidden layer&rsquo;s output before generating the Q-values. Additionally, the save method facilitates saving the model&rsquo;s state dictionary <code>self.state_dict()</code> to a specified file path using PyTorch&rsquo;s <code>torch.save</code> functionality, ensuring the preservation of trained model parameters.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>linear1(x))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>dropout(x)  <span style=color:#75715e># Applying dropout</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>linear2(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>save</span>(self, file_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;model.pth&#39;</span>):
</span></span><span style=display:flex><span>    model_folder_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;./model&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(model_folder_path):
</span></span><span style=display:flex><span>        os<span style=color:#f92672>.</span>makedirs(model_folder_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    file_name <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(model_folder_path, file_name)
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>save(self<span style=color:#f92672>.</span>state_dict(), file_name)    
</span></span></code></pre></div><p>Meanwhile, the QTrainer class manages the training process for the Q-network. Its initialization configures key parameters like learning rate and discount factor, along with setting up an Adam optimizer to update the model&rsquo;s parameters based on the provided learning rate.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>QTrainer</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, model, lr, gamma, target_update_freq<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>lr <span style=color:#f92672>=</span> lr
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>gamma <span style=color:#f92672>=</span> gamma
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>model <span style=color:#f92672>=</span> model
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>optimizer <span style=color:#f92672>=</span> optim<span style=color:#f92672>.</span>Adam(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>lr)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>criterion <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>MSELoss()
</span></span></code></pre></div><p>This can integrated in the train_step function which orchestrates the neural network training process by executing a single iteration of Q-learning updates. It receives an experience tuple containing information about a state, action, received reward, the resulting next state, and an indicator of whether the episode has ended. The function begins by converting these components into <code>PyTorch</code> tensors to facilitate computation within the neural network. Using the provided state, the neural network predicts Q-values for different actions, storing these predictions as <code>pred</code>. Now, we should take a look at how a new Q-value for a state-action pair based on the previous Q-value and the received reward, plus the discounted maximum Q-value achievable in the resulting state is computed. The process for the agent to adjust Q-values iteratively, gradually converging towards optimal action-selection strategies by learning from experiences obtained while interacting with the environment can be seen below for the Q-learning update rule.</p><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\[Q(s,a) = Q(s,a) + \alpha (r + \gamma \cdot max_{a'} Q(s',a')- Q(s,a))\]</span><p>Q(s,a) represents the Q-value, indicating the expected cumulative reward by taking action <code>a</code> in state <code>s</code>. It quantifies the agent&rsquo;s understanding of the long-term desirability of choosing action <code>a</code> while in state <code>s</code>. The evaluation on whether to overwrite old information during upcoming Q-updates is given by the variable <span>\(\alpha\)
</span>. Higher learning rate (<span>
\(\alpha\)
</span>) gives more weight to recent experiences, influencing how much the Q-value changes based on the new piece of information. The agent gets an immediate reward <code>r</code> gained for the action after executing action <code>a</code> at this current state <code>s</code>. This immediate feedback guides the agent&rsquo;s learning, impacting the adjustment of Q-values based on the obtained rewards in each state-action pair. Moreover, the significance of future rewards compared to current rewards is given by <span>\(\gamma\)
</span>. Naturally higher discount factor <span>\(\gamma\)
</span>prioritizes long-term rewards more. Lastly, Q(s&rsquo;,a&rsquo;) indicates the maximum Q-value achievable in the subsequent state <code>s'</code> when taking an action <code>a'</code>. It influences agent&rsquo;s decision-making by taking into account potential future rewards based on the best action available from the next state, guiding him to more rewarding states.</p><p>Employing the Q-learning update rule, the function computes target Q-values based on the predicted values, updating them according to the received rewards and the next state&rsquo;s Q-values if the episode hasn&rsquo;t terminated. Subsequently, it calculates the Mean Squared Error (MSE) loss between the predicted and target Q-values and performs backpropagation to derive gradients for updating the neural network&rsquo;s parameters. Finally, the optimizer applies these gradients to adjust the model&rsquo;s weights through optimization, enhancing the network&rsquo;s ability to approximate accurate Q-values for efficient decision-making in reinforcement learning scenarios.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_step</span>(self, state, action, reward, next_state, done):
</span></span><span style=display:flex><span>    state <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(state, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float)
</span></span><span style=display:flex><span>    next_state <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(next_state, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float)
</span></span><span style=display:flex><span>    action <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(action, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>long)
</span></span><span style=display:flex><span>    reward <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(reward, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    pred <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>model(state)
</span></span><span style=display:flex><span>    target <span style=color:#f92672>=</span> pred<span style=color:#f92672>.</span>clone()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Q-learning update rule</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Handling single-dimensional state and action tensors</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> state<span style=color:#f92672>.</span>dim() <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>:  <span style=color:#75715e># (1,x)</span>
</span></span><span style=display:flex><span>        state <span style=color:#f92672>=</span> state<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        next_state <span style=color:#f92672>=</span> next_state<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        action <span style=color:#f92672>=</span> action<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        reward <span style=color:#f92672>=</span> reward<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        done <span style=color:#f92672>=</span> (done,)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Predicting Q-values based on current state-action pair</span>
</span></span><span style=display:flex><span>    pred <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>model(state)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Clone the prediction for updating</span>
</span></span><span style=display:flex><span>    target <span style=color:#f92672>=</span> pred<span style=color:#f92672>.</span>clone()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> idx <span style=color:#f92672>in</span> range(len(done)):
</span></span><span style=display:flex><span>        Q_new <span style=color:#f92672>=</span> reward[idx]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> done[idx]:
</span></span><span style=display:flex><span>            Q_new <span style=color:#f92672>=</span> reward[idx] <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>gamma <span style=color:#f92672>*</span> torch<span style=color:#f92672>.</span>max(self<span style=color:#f92672>.</span>model(next_state[idx]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        action_idx <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>argmax(action[idx])<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>        target[idx][action_idx] <span style=color:#f92672>=</span> Q_new
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Zero the gradients, compute loss, backpropagate, and update weights</span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>criterion(target, pred)
</span></span><span style=display:flex><span>    loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>optimizer<span style=color:#f92672>.</span>step()
</span></span></code></pre></div><p>Together, these classes form the backbone of a Q-learning approach, where the LinearQNet acts as the neural network to estimate Q-values and the QTrainer orchestrates the training process by updating the network&rsquo;s parameters to improve Q-value predictions.</p><blockquote class="book-hint2 warning"><p class="hint-title warning"><svg class="book-icon"><use href="/svg/hint-icons.svg#warning-notice"/></svg><span>warning</span></p><p>Modifications:</p><ul><li>Experiment with deeper networks, different activation functions (Sigmoid, i.e), regularization L2/ Dropout to prevent overfitting (getting stuck) and use different optimizers (RMSprop or SGD) for <code>LinearQClass</code>.</li><li>It is possible to implement different loss functions or Double Q-learning to mitigate bias impact using separate networks to update Q-values.</li><li>Explore how to improve learning efficiency of the agent.</li></ul></blockquote><h3 id=32-deploying-a-reinforcement-learning-agent>3.2. Deploying a Reinforcement Learning Agent
<a class=anchor href=#32-deploying-a-reinforcement-learning-agent>#</a></h3><p>Let us start by applying the constructor method (<strong>init</strong>) of a class. This is where several fundamental attributes and objects are initialized for a reinforcement learning agent.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>n_games <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span> <span style=color:#75715e># Number of games played</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>epsilon <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span> <span style=color:#75715e># Parameter for exploration-exploitation trade-off</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>gamma <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.9</span> <span style=color:#75715e># Discount factor for future rewards</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>memory <span style=color:#f92672>=</span> deque(maxlen<span style=color:#f92672>=</span>MAX_MEMORY) <span style=color:#75715e># Replay memory for storing experiences</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>trainer <span style=color:#f92672>=</span> QTrainer(self<span style=color:#f92672>.</span>model, lr<span style=color:#f92672>=</span>ALPHA, gamma<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>gamma) <span style=color:#75715e># QTrainer for model training </span>
</span></span></code></pre></div><p><code>self.n_games</code> tracks the number of games the agent has played. <code>self.epsilon</code> represents a parameter essential for the <a href=https://en.wikipedia.org/wiki/Exploration-exploitation_dilemma>exploration-exploitation trade-off</a>, influencing the agent&rsquo;s decision-making process. The <code>self.gamma</code> variable signifies the discount factor applied to future rewards, impacting the agent&rsquo;s prioritization of immediate versus delayed rewards. The <code>deque</code> named <code>self.memory</code>, constrained by <code>MAX_MEMORY</code>, functions as a replay memory, storing previous experiences crucial for the agent&rsquo;s learning process. Additionally, the <code>self.trainer</code>, instantiated as <code>QTrainer</code>, facilitates model training using the <code>self.model</code>, employing the provided learning rate (<code>ALPHA</code>) and discount factor (<code>gamma</code>) within the <code>QTrainer</code> class.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>self<span style=color:#f92672>.</span>model <span style=color:#f92672>=</span> LinearQNet(<span style=color:#ae81ff>11</span>, <span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>3</span>)  <span style=color:#75715e># Neural network model (input size, hidden size, output size)</span>
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>target_model <span style=color:#f92672>=</span> LinearQNet(<span style=color:#ae81ff>11</span>, <span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>3</span>)
</span></span></code></pre></div><p>The <code>self.model</code> is a neural network structure defined as <code>LinearQNet(11, 256, 3)</code> indicates specific architectural details tailored for a reinforcement learning task. The choice of these parameters signifies the design of the neural network for this particular problem domain.</p><p>The &lsquo;11&rsquo; in the network signifies the input size, representing the number of features or variables characterizing the environment&rsquo;s state, which encompass details directions: vertical and horizontal distance, dangers based on moving forward, left or right and snake and food locations. The &lsquo;256&rsquo; hidden units denote the number of neurons in the hidden layer. This specific number, 256, is a heuristic or empirical choice commonly used across different domains: ranging from generative Convolutional Neural Networks (CNNs) architectures such as AlexNet or VGG to Natural Language Processing (NLPs) Recurrent Neural Network (RNNs) architectures. It provides a moderately sized layer that offers sufficient capacity for learning complex representations from the input data without overly increasing computational costs. Additionally, being a <a href=https://stackoverflow.com/questions/63515846/in-neural-networks-why-conventionally-set-number-of-neurons-to-2n>power of two, it aligns well with computational optimizations and hardware implementations</a>, making it computationally efficient for many systems. Lastly, the &lsquo;3&rsquo; as the output size corresponds to the number of actions the agent can undertake in the environment. In this case, having three outputs suggests that the agent has three distinct possible actions it can choose from in response to a given state in the environment. For instance, these actions represent movements such as &ldquo;left&rdquo;, &ldquo;right&rdquo; and &ldquo;forward&rdquo; seen previously in the development of <code>game.py</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_state</span>(self, game):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Extracting snake&#39;s head position and defining points in different directions</span>
</span></span><span style=display:flex><span>    head <span style=color:#f92672>=</span> game<span style=color:#f92672>.</span>snake[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    point_l <span style=color:#f92672>=</span> Point(head<span style=color:#f92672>.</span>x <span style=color:#f92672>-</span> <span style=color:#ae81ff>20</span>, head<span style=color:#f92672>.</span>y)
</span></span><span style=display:flex><span>    point_r <span style=color:#f92672>=</span> Point(head<span style=color:#f92672>.</span>x <span style=color:#f92672>+</span> <span style=color:#ae81ff>20</span>, head<span style=color:#f92672>.</span>y)
</span></span><span style=display:flex><span>    point_u <span style=color:#f92672>=</span> Point(head<span style=color:#f92672>.</span>x, head<span style=color:#f92672>.</span>y <span style=color:#f92672>-</span> <span style=color:#ae81ff>20</span>)
</span></span><span style=display:flex><span>    point_d <span style=color:#f92672>=</span> Point(head<span style=color:#f92672>.</span>x, head<span style=color:#f92672>.</span>y <span style=color:#f92672>+</span> <span style=color:#ae81ff>20</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Determining the snake&#39;s current direction</span>
</span></span><span style=display:flex><span>    dir_l <span style=color:#f92672>=</span> game<span style=color:#f92672>.</span>direction <span style=color:#f92672>==</span> Direction<span style=color:#f92672>.</span>LEFT
</span></span><span style=display:flex><span>    dir_r <span style=color:#f92672>=</span> game<span style=color:#f92672>.</span>direction <span style=color:#f92672>==</span> Direction<span style=color:#f92672>.</span>RIGHT
</span></span><span style=display:flex><span>    dir_u <span style=color:#f92672>=</span> game<span style=color:#f92672>.</span>direction <span style=color:#f92672>==</span> Direction<span style=color:#f92672>.</span>UP
</span></span><span style=display:flex><span>    dir_d <span style=color:#f92672>=</span> game<span style=color:#f92672>.</span>direction <span style=color:#f92672>==</span> Direction<span style=color:#f92672>.</span>DOWN
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Checking for potential dangers in different directions</span>
</span></span><span style=display:flex><span>    danger_straight <span style=color:#f92672>=</span> (dir_r <span style=color:#f92672>and</span> game<span style=color:#f92672>.</span>is_collision(point_r)) <span style=color:#f92672>or</span> \
</span></span><span style=display:flex><span>                      (dir_l <span style=color:#f92672>and</span> game<span style=color:#f92672>.</span>is_collision(point_l)) <span style=color:#f92672>or</span> \
</span></span><span style=display:flex><span>                      (dir_u <span style=color:#f92672>and</span> game<span style=color:#f92672>.</span>is_collision(point_u)) <span style=color:#f92672>or</span> \
</span></span><span style=display:flex><span>                      (dir_d <span style=color:#f92672>and</span> game<span style=color:#f92672>.</span>is_collision(point_d))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Indicating the snake&#39;s movement direction</span>
</span></span><span style=display:flex><span>    move_direction <span style=color:#f92672>=</span> [int(dir_l), int(dir_r), int(dir_u), int(dir_d)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Determining food&#39;s relative position compared to the snake&#39;s head</span>
</span></span><span style=display:flex><span>    food_position <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>        game<span style=color:#f92672>.</span>food<span style=color:#f92672>.</span>x <span style=color:#f92672>&lt;</span> game<span style=color:#f92672>.</span>head<span style=color:#f92672>.</span>x,  <span style=color:#75715e># food left</span>
</span></span><span style=display:flex><span>        game<span style=color:#f92672>.</span>food<span style=color:#f92672>.</span>x <span style=color:#f92672>&gt;</span> game<span style=color:#f92672>.</span>head<span style=color:#f92672>.</span>x,  <span style=color:#75715e># food right</span>
</span></span><span style=display:flex><span>        game<span style=color:#f92672>.</span>food<span style=color:#f92672>.</span>y <span style=color:#f92672>&lt;</span> game<span style=color:#f92672>.</span>head<span style=color:#f92672>.</span>y,  <span style=color:#75715e># food up</span>
</span></span><span style=display:flex><span>        game<span style=color:#f92672>.</span>food<span style=color:#f92672>.</span>y <span style=color:#f92672>&gt;</span> game<span style=color:#f92672>.</span>head<span style=color:#f92672>.</span>y   <span style=color:#75715e># food down</span>
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Constructing the state representation</span>
</span></span><span style=display:flex><span>    state <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>        int(danger_straight),  <span style=color:#75715e># Danger straight</span>
</span></span><span style=display:flex><span>        int(dir_r), int(dir_l), int(dir_u), int(dir_d),  <span style=color:#75715e># Move direction</span>
</span></span><span style=display:flex><span>    ] <span style=color:#f92672>+</span> food_position  <span style=color:#75715e># Food location</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>array(state, dtype<span style=color:#f92672>=</span>int)
</span></span></code></pre></div><p>The <code>get_state</code> function within the code constructs a comprehensive representation of the game state in the Snake environment. It begins by extracting vital information such as the snake&rsquo;s head position and defining points in multiple directions to detect potential dangers, which include positions 20 units away in various directions. The function then derives the snake&rsquo;s current direction by comparing it with predefined directional indicators (left, right, up, down) based on the game&rsquo;s orientation. It proceeds to assess the presence of <strong>potential dangers in the straight, right, and left directions</strong> by checking for collisions with specific points relative to the snake&rsquo;s current orientation. Furthermore, binary flags are employed to indicate the snake&rsquo;s movement direction, while the relative position of the food compared to the snake&rsquo;s head (left, right, up, down) is determined. Finally, all these features are flushed into an array that serves as a numeric representation of the game state.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_action</span>(self, state):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Select actions based on an epsilon-greedy strategy</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>epsilon <span style=color:#f92672>=</span> <span style=color:#ae81ff>80</span> <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>n_games
</span></span><span style=display:flex><span>        final_move <span style=color:#f92672>=</span> [<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>200</span>) <span style=color:#f92672>&lt;</span> self<span style=color:#f92672>.</span>epsilon:
</span></span><span style=display:flex><span>            move <span style=color:#f92672>=</span> random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>            final_move[move] <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            state0 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(state, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float)
</span></span><span style=display:flex><span>            prediction <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>model(state0)
</span></span><span style=display:flex><span>            move <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>argmax(prediction)<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>            final_move[move] <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> final_move
</span></span></code></pre></div><p>The <code>get_action</code> method operates as the decision-maker, employing an <a href=https://medium.com/@gridflowai/part-2-in-depth-exploration-on-epsilon-greedy-algorithm-2b19e59bbe22>epsilon-greedy strategy</a> to balance exploration and exploitation. This strategy dynamically adjusts the agent&rsquo;s behaviour by modifying the exploration rate (<code>epsilon</code>) based on the number of games played (<code>n_games</code>). If a randomnly generated value falls below the epsilon threshold, indicating exploration, the agent randomly selects an action from the available choices (<strong>move left, right, or straight</strong>). Conversely, in the exploitation phase, when the generated value surpasses the epsilon threshold, the agent exploits its learned knowledge. It leverages its neural network model (<code>self.model</code>) to predict Q-values for each potential action given the current state, selecting the action with the highest predicted Q-value. The resulting one-hot encoded representation (<code>final_move</code>) denotes the chosen action, guiding the agent&rsquo;s movement and decision-making process within the game.</p><h3 id=33-max-replay-buffer-and-target-network>3.3. Max Replay Buffer and Target Network
<a class=anchor href=#33-max-replay-buffer-and-target-network>#</a></h3><p>In reinforcement learning, a replay buffer serves as a memory repository, capturing and retaining past experiences encountered by an agent during its interactions with an environment. This data structure enables the agent to reutilize and learn from diverse historical interactions by storing state-action-reward-next_state tuples. By decoupling the immediate use of experiences and instead sampling randomnly from this stored memory during training, the replay buffer breaks the temporal correlation between consecutive experiences, leading to more stable and efficient learning. Meanwhile, a target network, often employed in algorithms like Deep Q-Networks (DQN), functions as a stabilized reference for target Q-values during training. This secondary neural network provides less frequently updated Q-value targets, addressing the issue of rapidly changing targets and enhancing training stability by decoupling the estimation of target Q-values from the primary network&rsquo;s parameters.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> collections <span style=color:#f92672>import</span> deque
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> random
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>MAX_MEMORY <span style=color:#f92672>=</span> <span style=color:#ae81ff>100_000</span>
</span></span><span style=display:flex><span>BATCH_SIZE <span style=color:#f92672>=</span> <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>QLearningAgent</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Initialize replay buffer</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>memory <span style=color:#f92672>=</span> deque(maxlen<span style=color:#f92672>=</span>MAX_MEMORY)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>remember</span>(self, state, action, reward, next_state, done):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Store experiences in memory</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>memory<span style=color:#f92672>.</span>append((state, action, reward, next_state, done))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_long_memory</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Perform training using experiences from the replay buffer</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> len(self<span style=color:#f92672>.</span>memory) <span style=color:#f92672>&lt;</span> BATCH_SIZE:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        mini_batch <span style=color:#f92672>=</span> random<span style=color:#f92672>.</span>sample(self<span style=color:#f92672>.</span>memory, BATCH_SIZE)
</span></span><span style=display:flex><span>        states, actions, rewards, next_states, dones <span style=color:#f92672>=</span> zip(<span style=color:#f92672>*</span>mini_batch)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>trainer<span style=color:#f92672>.</span>train_step(states, actions, rewards, next_states, dones)
</span></span></code></pre></div><p>The applied methodology enhances stability by mitigating overfitting to recent experiences and improves learning efficiency by allowing the agent to reuse and learn from its past interactions, contributing to more stable and effective training in reinforcement learning algorithms. In the <code>agent.py</code>, particularly in the <code>train_and_record</code> function, the integration of the replay buffer involves augmenting the agent&rsquo;s interactions with the environment to store experiences and utilizing those experiences for training. As the agent interacts with the environment in each game step, the <code>remember</code> function within the <code>QLearningAgent</code> class captures the state-action-reward-next_state tuples and stores them in the replay buffer. They are essential for off-policy learning, and the <code>train_short_memory</code> and <code>train_long_memory</code> functions facilitate short-term and long-term learning, respectively. After each completed game, the agent leverages the stored experiences by calling <code>train_long_memory</code>, which samples a batch of experiences from the replay buffer and uses these experiences to update the agent&rsquo;s model via the <code>train_step</code> method in the <code>QTrainer</code> class. This integration facilitates learning from a diverse set of past interactions, contributing to more stable and efficient training by breaking temporal correlations between consecutive experiences. Adjusting memory replay features in the likes of <code>MAX_MEMORY</code> and <code>BATCH_SIZE</code> in addition to initialization variables allows not only for fine-tuning of the replay buffer&rsquo;s capacity and the size of experiences utilized for training, but also for studying the whole agent&rsquo;s learning process which can be studied using commonly known evolutionary algorithms such as: Genetic Algorithms.</p><h2 id=4-genetic-optimization-of-a-rl-deep-q-network>4. Genetic Optimization of a RL Deep-Q-Network
<a class=anchor href=#4-genetic-optimization-of-a-rl-deep-q-network>#</a></h2><p>In the context of optimizing key parameters for reinforcement learning — such as batch size, learning rate, memory capacity for replay buffers, and the architecture of a target network—genetic algorithms (GAs) provide a systematic approach. To apply GAs in this scenario, the first step involves defining a chromosome that encodes these parameters. For instance, genes within the chromosome can represent batch size, learning rate, memory capacity, and the structure of the target network — specifying inputs, outputs, and hidden layers.</p><h3 id=41-theoretical-background>4.1. Theoretical Background
<a class=anchor href=#41-theoretical-background>#</a></h3><p>The process begins by generating an initial population of diverse chromosomes, each embodying a unique combination of these hyperparameters. These chromosomes are then evaluated through training RL agents, employing the specified parameters within each chromosome, and assessing their performance using a fitness function that measures success in accomplishing RL tasks or objectives.</p><p>The next phase revolves around selection, where high-performing chromosomes are chosen based on their fitness scores. These selected chromosomes undergo crossover and mutation operations, allowing for the creation of offspring that inherit genetic information from their parents—enabling exploration of new hyperparameter combinations.</p><p>In the context of optimizing the target network&rsquo;s structure, the crossover and mutation operations would specifically manipulate genes representing the network&rsquo;s architecture—modifying inputs, outputs, and hidden layers&rsquo; configurations.</p><p>The offspring are then evaluated by training RL agents with their respective hyperparameters. Through this iterative process of selection, crossover, and mutation across multiple generations, the GA systematically refines the population, fine-tuning parameters like batch size, learning rates, memory capacities, and the architecture of the target network.</p><p><img src=https://s5.gifyu.com/images/SiDzw.gif alt=123019></p><h2 id=5-results>5. Results
<a class=anchor href=#5-results>#</a></h2><p><code>(to insert images)</code></p><h2 id=6-outcomes>6. Outcomes
<a class=anchor href=#6-outcomes>#</a></h2></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/roaked/roaked.github.io/commit/38bbd679552638c7d78bf6368cf2a000a2a9beda title='Last modified by roaked | December 26, 2023' target=_blank rel=noopener><img src=/svg/calendar.svg class=book-icon alt=Calendar>
<span>December 26, 2023</span></a></div><div><a class="flex align-center" href=https://github.com/roaked/roaked.github.io/edit/main/content/content/docs/code/snake-game/_index.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Ricardo Chin</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#1-snake-game>1. Snake Game</a></li><li><a href=#2-user-and-ai-controlled-snake>2. User and AI Controlled Snake</a></li><li><a href=#3-reinforcement-deep-q-network-model>3. Reinforcement Deep Q-Network Model</a><ul><li><a href=#31-linearqnet-and-qtrainer-classes>3.1. LinearQNet and QTrainer Classes</a></li><li><a href=#32-deploying-a-reinforcement-learning-agent>3.2. Deploying a Reinforcement Learning Agent</a></li><li><a href=#33-max-replay-buffer-and-target-network>3.3. Max Replay Buffer and Target Network</a></li></ul></li><li><a href=#4-genetic-optimization-of-a-rl-deep-q-network>4. Genetic Optimization of a RL Deep-Q-Network</a><ul><li><a href=#41-theoretical-background>4.1. Theoretical Background</a></li></ul></li><li><a href=#5-results>5. Results</a></li><li><a href=#6-outcomes>6. Outcomes</a></li></ul></nav></div></aside></main></body></html>