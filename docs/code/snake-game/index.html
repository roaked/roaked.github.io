<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Genetic Tuning for Smarter Deep Q-Learning # Working on - some functions might be outdated # note
To do:
Script for recording / saving game states Finish adapting the fitness function of my genetic algorithm Optimize game code for game environment 1. Snake Game # The Snake game has served as a fundamental project for programming novices due to its simplicity and versatility. In this work, Pygame is used, a Python library designed for game development, to create a Snake game environment."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:title" content="Snake Game: Genetic RL-DQN"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://xsleaks.dev/docs/code/snake-game/"><title>Snake Game: Genetic RL-DQN | Ricardo Chin</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png><link rel=stylesheet href=/book.min.b74e85bd7803de00c09a320dcf09ae0d7e37702a9918995f5fe9d1c71c55a223.css integrity="sha256-t06FvXgD3gDAmjINzwmuDX43cCqZGJlfX+nRxxxVoiM=" crossorigin=anonymous><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-10WQY47KS2"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-10WQY47KS2",{anonymize_ip:!1})}</script><link rel=alternate type=application/rss+xml href=https://xsleaks.dev/docs/code/snake-game/index.xml title="Ricardo Chin"></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>Ricardo Chin</span></a></h2><ul><li class=book-section-flat><a href=/docs/design/>Design Repository</a><ul><li><a href=/docs/design/fea-beam-simulation/>FEA Beam Instability Modes</a><ul></ul></li><li><a href=/docs/design/industrial-crane-design/>Industrial Girder Crane</a><ul></ul></li><li><a href=/docs/design/finite-element-method-development/>FEM Package Development</a><ul></ul></li><li><a href=/docs/design/manual-transmission-design/>Gear Train Transmission</a><ul></ul></li><li><a href=/docs/design/electron-beam-tech/>Electron Beam Technology</a><ul></ul></li></ul></li><li class=book-section-flat><a href=/docs/code/>Coding Repository</a><ul><li><a href=/docs/code/uav/>UAV Red Bull Air Racing</a><ul><li><a href=/docs/code/uav/dynamics/>Drone Dynamics</a></li><li><a href=/docs/code/uav/continuous-controller-design/>Drone Continuous Controller</a></li><li><a href=/docs/code/uav/discrete-controller-design/>Drone Discrete Controller</a></li><li><a href=/docs/code/uav/computer-vision/>Drone Computer Vision</a></li></ul></li><li><a href=/docs/code/agv/>AGV: System Identification</a><ul></ul></li><li><a href=/docs/code/deep-learning-fake-news/>Deep Learning on Fake News</a><ul></ul></li><li><a href=/docs/code/hamiltonian-graphs/hamiltonian/>Hamiltonian Graphs: Linked Lists</a></li><li><a href=/docs/code/bin-packing/>EC Optimization: Space & Time</a><ul><li><a href=/docs/code/bin-packing/genetic-algorithm/>Genetic Algorithm</a></li><li><a href=/docs/code/bin-packing/particle-swarm-optimization/>Particle Swarm Optimization</a></li></ul></li><li><a href=/docs/code/snake-game/ class=active>Snake Game: Genetic RL-DQN</a><ul></ul></li><li><a href=/docs/code/pybamm-ml-battery/>PyBaMM-ML EV Battery Status</a><ul></ul></li></ul></li><li class=book-section-flat><a href=/docs/competitions/>Academic Competitions</a><ul><li><a href=/docs/competitions/fst/>Formula Student Lisbon</a></li><li><a href=/docs/competitions/thermocup/>ThermoCup</a></li></ul></li><li class=book-section-flat><a href=/docs/leet/>My LeetCode Solutions</a><ul></ul></li><li class=book-section-flat><a href=/docs/mod/>Website Modifications</a><ul></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Snake Game: Genetic RL-DQN</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#1-snake-game>1. Snake Game</a></li><li><a href=#2-user-controlled-snake>2. User Controlled Snake</a></li><li><a href=#3-reinforcement-deep-q-network-model>3. Reinforcement Deep Q-Network Model</a></li><li><a href=#4-genetic-tuning-of-a-rl-deep-q-network>4. Genetic Tuning of a RL Deep-Q-Network</a></li><li><a href=#5-results>5. Results</a></li><li><a href=#6-outcomes>6. Outcomes</a></li></ul></nav></aside></header><article class=markdown><h1 id=genetic-tuning-for-smarter-deep-q-learning><strong>Genetic Tuning for Smarter Deep Q-Learning</strong>
<a class=anchor href=#genetic-tuning-for-smarter-deep-q-learning>#</a></h1><p><img src=https://miro.medium.com/v2/resize:fit:2800/1*zRZ46MeFZMd5F52CHM6EYA.png alt=213d></p><h1 id=working-on---some-functions-might-be-outdated>Working on - some functions might be outdated
<a class=anchor href=#working-on---some-functions-might-be-outdated>#</a></h1><blockquote class="book-hint2 note"><p class="hint-title note"><svg class="book-icon"><use href="/svg/hint-icons.svg#note-notice"/></svg><span>note</span></p><p>To do:</p><ul><li>Script for recording / saving game states</li><li>Finish adapting the fitness function of my genetic algorithm</li><li>Optimize game code for game environment</li></ul></blockquote><h2 id=1-snake-game>1. Snake Game
<a class=anchor href=#1-snake-game>#</a></h2><p>The Snake game has served as a fundamental project for programming novices due to its simplicity and versatility. In this work, Pygame is used, a Python library designed for game development, to create a Snake game environment. The core motivation is to provide a controlled and adaptable setting for AI development and reinforcement learning combined with metaheuristics.</p><h2 id=2-user-controlled-snake>2. User Controlled Snake
<a class=anchor href=#2-user-controlled-snake>#</a></h2><blockquote class="book-hint2 important"><p class="hint-title important"><svg class="book-icon"><use href="/svg/hint-icons.svg#important-notice"/></svg><span>important</span></p>Most of the code for the game implementation was adapted using Pygame for providing a robust framework for game development in Python. &ndash; <a href=pygame.org/docs>Pygame Documentation</a></blockquote><p>The code begins with initializing Pygame and setting up essential game parameters such as window size, colors, and game speed. Pygame&rsquo;s functionalities are leveraged for window creation, event handling, and display rendering.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> random<span style=color:#f92672>,</span> pygame<span style=color:#f92672>,</span> sys<span style=color:#f92672>,</span> time
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Pygame initialization and window setup</span>
</span></span><span style=display:flex><span>check_errors <span style=color:#f92672>=</span> pygame<span style=color:#f92672>.</span>init()
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> check_errors[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;[!] Had </span><span style=color:#e6db74>{</span>check_errors[<span style=color:#ae81ff>1</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74> errors when initializing game, exiting...&#39;</span>)
</span></span><span style=display:flex><span>    sys<span style=color:#f92672>.</span>exit(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;[+] Game successfully initialized&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Defined other variables here, adapt fontsize, window size, colors, speed, block size, etc...</span>
</span></span></code></pre></div><p>In addition, SnakeGameAI Class is defined which encapsulates the game logic, managing the game state, snake movement, collision detection, and food placement.</p><blockquote class="book-hint2 important"><p class="hint-title important"><svg class="book-icon"><use href="/svg/hint-icons.svg#important-notice"/></svg><span>important</span></p><p>Vital functions encompass:</p><ul><li><code>def __init__</code> &ndash; Initializing game window and start up the game state</li><li><code>def _init_game</code> &ndash; Initialize game state variables (snake position, score, food)</li><li><code>def _place_food</code> &ndash; Places food randomly within the game window, avoiding snake collision</li><li><code>def _move</code> &ndash; Handling snake movement based on user or AI action</li><li><code>def play_step </code>&ndash; Process each step of the game based on user (and later AI actions). (in addition, collects user input, move snake, check collision, update score, etc.)</li></ul></blockquote><p>Given <code>__init__</code> and <code>__init__game</code> functions, the snake is initialized with a starting position, consisting of three body parts (<code>self.head</code> and two segments) positioned horizontally to the right - this is normally executed in good practice for developing a snake game, including representing the <code>self.head</code> at the center of the window. This setup essentially creates an initial length for the snake, allowing it to start the game with a visible length and direction. The snake initially consists of these three segments, and as the game progresses and the snake moves, additional segments will be added or removed based on its movement, food consumption, and collision detection.</p><blockquote class="book-hint2 tip"><p class="hint-title tip"><svg class="book-icon"><use href="/svg/hint-icons.svg#tip-notice"/></svg><span>tip</span></p>Having an initial length provides a visible presence of the snake on the screen, allowing the player or AI agent to see its position and direction from the beginning of the game.</blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_init_game</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Initializing game state variables (snake position, score, food)</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>direction <span style=color:#f92672>=</span> Direction<span style=color:#f92672>.</span>RIGHT
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>head <span style=color:#f92672>=</span> Point(self<span style=color:#f92672>.</span>width <span style=color:#f92672>/</span> <span style=color:#ae81ff>2</span>, self<span style=color:#f92672>.</span>height <span style=color:#f92672>/</span> <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>snake <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>head,
</span></span><span style=display:flex><span>            Point(self<span style=color:#f92672>.</span>head<span style=color:#f92672>.</span>x <span style=color:#f92672>-</span> BLOCK_SIZE, self<span style=color:#f92672>.</span>head<span style=color:#f92672>.</span>y),
</span></span><span style=display:flex><span>            Point(self<span style=color:#f92672>.</span>head<span style=color:#f92672>.</span>x <span style=color:#f92672>-</span> (<span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> BLOCK_SIZE), self<span style=color:#f92672>.</span>head<span style=color:#f92672>.</span>y)
</span></span><span style=display:flex><span>        ]
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>score <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>food <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>frame_iteration <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>_place_food()
</span></span></code></pre></div><p>After snake initialization, the <code>_move</code> function adjusts the snake&rsquo;s direction based on the received action (from user or AI). This is done using a list <code>clock_wise</code> that represents the directional movement of the snake - right, down, left, up (clockwise). It determines the current direction&rsquo;s index and handles changes in direction based on the received action:</p><blockquote class="book-hint2 note"><p class="hint-title note"><svg class="book-icon"><use href="/svg/hint-icons.svg#note-notice"/></svg><span>note</span></p>Case scenarios:
If the action indicates no change [1, 0, 0] -> snake maintains its current direction.
If the action indicates a right turn [0, 1, 0] -> snake updates the direction to the next index in the clockwise order.
If the action indicates a left turn [0, 0, 1] -> snake updates the direction to the previous index in the clockwise order (effectively, anti-clockwise).</blockquote><p>Based on the updated direction, the function calculates the new <code>position (x, y)</code> for the snake&rsquo;s head. It increments or decrements the <code>x</code> or <code>y</code> coordinate of the head based on the direction (right, left, down, up), moving it by <code>BLOCK_SIZE</code>, which represents the size of each snake block.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_move</span>(self, action):
</span></span><span style=display:flex><span>        <span style=color:#75715e># [straight, right, left]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        clock_wise <span style=color:#f92672>=</span> [Direction<span style=color:#f92672>.</span>RIGHT, Direction<span style=color:#f92672>.</span>DOWN, Direction<span style=color:#f92672>.</span>LEFT, Direction<span style=color:#f92672>.</span>UP]
</span></span><span style=display:flex><span>        idx <span style=color:#f92672>=</span> clock_wise<span style=color:#f92672>.</span>index(self<span style=color:#f92672>.</span>direction)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> np<span style=color:#f92672>.</span>array_equal(action, [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>]):
</span></span><span style=display:flex><span>            new_dir <span style=color:#f92672>=</span> clock_wise[idx] <span style=color:#75715e># No change</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>elif</span> np<span style=color:#f92672>.</span>array_equal(action, [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>]):
</span></span><span style=display:flex><span>            next_idx <span style=color:#f92672>=</span> (idx <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>%</span> <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>            new_dir <span style=color:#f92672>=</span> clock_wise[next_idx] <span style=color:#75715e># right turn r -&gt; d -&gt; l -&gt; u</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>: <span style=color:#75715e># [0, 0, 1]</span>
</span></span><span style=display:flex><span>            next_idx <span style=color:#f92672>=</span> (idx <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>%</span> <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>            new_dir <span style=color:#f92672>=</span> clock_wise[next_idx] <span style=color:#75715e># left turn r -&gt; u -&gt; l -&gt; d</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>direction <span style=color:#f92672>=</span> new_dir
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>head<span style=color:#f92672>.</span>x
</span></span><span style=display:flex><span>        y <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>head<span style=color:#f92672>.</span>y
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>direction <span style=color:#f92672>==</span> Direction<span style=color:#f92672>.</span>RIGHT:
</span></span><span style=display:flex><span>            x <span style=color:#f92672>+=</span> BLOCK_SIZE
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>elif</span> self<span style=color:#f92672>.</span>direction <span style=color:#f92672>==</span> Direction<span style=color:#f92672>.</span>LEFT:
</span></span><span style=display:flex><span>            x <span style=color:#f92672>-=</span> BLOCK_SIZE
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>elif</span> self<span style=color:#f92672>.</span>direction <span style=color:#f92672>==</span> Direction<span style=color:#f92672>.</span>DOWN:
</span></span><span style=display:flex><span>            y <span style=color:#f92672>+=</span> BLOCK_SIZE
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>elif</span> self<span style=color:#f92672>.</span>direction <span style=color:#f92672>==</span> Direction<span style=color:#f92672>.</span>UP:
</span></span><span style=display:flex><span>            y <span style=color:#f92672>-=</span> BLOCK_SIZE
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>head <span style=color:#f92672>=</span> Point(x, y)
</span></span></code></pre></div><p>Afterwards, the <code>_place_food</code> function randomnly places food within the game window. It ensures the food doesn&rsquo;t spawn on the snake&rsquo;s body by repositioning it until a valid location is found.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_place_food</span>(self):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, (self<span style=color:#f92672>.</span>width<span style=color:#f92672>-</span>BLOCK_SIZE )<span style=color:#f92672>//</span>BLOCK_SIZE )<span style=color:#f92672>*</span>BLOCK_SIZE
</span></span><span style=display:flex><span>        y <span style=color:#f92672>=</span> random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, (self<span style=color:#f92672>.</span>height<span style=color:#f92672>-</span>BLOCK_SIZE )<span style=color:#f92672>//</span>BLOCK_SIZE )<span style=color:#f92672>*</span>BLOCK_SIZE
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>food <span style=color:#f92672>=</span> Point(x, y)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>food <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>snake:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>_place_food()
</span></span></code></pre></div><p>For defining the game over condition, two possibilities can be addressed in the <code>is_collision</code> function. The first possibility checks whether the specified point (defaulted to <code>self.head</code> if no point is provided) is outside the game window. It compares the <code>x</code> and <code>y</code> coordinates of the point against the window boundaries, considering the size of the game elements (the snake blocks) to ensure they remain within the window. Additionally, the second possibility consideres if the specified point (or <code>self.head</code>) is present within the <code>self.snake</code> list beyond the first element. This effectively checks if the snake&rsquo;s head or a specified point coincides with any part of the snake&rsquo;s body excluding the head. If a collision is detected, it means the snake has collided with itself.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>is_collision</span>(self, pt<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> pt <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            pt <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>head
</span></span><span style=display:flex><span>        <span style=color:#75715e># hits boundary</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> pt<span style=color:#f92672>.</span>x <span style=color:#f92672>&gt;</span> self<span style=color:#f92672>.</span>width <span style=color:#f92672>-</span> BLOCK_SIZE <span style=color:#f92672>or</span> pt<span style=color:#f92672>.</span>x <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>or</span> pt<span style=color:#f92672>.</span>y <span style=color:#f92672>&gt;</span> self<span style=color:#f92672>.</span>height <span style=color:#f92672>-</span> BLOCK_SIZE <span style=color:#f92672>or</span> pt<span style=color:#f92672>.</span>y <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0</span>: 
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># hits itself</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> pt <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>snake[<span style=color:#ae81ff>1</span>:]: 
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>False</span>
</span></span></code></pre></div><p>Lastly, all previous vital functions are combined through the core function <code>play_step</code> which manages the core game loop, processing each step by collecting user input, moving the snake, and updating the game state based on collisions and food consumption & spawning. The <code>_update_ui</code> function refreshes the game display, showing the snake, food, score, and any other visual elements.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>play_step</span>(self, action):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>frame_iteration <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 1. input data from user</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> event <span style=color:#f92672>in</span> pygame<span style=color:#f92672>.</span>event<span style=color:#f92672>.</span>get():
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> event<span style=color:#f92672>.</span>type <span style=color:#f92672>==</span> pygame<span style=color:#f92672>.</span>QUIT:
</span></span><span style=display:flex><span>                pygame<span style=color:#f92672>.</span>quit()
</span></span><span style=display:flex><span>                quit()
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># 2. move</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>_move(action) <span style=color:#75715e># update the head</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>snake<span style=color:#f92672>.</span>insert(<span style=color:#ae81ff>0</span>, self<span style=color:#f92672>.</span>head)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># 3. do we reset? / game over</span>
</span></span><span style=display:flex><span>        reward <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        game_over <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>is_collision() <span style=color:#f92672>or</span> self<span style=color:#f92672>.</span>frame_iteration <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>100</span><span style=color:#f92672>*</span>len(self<span style=color:#f92672>.</span>snake):
</span></span><span style=display:flex><span>            game_over <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>            reward <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> reward, game_over, self<span style=color:#f92672>.</span>score
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 4. place food if eaten, else keep moving</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>head <span style=color:#f92672>==</span> self<span style=color:#f92672>.</span>food:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>score <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>            reward <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>_place_food()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>snake<span style=color:#f92672>.</span>pop()
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># 5. update ui //</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>_update_ui()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>clock<span style=color:#f92672>.</span>tick(SPEED)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 6. return score or game over</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> reward, game_over, self<span style=color:#f92672>.</span>score
</span></span></code></pre></div><blockquote class="book-hint2 warning"><p class="hint-title warning"><svg class="book-icon"><use href="/svg/hint-icons.svg#warning-notice"/></svg><span>warning</span></p><p>The game itself is still under development and some of the functions might have been changed already, but not updated here. Nevertheless, current tasks to further enhance the game environment and codebase consist of:</p><ul><li>Complete User Input Handling: Implement the get_user_input method to capture user actions, enabling manual control of the snake.</li><li>Display Rendering: Develop the _update_display method to visually represent the game state using Pygame&rsquo;s rendering capabilities.</li><li>Enhancements: Explore additional features such as different game difficulty levels, multiplayer functionalities, or advanced AI algorithms to enrich the gaming experience.</li></ul></blockquote><h2 id=3-reinforcement-deep-q-network-model>3. Reinforcement Deep Q-Network Model
<a class=anchor href=#3-reinforcement-deep-q-network-model>#</a></h2><p>(to insert background)</p><p>The <a href=https://github.com/roaked/snake-q-learning-genetic-algorithm/blob/main/model.py>LinearQNet</a> class represents a simple neural network architecture tailored for Q-value approximation in reinforcement learning. It consists of two linear layers initialized during instantiation, with the first layer transforming input features to a hidden layer and the second layer producing Q-values for different actions.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>LinearQNet</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, input_size, hidden_size, output_size):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>linear1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(input_size, hidden_size)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>linear2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(hidden_size, output_size)
</span></span></code></pre></div><p>The forward method defines the forward pass, applying a rectified linear unit (ReLU) activation to the hidden layer&rsquo;s output before generating the Q-values. Additionally, the save method facilitates saving the model&rsquo;s state dictionary <code>self.state_dict()</code> to a specified file path using PyTorch&rsquo;s <code>torch.save</code> functionality, ensuring the preservation of trained model parameters.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>linear1(x))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>linear2(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>save</span>(self, file_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;model.pth&#39;</span>):
</span></span><span style=display:flex><span>        model_folder_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;./model&#39;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(model_folder_path):
</span></span><span style=display:flex><span>            os<span style=color:#f92672>.</span>makedirs(model_folder_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        file_name <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(model_folder_path, file_name)
</span></span><span style=display:flex><span>        torch<span style=color:#f92672>.</span>save(self<span style=color:#f92672>.</span>state_dict(), file_name)    
</span></span></code></pre></div><p>Meanwhile, the QTrainer class manages the training process for the Q-network. Its initialization configures key parameters like learning rate and discount factor, along with setting up an Adam optimizer to update the model&rsquo;s parameters based on the provided learning rate.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>QTrainer</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, model, lr, gamma, target_update_freq<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>lr <span style=color:#f92672>=</span> lr
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>gamma <span style=color:#f92672>=</span> gamma
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>model <span style=color:#f92672>=</span> model
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>optimizer <span style=color:#f92672>=</span> optim<span style=color:#f92672>.</span>Adam(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>lr)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>criterion <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>MSELoss()
</span></span></code></pre></div><p>This can integrated in the train_step function which orchestrates the neural network training process by executing a single iteration of Q-learning updates. It receives an experience tuple containing information about a state, action, received reward, the resulting next state, and an indicator of whether the episode has ended. The function begins by converting these components into <code>PyTorch</code> tensors to facilitate computation within the neural network. Using the provided state, the neural network predicts Q-values for different actions, storing these predictions as <code>pred</code>. Now, we should take a look at how a new Q-value for a state-action pair based on the previous Q-value and the received reward, plus the discounted maximum Q-value achievable in the resulting state is computed. The process for the agent to adjust Q-values iteratively, gradually converging towards optimal action-selection strategies by learning from experiences obtained while interacting with the environment can be seen below for the Q-learning update rule.</p><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\[Q(s,a) = Q(s,a) + /alpha (r + \gamma \cdot max_{a'} Q(s',a')- Q(s,a))\]</span><p>Q(s,a) represents the Q-value, indicating the expected cumulative reward by taking action <code>a</code> in state <code>s</code>. It quantifies the agent&rsquo;s understanding of the long-term desirability of choosing action <code>a</code> while in state <code>s</code>. The evaluation on whether to overwrite old information during upcoming Q-updates is given by the variable <span>\(\alpha\)
</span>. Higher learning rate (<span>
\(\alpha\)
</span>) gives more weight to recent experiences, influencing how much the Q-value changes based on the new piece of information. The agent gets an immediate reward <code>r</code> gained for the action after executing action <code>a</code> at this current state <code>s</code>. This immediate feedback guides the agent&rsquo;s learning, impacting the adjustment of Q-values based on the obtained rewards in each state-action pair. Moreover, the significance of future rewards compared to current rewards is given by <span>\(\gamma\)
</span>. Naturally higher discount factor <span>\(\gamma\)
</span>prioritizes long-term rewards more. Lastly, Q(s&rsquo;,a&rsquo;) indicates the maximum Q-value achievable in the subsequent state <code>s'</code> when taking an action <code>a'</code>. It influences agent&rsquo;s decision-making by taking into account potential future rewards based on the best action available from the next state, guiding him to more rewarding states.</p><p>Employing the Q-learning update rule, the function computes target Q-values based on the predicted values, updating them according to the received rewards and the next state&rsquo;s Q-values if the episode hasn&rsquo;t terminated. Subsequently, it calculates the Mean Squared Error (MSE) loss between the predicted and target Q-values and performs backpropagation to derive gradients for updating the neural network&rsquo;s parameters. Finally, the optimizer applies these gradients to adjust the model&rsquo;s weights through optimization, enhancing the network&rsquo;s ability to approximate accurate Q-values for efficient decision-making in reinforcement learning scenarios.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_step</span>(self, state, action, reward, next_state, done):
</span></span><span style=display:flex><span>        state <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(state, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float)
</span></span><span style=display:flex><span>        next_state <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(next_state, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float)
</span></span><span style=display:flex><span>        action <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(action, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>long)
</span></span><span style=display:flex><span>        reward <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(reward, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        pred <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>model(state)
</span></span><span style=display:flex><span>        target <span style=color:#f92672>=</span> pred<span style=color:#f92672>.</span>clone()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Q-learning update rule</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> len(state<span style=color:#f92672>.</span>shape) <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>: <span style=color:#75715e># (1,x)</span>
</span></span><span style=display:flex><span>            state <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>unsqueeze(state, <span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>            next_state <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>unsqueeze(next_state, <span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>            action <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>unsqueeze(action, <span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>            reward <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>unsqueeze(reward, <span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>            done <span style=color:#f92672>=</span> (done, )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Predicting Q-values based on current state-action pair</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        pred <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>model(state)
</span></span><span style=display:flex><span>        target <span style=color:#f92672>=</span> pred<span style=color:#f92672>.</span>clone()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> idx <span style=color:#f92672>in</span> range(len(done)):
</span></span><span style=display:flex><span>            Q_new <span style=color:#f92672>=</span> reward[idx]
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> done[idx]:
</span></span><span style=display:flex><span>                Q_new <span style=color:#f92672>=</span> reward[idx] <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>gamma <span style=color:#f92672>*</span> torch<span style=color:#f92672>.</span>max(self<span style=color:#f92672>.</span>model(next_state[idx]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            target[idx][torch<span style=color:#f92672>.</span>argmax(action[idx])<span style=color:#f92672>.</span>item()] <span style=color:#f92672>=</span> Q_new
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>        loss <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>criterion(target, pred)
</span></span><span style=display:flex><span>        loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>optimizer<span style=color:#f92672>.</span>step()
</span></span></code></pre></div><p>Together, these classes form the backbone of a Q-learning approach, where the LinearQNet acts as the neural network to estimate Q-values and the QTrainer orchestrates the training process by updating the network&rsquo;s parameters to improve Q-value predictions.</p><blockquote class="book-hint2 warning"><p class="hint-title warning"><svg class="book-icon"><use href="/svg/hint-icons.svg#warning-notice"/></svg><span>warning</span></p><p>Modifications:</p><ul><li>Experiment with deeper networks, different activation functions (Sigmoid, i.e), regularization L2/ Dropout to prevent overfitting (getting stuck) and use different optimizers (RMSprop or SGD) for <code>LinearQClass</code>.</li><li>It is possible to implement different loss functions or Double Q-learning to mitigate bias impact using separate networks to update Q-values.</li><li>Explore how to improve learning efficiency of the agent.</li></ul></blockquote><h2 id=4-genetic-tuning-of-a-rl-deep-q-network>4. Genetic Tuning of a RL Deep-Q-Network
<a class=anchor href=#4-genetic-tuning-of-a-rl-deep-q-network>#</a></h2><p>(to insert background)</p><h2 id=5-results>5. Results
<a class=anchor href=#5-results>#</a></h2><p>(to insert images)</p><h2 id=6-outcomes>6. Outcomes
<a class=anchor href=#6-outcomes>#</a></h2></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/roaked/roaked.github.io/commit/9eec1ca95d4226fe34edc39efad033531563d8ad title='Last modified by roaked | December 16, 2023' target=_blank rel=noopener><img src=/svg/calendar.svg class=book-icon alt=Calendar>
<span>December 16, 2023</span></a></div><div><a class="flex align-center" href=https://github.com/roaked/roaked.github.io/edit/main/content/content/docs/code/snake-game/_index.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Ricardo Chin</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#1-snake-game>1. Snake Game</a></li><li><a href=#2-user-controlled-snake>2. User Controlled Snake</a></li><li><a href=#3-reinforcement-deep-q-network-model>3. Reinforcement Deep Q-Network Model</a></li><li><a href=#4-genetic-tuning-of-a-rl-deep-q-network>4. Genetic Tuning of a RL Deep-Q-Network</a></li><li><a href=#5-results>5. Results</a></li><li><a href=#6-outcomes>6. Outcomes</a></li></ul></nav></div></aside></main></body></html>