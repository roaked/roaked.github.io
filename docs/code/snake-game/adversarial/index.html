<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Adversarial Multi-Agent Reinforcement Learning
  #

This page documents the battle mode implemented in agent_RL.py, where three controllers are compared in the same experiment:



GA-Optimized RL (DQN + genetic hyperparameter updates)




Standard RL (baseline DQN)




Hamiltonian (deterministic cycle-following policy)



Unlike single-agent training, this setup emphasizes relative performance under shared constraints, direct competition, and robustness over many rounds.

  1 Multi-Agent Setup
  #

The adversarial environment is built around SnakeBattleArena, which places all three snakes in one board and advances them in lockstep each frame."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://xsleaks.dev/docs/code/snake-game/adversarial/"><meta property="og:site_name" content="Ricardo Chin"><meta property="og:title" content="Adversarial MultiAgent-RL, BattleMode"><meta property="og:description" content="Adversarial Multi-Agent Reinforcement Learning # This page documents the battle mode implemented in agent_RL.py, where three controllers are compared in the same experiment:
GA-Optimized RL (DQN + genetic hyperparameter updates) Standard RL (baseline DQN) Hamiltonian (deterministic cycle-following policy) Unlike single-agent training, this setup emphasizes relative performance under shared constraints, direct competition, and robustness over many rounds.
1 Multi-Agent Setup # The adversarial environment is built around SnakeBattleArena, which places all three snakes in one board and advances them in lockstep each frame."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2026-02-26T14:49:38+01:00"><title>Adversarial MultiAgent-RL, BattleMode | Ricardo Chin</title><link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png><link rel=stylesheet href=/book.min.6e4d08d120eef807bed2ef1c891443c8cac81abb054e9afbe962c4da325c5c2b.css integrity="sha256-bk0I0SDu+Ae+0u8ciRRDyMrIGrsFTpr76WLE2jJcXCs=" crossorigin=anonymous><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>Ricardo Chin</span></a></h2><ul><li class=book-section-flat><a href=/docs/design/>Engineering Repository</a><ul><li><a href=/docs/design/fea-beam-simulation/>FEA Beam Instability Modes</a><ul></ul></li><li><a href=/docs/design/finite-element-method-development/>FEM Package Development</a><ul></ul></li></ul></li><li class=book-section-flat><a href=/docs/code/>System Repository</a><ul><li><a href=/docs/code/agv/>AGV: Stochastic Identification</a><ul></ul></li><li><a href=/docs/code/uav/>UAV: Red Bull Air Racing</a><ul><li><a href=/docs/code/uav/dynamics/>Drone System Dynamics</a></li><li><a href=/docs/code/uav/continuous-controller-design/>Continuous Controller</a></li><li><a href=/docs/code/uav/discrete-controller-design/>Discrete Controller</a></li><li><a href=/docs/code/uav/computer-vision/>Gate Sense: Computer Vision</a></li></ul></li><li><a href=/docs/code/robotics/>Robotics: Kin, Dynamics & Control</a><ul></ul></li><li><a href=/docs/code/deep-learning-fake-news/>Fake News: Inference & Clusters</a><ul></ul></li><li><a href=/docs/code/bin-packing/>EC Optimization: Space & Time</a><ul><li><a href=/docs/code/bin-packing/genetic-algorithm/>Genetic Algorithm</a></li><li><a href=/docs/code/bin-packing/particle-swarm-optimization/>Particle Swarm Optimization</a></li></ul></li><li><a href=/docs/code/snake-game/>Snake: Reinforcement Learning</a><ul><li><a href=/docs/code/snake-game/deepqnetwork/>BaseRL, GA-RL & Hamiltonian</a></li><li><a href=/docs/code/snake-game/adversarial/ class=active>Adversarial MultiAgent-RL, BattleMode</a></li></ul></li></ul></li><li class=book-section-flat><a href=/docs/lectures/>My Notes and Lectures</a><ul><li><a href=/docs/lectures/bayesian-machine-learning/bayes/>Bayesian Inference</a></li><li><a href=/docs/lectures/bayesian-optimization/bayesopt/>Bayesian Optimization</a></li><li><a href=/docs/lectures/nonlinear-pro/nonlinear/>Nonlinear Programming</a></li><li><a href=/docs/lectures/hamiltonian-graphs/hamiltonian/>Fortran: Linked Lists</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Adversarial MultiAgent-RL, BattleMode</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#1-multi-agent-setup>1 Multi-Agent Setup</a><ul><li><a href=#11-arena-initialization>1.1. Arena initialization</a></li><li><a href=#12-per-round-spawn-state>1.2. Per-round spawn state</a></li></ul></li><li><a href=#2-shared-battle-dynamics>2 Shared-Battle Dynamics</a><ul><li><a href=#21-joint-action-step>2.1. Joint action step</a></li><li><a href=#22-collision-and-reward-rules>2.2. Collision and reward rules</a></li><li><a href=#23-round-termination-and-winner>2.3. Round termination and winner</a></li></ul></li><li><a href=#3-model-specific-behavior-inside-battle>3 Model-Specific Behavior Inside Battle</a><ul><li><a href=#31-standard-rl-agent>3.1. Standard RL agent</a></li><li><a href=#32-ga-optimized-rl-agent>3.2. GA-optimized RL agent</a></li><li><a href=#33-hamiltonian-controller>3.3. Hamiltonian controller</a></li></ul></li><li><a href=#4-training-and-evaluation-flow>4 Training and Evaluation Flow</a></li><li><a href=#5-running-the-battle-modes>5 Running the Battle Modes</a><ul><li><a href=#51-same-board-adversarial-battle-direct-interaction>5.1. Same-board adversarial battle (direct interaction)</a></li><li><a href=#52-side-by-side-comparison-mode-parallel-non-interacting-boards>5.2. Side-by-side comparison mode (parallel, non-interacting boards)</a></li></ul></li><li><a href=#6-interpretation-notes>6 Interpretation Notes</a></li></ul></nav></aside></header><article class=markdown><h1 id=adversarial-multi-agent-reinforcement-learning><strong>Adversarial Multi-Agent Reinforcement Learning</strong>
<a class=anchor href=#adversarial-multi-agent-reinforcement-learning>#</a></h1><p>This page documents the <strong>battle mode</strong> implemented in <a href=https://github.com/roaked/snake-q-learning-genetic-algorithm/blob/main/agent_RL.py><code>agent_RL.py</code></a>, where three controllers are compared in the same experiment:</p><ul><li><ol><li><strong>GA-Optimized RL</strong> (DQN + genetic hyperparameter updates)</li></ol></li><li><ol start=2><li><strong>Standard RL</strong> (baseline DQN)</li></ol></li><li><ol start=3><li><strong>Hamiltonian</strong> (deterministic cycle-following policy)</li></ol></li></ul><p>Unlike single-agent training, this setup emphasizes relative performance under shared constraints, direct competition, and robustness over many rounds.</p><h2 id=1-multi-agent-setup>1 Multi-Agent Setup
<a class=anchor href=#1-multi-agent-setup>#</a></h2><p>The adversarial environment is built around <code>SnakeBattleArena</code>, which places all three snakes in one board and advances them in lockstep each frame.</p><h3 id=11-arena-initialization>1.1. Arena initialization
<a class=anchor href=#11-arena-initialization>#</a></h3><p>The arena tracks per-agent state (<code>snake</code>, <code>direction</code>, <code>alive</code>, <code>scores</code>) and global round statistics (<code>wins</code>, <code>round</code>):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>SnakeBattleArena</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, width<span style=color:#f92672>=</span><span style=color:#ae81ff>640</span>, height<span style=color:#f92672>=</span><span style=color:#ae81ff>640</span>, block_size<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>, speed<span style=color:#f92672>=</span><span style=color:#ae81ff>220</span>):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>width <span style=color:#f92672>=</span> width
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>height <span style=color:#f92672>=</span> height
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>block_size <span style=color:#f92672>=</span> block_size
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>speed <span style=color:#f92672>=</span> speed
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>wins <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;ga&#39;</span>: <span style=color:#ae81ff>0</span>, <span style=color:#e6db74>&#39;rl&#39;</span>: <span style=color:#ae81ff>0</span>, <span style=color:#e6db74>&#39;ham&#39;</span>: <span style=color:#ae81ff>0</span>}
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>round <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>_reset_round()
</span></span></code></pre></div><h3 id=12-per-round-spawn-state>1.2. Per-round spawn state
<a class=anchor href=#12-per-round-spawn-state>#</a></h3><p>At the beginning of each round, each model starts from a distinct position and heading so no method receives positional advantage by default:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>self<span style=color:#f92672>.</span>snakes <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;ga&#39;</span>: [Point(b <span style=color:#f92672>*</span> <span style=color:#ae81ff>5</span>, b <span style=color:#f92672>*</span> <span style=color:#ae81ff>5</span>), Point(b <span style=color:#f92672>*</span> <span style=color:#ae81ff>4</span>, b <span style=color:#f92672>*</span> <span style=color:#ae81ff>5</span>), Point(b <span style=color:#f92672>*</span> <span style=color:#ae81ff>3</span>, b <span style=color:#f92672>*</span> <span style=color:#ae81ff>5</span>)],
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;rl&#39;</span>: [Point(self<span style=color:#f92672>.</span>width <span style=color:#f92672>-</span> b <span style=color:#f92672>*</span> <span style=color:#ae81ff>6</span>, b <span style=color:#f92672>*</span> <span style=color:#ae81ff>5</span>), Point(self<span style=color:#f92672>.</span>width <span style=color:#f92672>-</span> b <span style=color:#f92672>*</span> <span style=color:#ae81ff>5</span>, b <span style=color:#f92672>*</span> <span style=color:#ae81ff>5</span>), Point(self<span style=color:#f92672>.</span>width <span style=color:#f92672>-</span> b <span style=color:#f92672>*</span> <span style=color:#ae81ff>4</span>, b <span style=color:#f92672>*</span> <span style=color:#ae81ff>5</span>)],
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;ham&#39;</span>: [Point(cx, self<span style=color:#f92672>.</span>height <span style=color:#f92672>-</span> b <span style=color:#f92672>*</span> <span style=color:#ae81ff>6</span>), Point(cx, self<span style=color:#f92672>.</span>height <span style=color:#f92672>-</span> b <span style=color:#f92672>*</span> <span style=color:#ae81ff>5</span>), Point(cx, self<span style=color:#f92672>.</span>height <span style=color:#f92672>-</span> b <span style=color:#f92672>*</span> <span style=color:#ae81ff>4</span>)],
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>directions <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;ga&#39;</span>: Direction<span style=color:#f92672>.</span>RIGHT, <span style=color:#e6db74>&#39;rl&#39;</span>: Direction<span style=color:#f92672>.</span>LEFT, <span style=color:#e6db74>&#39;ham&#39;</span>: Direction<span style=color:#f92672>.</span>UP}
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>alive <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;ga&#39;</span>: <span style=color:#66d9ef>True</span>, <span style=color:#e6db74>&#39;rl&#39;</span>: <span style=color:#66d9ef>True</span>, <span style=color:#e6db74>&#39;ham&#39;</span>: <span style=color:#66d9ef>True</span>}
</span></span></code></pre></div><h2 id=2-shared-battle-dynamics>2 Shared-Battle Dynamics
<a class=anchor href=#2-shared-battle-dynamics>#</a></h2><p>Each frame, all alive snakes propose actions, then the arena resolves movement, collisions, rewards, food placement, and round termination.</p><h3 id=21-joint-action-step>2.1. Joint action step
<a class=anchor href=#21-joint-action-step>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>ga_action <span style=color:#f92672>=</span> ga_agent<span style=color:#f92672>.</span>get_action(ga_state_old) <span style=color:#66d9ef>if</span> arena<span style=color:#f92672>.</span>alive[<span style=color:#e6db74>&#39;ga&#39;</span>] <span style=color:#66d9ef>else</span> [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>rl_action <span style=color:#f92672>=</span> rl_agent<span style=color:#f92672>.</span>get_action(rl_state_old) <span style=color:#66d9ef>if</span> arena<span style=color:#f92672>.</span>alive[<span style=color:#e6db74>&#39;rl&#39;</span>] <span style=color:#66d9ef>else</span> [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>ham_action <span style=color:#f92672>=</span> _hamiltonian_move(arena<span style=color:#f92672>.</span>get_view(<span style=color:#e6db74>&#39;ham&#39;</span>), next_lookup, prev_lookup) <span style=color:#66d9ef>if</span> arena<span style=color:#f92672>.</span>alive[<span style=color:#e6db74>&#39;ham&#39;</span>] <span style=color:#66d9ef>else</span> [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>rewards, round_done <span style=color:#f92672>=</span> arena<span style=color:#f92672>.</span>step({<span style=color:#e6db74>&#39;ga&#39;</span>: ga_action, <span style=color:#e6db74>&#39;rl&#39;</span>: rl_action, <span style=color:#e6db74>&#39;ham&#39;</span>: ham_action})
</span></span></code></pre></div><p>This enforces synchronized progression: each policy acts under the same food location, board state evolution, and frame timing.</p><h3 id=22-collision-and-reward-rules>2.2. Collision and reward rules
<a class=anchor href=#22-collision-and-reward-rules>#</a></h3><p>The arena penalizes elimination and rewards food capture:</p><ul><li>Collision (wall/body/head conflict): reward <code>-10</code></li><li>Food consumed: reward <code>+10</code></li><li>Safe step without food: reward <code>0</code></li></ul><p>Representative logic:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>is_collision(snake_id, new_head):
</span></span><span style=display:flex><span>    dead<span style=color:#f92672>.</span>add(snake_id)
</span></span><span style=display:flex><span><span style=color:#f92672>...</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> snake_id <span style=color:#f92672>in</span> dead:
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>alive[snake_id] <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>    rewards[snake_id] <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span><span style=color:#f92672>...</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> new_heads[snake_id] <span style=color:#f92672>==</span> self<span style=color:#f92672>.</span>food:
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>scores[snake_id] <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    rewards[snake_id] <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
</span></span></code></pre></div><h3 id=23-round-termination-and-winner>2.3. Round termination and winner
<a class=anchor href=#23-round-termination-and-winner>#</a></h3><p>A round finishes when one or zero snakes remain alive, or when a frame cap is reached:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>alive_ids <span style=color:#f92672>=</span> [sid <span style=color:#66d9ef>for</span> sid, alive <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>alive<span style=color:#f92672>.</span>items() <span style=color:#66d9ef>if</span> alive]
</span></span><span style=display:flex><span>round_done <span style=color:#f92672>=</span> len(alive_ids) <span style=color:#f92672>&lt;=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>or</span> self<span style=color:#f92672>.</span>frame_iteration <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>200</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> round_done <span style=color:#f92672>and</span> len(alive_ids) <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>:
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>wins[alive_ids[<span style=color:#ae81ff>0</span>]] <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span></code></pre></div><h2 id=3-model-specific-behavior-inside-battle>3 Model-Specific Behavior Inside Battle
<a class=anchor href=#3-model-specific-behavior-inside-battle>#</a></h2><h3 id=31-standard-rl-agent>3.1. Standard RL agent
<a class=anchor href=#31-standard-rl-agent>#</a></h3><p>The baseline RL snake follows the same DQN update cycle as in the single-agent setting:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>rl_state_old <span style=color:#f92672>=</span> rl_agent<span style=color:#f92672>.</span>get_state(rl_view_old)
</span></span><span style=display:flex><span>rl_action <span style=color:#f92672>=</span> rl_agent<span style=color:#f92672>.</span>get_action(rl_state_old)
</span></span><span style=display:flex><span><span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>rl_agent<span style=color:#f92672>.</span>train_short_memory(rl_state_old, rl_action, rewards[<span style=color:#e6db74>&#39;rl&#39;</span>], rl_state_new, rl_done)
</span></span><span style=display:flex><span>rl_agent<span style=color:#f92672>.</span>remember(rl_state_old, rl_action, rewards[<span style=color:#e6db74>&#39;rl&#39;</span>], rl_state_new, rl_done)
</span></span></code></pre></div><p>This gives a direct baseline for learned behavior under adversarial pressure.</p><h3 id=32-ga-optimized-rl-agent>3.2. GA-optimized RL agent
<a class=anchor href=#32-ga-optimized-rl-agent>#</a></h3><p>The GA-controlled snake also learns via DQN, but periodically updates training hyperparameters using genetic search:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>_, best_params, _ <span style=color:#f92672>=</span> ga_genetic<span style=color:#f92672>.</span>genetic(
</span></span><span style=display:flex><span>    NUM_GENERATIONS,
</span></span><span style=display:flex><span>    score<span style=color:#f92672>=</span>ga_score,
</span></span><span style=display:flex><span>    record<span style=color:#f92672>=</span>ga_record,
</span></span><span style=display:flex><span>    steps<span style=color:#f92672>=</span>arena<span style=color:#f92672>.</span>frame_iteration,
</span></span><span style=display:flex><span>    collisions<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>    same_positions_counter<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>    game_metrics_list<span style=color:#f92672>=</span>ga_metrics
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>ga_agent<span style=color:#f92672>.</span>update_hyperparameters(ga_genetic<span style=color:#f92672>.</span>get_current_parameters())
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> completed_rounds <span style=color:#f92672>%</span> <span style=color:#ae81ff>20</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>and</span> isinstance(best_params, dict):
</span></span><span style=display:flex><span>    ga_agent<span style=color:#f92672>.</span>update_hyperparameters(best_params)
</span></span></code></pre></div><p>This allows GA-RL to adapt exploration and learning behavior while competing.</p><h3 id=33-hamiltonian-controller>3.3. Hamiltonian controller
<a class=anchor href=#33-hamiltonian-controller>#</a></h3><p>The Hamiltonian snake does not train; it computes deterministic moves from cycle neighbors:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>ham_action <span style=color:#f92672>=</span> _hamiltonian_move(arena<span style=color:#f92672>.</span>get_view(<span style=color:#e6db74>&#39;ham&#39;</span>), next_lookup, prev_lookup)
</span></span></code></pre></div><p>Its strength is consistency and survival; its weakness is reduced tactical flexibility for short-term food races.</p><h2 id=4-training-and-evaluation-flow>4 Training and Evaluation Flow
<a class=anchor href=#4-training-and-evaluation-flow>#</a></h2><p>At round end, both learning agents perform long-memory updates, while Hamiltonian only resets game state:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>if</span> round_done:
</span></span><span style=display:flex><span>    completed_rounds <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    ga_agent<span style=color:#f92672>.</span>n_games <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    rl_agent<span style=color:#f92672>.</span>n_games <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    ga_agent<span style=color:#f92672>.</span>train_long_memory()
</span></span><span style=display:flex><span>    rl_agent<span style=color:#f92672>.</span>train_long_memory()
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>    arena<span style=color:#f92672>.</span>_reset_round()
</span></span></code></pre></div><p>This structure supports two key measurements:</p><ul><li><strong>Win rate</strong> (<code>arena.wins</code>) as direct adversarial outcome</li><li><strong>Score trends</strong> per model for efficiency and consistency</li></ul><h2 id=5-running-the-battle-modes>5 Running the Battle Modes
<a class=anchor href=#5-running-the-battle-modes>#</a></h2><h3 id=51-same-board-adversarial-battle-direct-interaction>5.1. Same-board adversarial battle (direct interaction)
<a class=anchor href=#51-same-board-adversarial-battle-direct-interaction>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>python agent_RL.py battle
</span></span></code></pre></div><p>Aliases supported by the CLI: <code>arena</code>, <code>fight</code>, <code>vs</code>.</p><h3 id=52-side-by-side-comparison-mode-parallel-non-interacting-boards>5.2. Side-by-side comparison mode (parallel, non-interacting boards)
<a class=anchor href=#52-side-by-side-comparison-mode-parallel-non-interacting-boards>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>python agent_RL.py compare
</span></span></code></pre></div><p>Alias: <code>all3</code>.</p><h2 id=6-interpretation-notes>6 Interpretation Notes
<a class=anchor href=#6-interpretation-notes>#</a></h2><p>When reading results, compare all three dimensions together:</p><ul><li><strong>Wins</strong>: who survives and closes rounds.</li><li><strong>Score</strong>: who collects food most effectively.</li><li><strong>Stability</strong>: variance across many rounds/games.</li></ul><p>In practice, this adversarial page complements the single-model analysis by showing how each policy behaves when exposed to a more competitive and dynamic evaluation protocol.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/roaked/roaked.github.io/commit/8e3b1ce23c2ebf6c23d02591c8a9961b1c950674 title='Last modified by roaked | February 26, 2026' target=_blank rel=noopener><img src=/svg/calendar.svg class=book-icon alt=Calendar>
<span>February 26, 2026</span></a></div><div><a class="flex align-center" href=https://github.com/roaked/roaked.github.io/edit/main/content/content/docs/code/snake-game/adversarial.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Ricardo Chin</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#1-multi-agent-setup>1 Multi-Agent Setup</a><ul><li><a href=#11-arena-initialization>1.1. Arena initialization</a></li><li><a href=#12-per-round-spawn-state>1.2. Per-round spawn state</a></li></ul></li><li><a href=#2-shared-battle-dynamics>2 Shared-Battle Dynamics</a><ul><li><a href=#21-joint-action-step>2.1. Joint action step</a></li><li><a href=#22-collision-and-reward-rules>2.2. Collision and reward rules</a></li><li><a href=#23-round-termination-and-winner>2.3. Round termination and winner</a></li></ul></li><li><a href=#3-model-specific-behavior-inside-battle>3 Model-Specific Behavior Inside Battle</a><ul><li><a href=#31-standard-rl-agent>3.1. Standard RL agent</a></li><li><a href=#32-ga-optimized-rl-agent>3.2. GA-optimized RL agent</a></li><li><a href=#33-hamiltonian-controller>3.3. Hamiltonian controller</a></li></ul></li><li><a href=#4-training-and-evaluation-flow>4 Training and Evaluation Flow</a></li><li><a href=#5-running-the-battle-modes>5 Running the Battle Modes</a><ul><li><a href=#51-same-board-adversarial-battle-direct-interaction>5.1. Same-board adversarial battle (direct interaction)</a></li><li><a href=#52-side-by-side-comparison-mode-parallel-non-interacting-boards>5.2. Side-by-side comparison mode (parallel, non-interacting boards)</a></li></ul></li><li><a href=#6-interpretation-notes>6 Interpretation Notes</a></li></ul></nav></div></aside></main></body></html>