<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Off Policy and DQN Parameter Optimization # 1 Reinforcement Deep Q-Network Architecture # DQN (Deep Q-Network) stands as a RL algorithm rooted in deep learning principles. It integrates a Q-Learning algorithm with a deep neural network to address RL challenges in expansive state and action spaces. In the DQN algorithm, a neural network is employed to approximate the Q function, where states and actions serve as inputs, yielding corresponding Q values as outputs."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:title" content="Off Policy RL & Neuroevolution"><meta property="og:description" content="Off Policy and DQN Parameter Optimization # 1 Reinforcement Deep Q-Network Architecture # DQN (Deep Q-Network) stands as a RL algorithm rooted in deep learning principles. It integrates a Q-Learning algorithm with a deep neural network to address RL challenges in expansive state and action spaces. In the DQN algorithm, a neural network is employed to approximate the Q function, where states and actions serve as inputs, yielding corresponding Q values as outputs."><meta property="og:type" content="article"><meta property="og:url" content="https://xsleaks.dev/docs/code/snake-game/deepqnetwork/"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2024-02-23T14:05:36+01:00"><title>Off Policy RL & Neuroevolution | Ricardo Chin</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png><link rel=stylesheet href=/book.min.b74e85bd7803de00c09a320dcf09ae0d7e37702a9918995f5fe9d1c71c55a223.css integrity="sha256-t06FvXgD3gDAmjINzwmuDX43cCqZGJlfX+nRxxxVoiM=" crossorigin=anonymous><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>Ricardo Chin</span></a></h2><ul><li class=book-section-flat><a href=/docs/design/>Design Repository</a><ul><li><a href=/docs/design/fea-beam-simulation/>FEA Beam Instability Modes</a><ul></ul></li><li><a href=/docs/design/industrial-crane-design/>Industrial Girder Crane</a><ul></ul></li><li><a href=/docs/design/finite-element-method-development/>FEM Package Development</a><ul></ul></li><li><a href=/docs/design/manual-transmission-design/>Gear Train Transmission</a><ul></ul></li></ul></li><li class=book-section-flat><a href=/docs/code/>System Repository</a><ul><li><a href=/docs/code/agv/>AGV: Stochastic Identification</a><ul></ul></li><li><a href=/docs/code/uav/>UAV: Red Bull Air Racing</a><ul><li><a href=/docs/code/uav/dynamics/>Drone System Dynamics</a></li><li><a href=/docs/code/uav/continuous-controller-design/>Continuous Controller</a></li><li><a href=/docs/code/uav/discrete-controller-design/>Discrete Controller</a></li><li><a href=/docs/code/uav/computer-vision/>Gate Sense: Computer Vision</a></li></ul></li><li><a href=/docs/code/robotics/>Robotics: Kin, Dynamics & Control</a><ul></ul></li><li><a href=/docs/code/deep-learning-fake-news/>Fake News: Inference & Clusters</a><ul></ul></li><li><a href=/docs/code/bin-packing/>EC Optimization: Space & Time</a><ul><li><a href=/docs/code/bin-packing/genetic-algorithm/>Genetic Algorithm</a></li><li><a href=/docs/code/bin-packing/particle-swarm-optimization/>Particle Swarm Optimization</a></li></ul></li><li><a href=/docs/code/snake-game/>Snake: Reinforcement Learning</a><ul><li><a href=/docs/code/snake-game/deepqnetwork/ class=active>Off Policy RL & Neuroevolution</a></li><li><a href=/docs/code/snake-game/adversarial/>Adversarial Multi-Agent RL</a></li></ul></li></ul></li><li class=book-section-flat><a href=/docs/lectures/>My Notes and Lectures</a><ul><li><a href=/docs/lectures/bayesian-machine-learning/bayes/>Bayesian Inference</a></li><li><a href=/docs/lectures/bayesian-optimization/bayesopt/>Bayesian Optimization</a></li><li><a href=/docs/lectures/nonlinear-opt/nonlinear/>Nonlinear Optimization</a></li></ul></li><li class=book-section-flat><a href=/docs/competitions/>Academic Competitions</a><ul><li><a href=/docs/competitions/fst/>Formula Student Lisbon</a></li><li><a href=/docs/competitions/thermocup/>ThermoCup</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Off Policy RL & Neuroevolution</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#1-reinforcement-deep-q-network-architecture>1 Reinforcement Deep Q-Network Architecture</a><ul><li><a href=#11-linearqnet-and-qtrainer-classes>1.1. LinearQNet and QTrainer Classes</a></li><li><a href=#12-deploying-a-reinforcement-learning-agent>1.2. Deploying a Reinforcement Learning Agent</a></li><li><a href=#13-max-replay-buffer-and-target-network>1.3. Max Replay Buffer and Target Network</a></li></ul></li><li><a href=#2-genetic-optimization-of-a-rl-deep-q-network>2 Genetic Optimization of a RL Deep-Q-Network</a><ul><li><a href=#21-parameter-space-definition>2.1. Parameter Space Definition</a></li><li><a href=#22-genetic-algorithm>2.2. Genetic Algorithm</a></li></ul></li><li><a href=#3-results>3 Results</a></li><li><a href=#4-outcomes>4 Outcomes</a></li></ul></nav></aside></header><article class=markdown><h1 id=off-policy-and-dqn-parameter-optimization><strong>Off Policy and DQN Parameter Optimization</strong>
<a class=anchor href=#off-policy-and-dqn-parameter-optimization>#</a></h1><p><img src=https://miro.medium.com/v2/resize:fit:1200/1*zRZ46MeFZMd5F52CHM6EYA.png alt=sidas></p><h2 id=1-reinforcement-deep-q-network-architecture>1 Reinforcement Deep Q-Network Architecture
<a class=anchor href=#1-reinforcement-deep-q-network-architecture>#</a></h2><p>DQN (Deep Q-Network) stands as a RL algorithm rooted in deep learning principles. It integrates a Q-Learning algorithm with a deep neural network to address RL challenges in expansive state and action spaces. In the DQN algorithm, a neural network is employed to approximate the Q function, where states and actions serve as inputs, yielding corresponding Q values as outputs. Essentially, the DQN algorithm can be broken down into the following steps:</p><ul><li><ol><li><strong>Initialization</strong>: Initialize a neural network with random weights θ to approximate the Q function Q(s,a,θ).</li></ol></li><li><ol start=2><li><strong>Experience Replay</strong>: Maintain a replay memory buffer
<link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\(\mathcal{D}\)
</span>to store experiences <span>\(e_t = (s_t, a_t, r_t, s_{t+1})\)
</span>consisting of state-action pairs at state t, rewards, and subsequent states encountered during interactions with the environment.</li></ol></li><li><ol start=3><li><strong>Action Selection</strong>: Utilize an epsilon-greedy policy to select actions based on the current state, balancing between exploration and exploitation.</li></ol></li></ul><span>\[a_t = \begin{cases}
\text{random action} & \text{with probability } \epsilon \\
\text{argmax}_a Q(s_t, a, \theta) & \text{otherwise}
\end{cases}\]</span><ul><li><ol start=4><li><strong>Training</strong>: Periodically sample batches of experiences from the replay memory buffer to update the neural network&rsquo;s parameters. The loss function used for training is typically derived from the Bellman equation, aiming to minimize the discrepancy between the predicted Q values and the target Q values.</li></ol></li></ul><span>\[\mathcal{L}(\theta) = \mathbb{E}_{(s, a, r, s') \sim U(D)} \left[ \left( r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a, \theta) \right)^2 \right]\]</span><ul><li><ol start=5><li><strong>Repeat</strong>: Iterate step 3 and 4 until convergence, continually updating the neural network to better approximate the Q function.</li></ol></li></ul><p>Overall, the DQN algorithm leverages deep learning techniques to efficiently handle RL problems characterized by large state and action spaces, enabling effective learning and decision-making in complex environments.</p><h3 id=11-linearqnet-and-qtrainer-classes>1.1. LinearQNet and QTrainer Classes
<a class=anchor href=#11-linearqnet-and-qtrainer-classes>#</a></h3><p>The <a href=https://github.com/roaked/snake-q-learning-genetic-algorithm/blob/main/model.py>LinearQNet</a> class represents a simple neural network architecture tailored for Q-value approximation in RL. It consists of two linear layers initialized during instantiation, with the first layer transforming input features to a hidden layer and the second layer producing Q-values for different actions. Additionally, it sets up other optional components, such as dropout regularization or weight initialization techniques, aiming to enhance the network&rsquo;s learning process and generalization ability.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> __init__(self, input_size, hidden_size, output_size):
</span></span><span style=display:flex><span>    super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>linear1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(input_size, hidden_size)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>linear2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(hidden_size, output_size)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>dropout <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dropout(<span style=color:#ae81ff>0.2</span>)  <span style=color:#75715e># Example: Adding dropout with p=0.2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Initialize weights using Xavier initialization</span>
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>xavier_uniform_(self<span style=color:#f92672>.</span>linear1<span style=color:#f92672>.</span>weight)
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>xavier_uniform_(self<span style=color:#f92672>.</span>linear2<span style=color:#f92672>.</span>weight)
</span></span></code></pre></div><p>The forward method defines the forward pass, applying a rectified linear unit (ReLU) activation to the hidden layer&rsquo;s output before generating the Q-values. Additionally, the save method facilitates saving the model&rsquo;s state dictionary <code>self.state_dict()</code> to a specified file path using PyTorch&rsquo;s <code>torch.save</code> functionality, ensuring the preservation of trained model parameters.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>linear1(x))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>dropout(x)  <span style=color:#75715e># Applying dropout</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>linear2(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>save</span>(self, file_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;model.pth&#39;</span>):
</span></span><span style=display:flex><span>    model_folder_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;./model&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(model_folder_path):
</span></span><span style=display:flex><span>        os<span style=color:#f92672>.</span>makedirs(model_folder_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    file_name <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(model_folder_path, file_name)
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>save(self<span style=color:#f92672>.</span>state_dict(), file_name)    
</span></span></code></pre></div><p>Meanwhile, the QTrainer class manages the training process for the Q-network. Its initialization configures key parameters like learning rate and discount factor, along with setting up an Adam optimizer to update the model&rsquo;s parameters based on the provided learning rate.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>QTrainer</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, model, lr, gamma, target_update_freq<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>lr <span style=color:#f92672>=</span> lr
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>gamma <span style=color:#f92672>=</span> gamma
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>model <span style=color:#f92672>=</span> model
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>optimizer <span style=color:#f92672>=</span> optim<span style=color:#f92672>.</span>Adam(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>lr)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>criterion <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>MSELoss()
</span></span></code></pre></div><p>This can integrated in the train_step function which orchestrates the neural network training process by executing a single iteration of Q-learning updates. It receives an experience tuple containing information about a state, action, received reward, the resulting next state, and an indicator of whether the episode has ended. The function begins by converting these components into <code>PyTorch</code> tensors to facilitate computation within the neural network. Using the provided state, the neural network predicts Q-values for different actions, storing these predictions as <code>pred</code>. Now, we should take a look at how a new Q-value for a state-action pair based on the previous Q-value and the received reward, plus the discounted maximum Q-value achievable in the resulting state is computed. The process for the agent to adjust Q-values iteratively, gradually converging towards optimal action-selection strategies by learning from experiences obtained while interacting with the environment can be seen below for the Q-learning update rule.</p><span>\[Q(s,a) = Q(s,a) + \alpha (r + \gamma \cdot max_{a'} Q(s',a')- Q(s,a))\]</span><p>Q(s,a) represents the Q-value, indicating the expected cumulative reward by taking action <code>a</code> in state <code>s</code>. It quantifies the agent&rsquo;s understanding of the long-term desirability of choosing action <code>a</code> while in state <code>s</code>. The evaluation on whether to overwrite old information during upcoming Q-updates is given by the variable <span>\(\alpha\)
</span>. Higher learning rate (<span>
\(\alpha\)
</span>) gives more weight to recent experiences, influencing how much the Q-value changes based on the new piece of information. The agent gets an immediate reward <code>r</code> gained for the action after executing action <code>a</code> at this current state <code>s</code>. This immediate feedback guides the agent&rsquo;s learning, impacting the adjustment of Q-values based on the obtained rewards in each state-action pair. Moreover, the significance of future rewards compared to current rewards is given by <span>\(\gamma\)
</span>. Naturally higher discount factor <span>\(\gamma\)
</span>prioritizes long-term rewards more. Lastly, Q(s&rsquo;,a&rsquo;) indicates the maximum Q-value achievable in the subsequent state <code>s'</code> when taking an action <code>a'</code>. It influences agent&rsquo;s decision-making by taking into account potential future rewards based on the best action available from the next state, guiding him to more rewarding states.</p><p>Employing the Q-learning update rule, the function computes target Q-values based on the predicted values, updating them according to the received rewards and the next state&rsquo;s Q-values if the episode hasn&rsquo;t terminated. Subsequently, it calculates the Mean Squared Error (MSE) loss between the predicted and target Q-values and performs backpropagation to derive gradients for updating the neural network&rsquo;s parameters. Finally, the optimizer applies these gradients to adjust the model&rsquo;s weights through optimization, enhancing the network&rsquo;s ability to approximate accurate Q-values for efficient decision-making in RL scenarios.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_step</span>(self, state, action, reward, next_state, done):
</span></span><span style=display:flex><span>    state <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(state, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float)
</span></span><span style=display:flex><span>    next_state <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(next_state, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float)
</span></span><span style=display:flex><span>    action <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(action, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>long)
</span></span><span style=display:flex><span>    reward <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(reward, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    pred <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>model(state)
</span></span><span style=display:flex><span>    target <span style=color:#f92672>=</span> pred<span style=color:#f92672>.</span>clone()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Q-learning update rule</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Handling single-dimensional state and action tensors</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> state<span style=color:#f92672>.</span>dim() <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>:  <span style=color:#75715e># (1,x)</span>
</span></span><span style=display:flex><span>        state <span style=color:#f92672>=</span> state<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        next_state <span style=color:#f92672>=</span> next_state<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        action <span style=color:#f92672>=</span> action<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        reward <span style=color:#f92672>=</span> reward<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        done <span style=color:#f92672>=</span> (done,)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Predicting Q-values based on current state-action pair</span>
</span></span><span style=display:flex><span>    pred <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>model(state)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Clone the prediction for updating</span>
</span></span><span style=display:flex><span>    target <span style=color:#f92672>=</span> pred<span style=color:#f92672>.</span>clone()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> idx <span style=color:#f92672>in</span> range(len(done)):
</span></span><span style=display:flex><span>        Q_new <span style=color:#f92672>=</span> reward[idx]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> done[idx]:
</span></span><span style=display:flex><span>            Q_new <span style=color:#f92672>=</span> reward[idx] <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>gamma <span style=color:#f92672>*</span> torch<span style=color:#f92672>.</span>max(self<span style=color:#f92672>.</span>model(next_state[idx]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        action_idx <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>argmax(action[idx])<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>        target[idx][action_idx] <span style=color:#f92672>=</span> Q_new
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Zero the gradients, compute loss, backpropagate, and update weights</span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>criterion(target, pred)
</span></span><span style=display:flex><span>    loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>optimizer<span style=color:#f92672>.</span>step()
</span></span></code></pre></div><p>Together, these classes form the backbone of a Q-learning approach, where the LinearQNet acts as the neural network to estimate Q-values and the QTrainer orchestrates the training process by updating the network&rsquo;s parameters to improve Q-value predictions.</p><blockquote class="book-hint2 warning"><p class="hint-title warning"><svg class="book-icon"><use href="/svg/hint-icons.svg#warning-notice"/></svg><span>warning</span></p><p>Modifications:</p><ul><li>Experiment with deeper networks, different activation functions (Sigmoid, i.e), regularization L2/ Dropout to prevent overfitting (getting stuck) and use different optimizers (RMSprop or SGD) for <code>LinearQClass</code>.</li><li>It is possible to implement different loss functions or Double Q-learning to mitigate bias impact using separate networks to update Q-values.</li><li>Explore how to improve learning efficiency of the agent.</li></ul></blockquote><h3 id=12-deploying-a-reinforcement-learning-agent>1.2. Deploying a Reinforcement Learning Agent
<a class=anchor href=#12-deploying-a-reinforcement-learning-agent>#</a></h3><p>Let us start by applying the constructor method (<strong>init</strong>) of a class. This is where several fundamental attributes and objects are initialized for a RL agent.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>n_games <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span> <span style=color:#75715e># Number of games played</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>epsilon <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span> <span style=color:#75715e># Parameter for exploration-exploitation trade-off</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>gamma <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.9</span> <span style=color:#75715e># Discount factor for future rewards</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>memory <span style=color:#f92672>=</span> deque(maxlen<span style=color:#f92672>=</span>MAX_MEMORY) <span style=color:#75715e># Replay memory for storing experiences</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># QTrainer for model training </span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>trainer <span style=color:#f92672>=</span> QTrainer(self<span style=color:#f92672>.</span>model, lr<span style=color:#f92672>=</span>ALPHA, gamma<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>gamma) 
</span></span></code></pre></div><p><code>self.n_games</code> tracks the number of games the agent has played. <code>self.epsilon</code> represents a parameter essential for the <a href=https://en.wikipedia.org/wiki/Exploration-exploitation_dilemma>exploration-exploitation trade-off</a>, influencing the agent&rsquo;s decision-making process. The <code>self.gamma</code> variable signifies the discount factor applied to future rewards, impacting the agent&rsquo;s prioritization of immediate versus delayed rewards. The <code>deque</code> named <code>self.memory</code>, constrained by <code>MAX_MEMORY</code>, functions as a replay memory, storing previous experiences crucial for the agent&rsquo;s learning process. Additionally, the <code>self.trainer</code>, instantiated as <code>QTrainer</code>, facilitates model training using the <code>self.model</code>, employing the provided learning rate (<code>ALPHA</code>) and discount factor (<code>gamma</code>) within the <code>QTrainer</code> class.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Neural network model (input size, hidden layer no. neurons, output size)</span>
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>model <span style=color:#f92672>=</span> LinearQNet(<span style=color:#ae81ff>11</span>, <span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>3</span>) 
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>target_model <span style=color:#f92672>=</span> LinearQNet(<span style=color:#ae81ff>11</span>, <span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>3</span>)
</span></span></code></pre></div><p>The <code>self.model</code> is a neural network structure defined as <code>LinearQNet(11, 256, 3)</code> indicates specific architectural details tailored for a RL task. The choice of these parameters signifies the design of the neural network for this particular problem domain.</p><p>The &lsquo;11&rsquo; in the network signifies the input size, representing the number of features or variables characterizing the environment&rsquo;s state, which encompass details directions: vertical and horizontal distance, dangers based on moving forward, left or right and snake and food locations. The &lsquo;256&rsquo; hidden units denote the number of neurons in the hidden layer. This specific number, 256, is a heuristic or empirical choice commonly used across different domains: ranging from generative Convolutional Neural Networks (CNNs) architectures such as AlexNet or VGG to Natural Language Processing (NLPs) Recurrent Neural Network (RNNs) architectures. It provides a moderately sized layer that offers sufficient capacity for learning complex representations from the input data without overly increasing computational costs. Additionally, being a <a href=https://stackoverflow.com/questions/63515846/in-neural-networks-why-conventionally-set-number-of-neurons-to-2n>power of two, it aligns well with computational optimizations and hardware implementations</a>, making it computationally efficient for many systems. Lastly, the &lsquo;3&rsquo; as the output size corresponds to the number of actions the agent can undertake in the environment. In this case, having three outputs suggests that the agent has three distinct possible actions it can choose from in response to a given state in the environment. For instance, these actions represent movements such as &ldquo;left&rdquo;, &ldquo;right&rdquo; and &ldquo;forward&rdquo; seen previously in the development of <code>game.py</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_state</span>(self, game):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Extracting snake&#39;s head position and defining points in different directions</span>
</span></span><span style=display:flex><span>    head <span style=color:#f92672>=</span> game<span style=color:#f92672>.</span>snake[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    point_l <span style=color:#f92672>=</span> Point(head<span style=color:#f92672>.</span>x <span style=color:#f92672>-</span> <span style=color:#ae81ff>20</span>, head<span style=color:#f92672>.</span>y)
</span></span><span style=display:flex><span>    point_r <span style=color:#f92672>=</span> Point(head<span style=color:#f92672>.</span>x <span style=color:#f92672>+</span> <span style=color:#ae81ff>20</span>, head<span style=color:#f92672>.</span>y)
</span></span><span style=display:flex><span>    point_u <span style=color:#f92672>=</span> Point(head<span style=color:#f92672>.</span>x, head<span style=color:#f92672>.</span>y <span style=color:#f92672>-</span> <span style=color:#ae81ff>20</span>)
</span></span><span style=display:flex><span>    point_d <span style=color:#f92672>=</span> Point(head<span style=color:#f92672>.</span>x, head<span style=color:#f92672>.</span>y <span style=color:#f92672>+</span> <span style=color:#ae81ff>20</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Determining the snake&#39;s current direction</span>
</span></span><span style=display:flex><span>    dir_l <span style=color:#f92672>=</span> game<span style=color:#f92672>.</span>direction <span style=color:#f92672>==</span> Direction<span style=color:#f92672>.</span>LEFT
</span></span><span style=display:flex><span>    dir_r <span style=color:#f92672>=</span> game<span style=color:#f92672>.</span>direction <span style=color:#f92672>==</span> Direction<span style=color:#f92672>.</span>RIGHT
</span></span><span style=display:flex><span>    dir_u <span style=color:#f92672>=</span> game<span style=color:#f92672>.</span>direction <span style=color:#f92672>==</span> Direction<span style=color:#f92672>.</span>UP
</span></span><span style=display:flex><span>    dir_d <span style=color:#f92672>=</span> game<span style=color:#f92672>.</span>direction <span style=color:#f92672>==</span> Direction<span style=color:#f92672>.</span>DOWN
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Checking for potential dangers in different directions</span>
</span></span><span style=display:flex><span>    danger_straight <span style=color:#f92672>=</span> (dir_r <span style=color:#f92672>and</span> game<span style=color:#f92672>.</span>is_collision(point_r)) <span style=color:#f92672>or</span> \
</span></span><span style=display:flex><span>                      (dir_l <span style=color:#f92672>and</span> game<span style=color:#f92672>.</span>is_collision(point_l)) <span style=color:#f92672>or</span> \
</span></span><span style=display:flex><span>                      (dir_u <span style=color:#f92672>and</span> game<span style=color:#f92672>.</span>is_collision(point_u)) <span style=color:#f92672>or</span> \
</span></span><span style=display:flex><span>                      (dir_d <span style=color:#f92672>and</span> game<span style=color:#f92672>.</span>is_collision(point_d))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Indicating the snake&#39;s movement direction</span>
</span></span><span style=display:flex><span>    move_direction <span style=color:#f92672>=</span> [int(dir_l), int(dir_r), int(dir_u), int(dir_d)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Determining food&#39;s relative position compared to the snake&#39;s head</span>
</span></span><span style=display:flex><span>    food_position <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>        game<span style=color:#f92672>.</span>food<span style=color:#f92672>.</span>x <span style=color:#f92672>&lt;</span> game<span style=color:#f92672>.</span>head<span style=color:#f92672>.</span>x,  <span style=color:#75715e># food left</span>
</span></span><span style=display:flex><span>        game<span style=color:#f92672>.</span>food<span style=color:#f92672>.</span>x <span style=color:#f92672>&gt;</span> game<span style=color:#f92672>.</span>head<span style=color:#f92672>.</span>x,  <span style=color:#75715e># food right</span>
</span></span><span style=display:flex><span>        game<span style=color:#f92672>.</span>food<span style=color:#f92672>.</span>y <span style=color:#f92672>&lt;</span> game<span style=color:#f92672>.</span>head<span style=color:#f92672>.</span>y,  <span style=color:#75715e># food up</span>
</span></span><span style=display:flex><span>        game<span style=color:#f92672>.</span>food<span style=color:#f92672>.</span>y <span style=color:#f92672>&gt;</span> game<span style=color:#f92672>.</span>head<span style=color:#f92672>.</span>y   <span style=color:#75715e># food down</span>
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Constructing the state representation</span>
</span></span><span style=display:flex><span>    state <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>        int(danger_straight),  <span style=color:#75715e># Danger straight</span>
</span></span><span style=display:flex><span>        int(dir_r), int(dir_l), int(dir_u), int(dir_d),  <span style=color:#75715e># Move direction</span>
</span></span><span style=display:flex><span>    ] <span style=color:#f92672>+</span> food_position  <span style=color:#75715e># Food location</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>array(state, dtype<span style=color:#f92672>=</span>int)
</span></span></code></pre></div><p>The <code>get_state</code> function within the code constructs a comprehensive representation of the game state in the Snake environment. It begins by extracting vital information such as the snake&rsquo;s head position and defining points in multiple directions to detect potential dangers, which include positions 20 units away in various directions. The function then derives the snake&rsquo;s current direction by comparing it with predefined directional indicators (left, right, up, down) based on the game&rsquo;s orientation. It proceeds to assess the presence of <strong>potential dangers in the straight, right, and left directions</strong> by checking for collisions with specific points relative to the snake&rsquo;s current orientation. Furthermore, binary flags are employed to indicate the snake&rsquo;s movement direction, while the relative position of the food compared to the snake&rsquo;s head (left, right, up, down) is determined. Finally, all these features are flushed into an array that serves as a numeric representation of the game state.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_action</span>(self, state):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Select actions based on an epsilon-greedy strategy</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>epsilon <span style=color:#f92672>=</span> <span style=color:#ae81ff>80</span> <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>n_games
</span></span><span style=display:flex><span>        final_move <span style=color:#f92672>=</span> [<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>200</span>) <span style=color:#f92672>&lt;</span> self<span style=color:#f92672>.</span>epsilon:
</span></span><span style=display:flex><span>            move <span style=color:#f92672>=</span> random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>            final_move[move] <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            state0 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(state, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float)
</span></span><span style=display:flex><span>            prediction <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>model(state0)
</span></span><span style=display:flex><span>            move <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>argmax(prediction)<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>            final_move[move] <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> final_move
</span></span></code></pre></div><p>The <code>get_action</code> method operates as the decision-maker, employing an <a href=https://medium.com/@gridflowai/part-2-in-depth-exploration-on-epsilon-greedy-algorithm-2b19e59bbe22>epsilon-greedy strategy</a> to balance exploration and exploitation. This strategy dynamically adjusts the agent&rsquo;s behaviour by modifying the exploration rate (<code>epsilon</code>) based on the number of games played (<code>n_games</code>). If a randomnly generated value falls below the epsilon threshold, indicating exploration, the agent randomly selects an action from the available choices (<strong>move left, right, or straight</strong>). Conversely, in the exploitation phase, when the generated value surpasses the epsilon threshold, the agent exploits its learned knowledge. It leverages its neural network model (<code>self.model</code>) to predict Q-values for each potential action given the current state, selecting the action with the highest predicted Q-value. The resulting one-hot encoded representation (<code>final_move</code>) denotes the chosen action, guiding the agent&rsquo;s movement and decision-making process within the game.</p><h3 id=13-max-replay-buffer-and-target-network>1.3. Max Replay Buffer and Target Network
<a class=anchor href=#13-max-replay-buffer-and-target-network>#</a></h3><p>In RL, a replay buffer serves as a memory repository, capturing and retaining past experiences encountered by an agent during its interactions with an environment. This data structure enables the agent to reutilize and learn from diverse historical interactions by storing state-action-reward-next_state tuples. By decoupling the immediate use of experiences and instead sampling randomnly from this stored memory during training, the replay buffer breaks the temporal correlation between consecutive experiences, leading to more stable and efficient learning. Meanwhile, a target network, often employed in algorithms like Deep-Q-Networks (DQN), functions as a stabilized reference for target Q-values during training. This secondary neural network provides less frequently updated Q-value targets, addressing the issue of rapidly changing targets and enhancing training stability by decoupling the estimation of target Q-values from the primary network&rsquo;s parameters.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> collections <span style=color:#f92672>import</span> deque
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> random
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>MAX_MEMORY <span style=color:#f92672>=</span> <span style=color:#ae81ff>100_000</span>
</span></span><span style=display:flex><span>BATCH_SIZE <span style=color:#f92672>=</span> <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>QLearningAgent</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Initialize replay buffer</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>memory <span style=color:#f92672>=</span> deque(maxlen<span style=color:#f92672>=</span>MAX_MEMORY)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>remember</span>(self, state, action, reward, next_state, done):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Store experiences in memory</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>memory<span style=color:#f92672>.</span>append((state, action, reward, next_state, done))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_long_memory</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Perform training using experiences from the replay buffer</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> len(self<span style=color:#f92672>.</span>memory) <span style=color:#f92672>&lt;</span> BATCH_SIZE:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        mini_batch <span style=color:#f92672>=</span> random<span style=color:#f92672>.</span>sample(self<span style=color:#f92672>.</span>memory, BATCH_SIZE)
</span></span><span style=display:flex><span>        states, actions, rewards, next_states, dones <span style=color:#f92672>=</span> zip(<span style=color:#f92672>*</span>mini_batch)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>trainer<span style=color:#f92672>.</span>train_step(states, actions, rewards, next_states, dones)
</span></span></code></pre></div><p>The applied methodology enhances stability by mitigating overfitting to recent experiences and improves learning efficiency by allowing the agent to reuse and learn from its past interactions, contributing to more stable and effective training in RL algorithms. In the <code>agent.py</code>, particularly in the <code>train_and_record</code> function, the integration of the replay buffer involves augmenting the agent&rsquo;s interactions with the environment to store experiences and utilizing those experiences for training. As the agent interacts with the environment in each game step, the <code>remember</code> function within the <code>QLearningAgent</code> class captures the state-action-reward-next_state tuples and stores them in the replay buffer. They are essential for off-policy learning, and the <code>train_short_memory</code> and <code>train_long_memory</code> functions facilitate short-term and long-term learning, respectively. After each completed game, the agent leverages the stored experiences by calling <code>train_long_memory</code>, which samples a batch of experiences from the replay buffer and uses these experiences to update the agent&rsquo;s model via the <code>train_step</code> method in the <code>QTrainer</code> class. This integration facilitates learning from a diverse set of past interactions, contributing to more stable and efficient training by breaking temporal correlations between consecutive experiences. Adjusting memory replay features in the likes of <code>MAX_MEMORY</code> and <code>BATCH_SIZE</code> in addition to initialization variables allows not only for fine-tuning of the replay buffer&rsquo;s capacity and the size of experiences utilized for training, but also for studying the whole agent&rsquo;s learning process which can be studied using commonly known evolutionary algorithms such as: Genetic Algorithms (GAs).</p><h2 id=2-genetic-optimization-of-a-rl-deep-q-network>2 Genetic Optimization of a RL Deep-Q-Network
<a class=anchor href=#2-genetic-optimization-of-a-rl-deep-q-network>#</a></h2><p>In the context of optimizing key parameters for RL — such as batch size, learning rate, memory capacity for replay buffers, and the architecture of a target network — GAs provide a systematic approach.</p><h3 id=21-parameter-space-definition>2.1. Parameter Space Definition
<a class=anchor href=#21-parameter-space-definition>#</a></h3><p>The parameter space definition refers to the specification and range of parameters that influence the architecture, behavior, and learning process of a our DQN. This includes continuous parameters like learning rate and discount factor, discrete parameters such as activation functions and optimizer types, and integer parameters like the number of hidden layers and neurons per layer.</p><blockquote class="book-hint2 info"><p class="hint-title info"><svg class="book-icon"><use href="/svg/hint-icons.svg#info-notice"/></svg><span>info</span></p><p><strong>Continuous Parameters</strong></p><ul><li><code>learning_rate</code>: Influences the speed at which the DQN learns. Higher values enable faster learning, while lower values promote stability.</li><li><code>discount_factor</code>: Determines the importance of future rewards in the learning process. Values closer to 1 emphasize long-term rewards.</li><li><code>dropout_rate</code>: Affects the number of neurons dropped out during training to prevent overfitting.</li><li><code>exploration_rate</code>: Controls the level of exploration versus exploitation in the learning process.</li></ul><p><strong>Discrete Parameters</strong></p><ul><li><code>batch_size</code>: Dictates the number of experiences sampled from the replay buffer for training.</li><li><code>activation_function</code>: Determines the type of activation function used in neural network layers (e.g., ReLU, sigmoid, tanh).</li><li><code>optimizer</code>: Specifies the optimization algorithm for updating the DQN&rsquo;s parameters during training (e.g., Adam, SGD, RMSprop).</li></ul><p><strong>Integer Parameters</strong></p><ul><li><code>num_hidden_layers</code>: Specifies the number of hidden layers in the neural network.</li><li><code>neurons_per_layer</code>: Defines the number of neurons in each hidden layer.</li></ul></blockquote><p>For each parameter, a defined range or set of possible values is established. These ranges are carefully chosen based on prior knowledge, domain expertise, or empirical observations of their impact on the DQN&rsquo;s behaviour and performance. By exploring this parameter space, the GA aims to discover configurations that maximize game-related metrics, such as higher scores or fewer steps.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>param_ranges <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#75715e># Continuous parameters</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Alpha / Higher values allow faster learning, while lower values ensure more stability</span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;learning_rate&#39;</span>: (<span style=color:#ae81ff>0.001</span>, <span style=color:#ae81ff>0.1</span>), 
</span></span><span style=display:flex><span>    <span style=color:#75715e>#Gamma / Closer to 1 indicate future rewards are highly important, emphasizing long-term rewards</span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;discount_factor&#39;</span>: (<span style=color:#ae81ff>0.9</span>, <span style=color:#ae81ff>0.999</span>), 
</span></span><span style=display:flex><span>    <span style=color:#75715e># Higher drops out a more neurons -&gt; prevent overfit in complex models/datasets with limited samples</span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;dropout_rate&#39;</span>: (<span style=color:#ae81ff>0.1</span>, <span style=color:#ae81ff>0.5</span>), 
</span></span><span style=display:flex><span>    <span style=color:#75715e># Epsilon /More exploration -&gt; Possibly better actions /Lower -&gt; More stability using learned policy</span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;exploration_rate&#39;</span>: (<span style=color:#ae81ff>0.1</span>, <span style=color:#ae81ff>0.5</span>), 
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Discrete parameters</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Number of experiences sampled from the replay buffer for training</span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;batch_size&#39;</span>: [<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>250</span>, <span style=color:#ae81ff>500</span>, <span style=color:#ae81ff>1000</span>, <span style=color:#ae81ff>2000</span>, <span style=color:#ae81ff>5000</span>], 
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;activation_function&#39;</span>: [<span style=color:#e6db74>&#39;relu&#39;</span>, <span style=color:#e6db74>&#39;sigmoid&#39;</span>, <span style=color:#e6db74>&#39;tanh&#39;</span>],
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;optimizer&#39;</span>: [<span style=color:#e6db74>&#39;adam&#39;</span>, <span style=color:#e6db74>&#39;sgd&#39;</span>, <span style=color:#e6db74>&#39;rmsprop&#39;</span>], 
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Integer parameters (num_inputs, num_neurons, num_outputs of NN)</span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;num_hidden_layers&#39;</span>: [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>],
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;neurons_per_layer&#39;</span>: [<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>512</span>, <span style=color:#ae81ff>1024</span>]
</span></span><span style=display:flex><span>    }
</span></span></code></pre></div><h3 id=22-genetic-algorithm>2.2. Genetic Algorithm
<a class=anchor href=#22-genetic-algorithm>#</a></h3><p>The algorithm starts with the initialization of a class <code>GeneticAlgorithm</code> and an initial population, both comprising diverse parameter sets. This involves creating a collection of potential solutions, representing different combinations of parameters within the specified ranges. The population&rsquo;s diversity plays a pivotal role in enabling the exploration of a broad spectrum of parameter configurations.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generate_population</span>(self, population_size, param_ranges, chromosome_length): 
</span></span><span style=display:flex><span>    <span style=color:#75715e>#Random init or heuristic init (using prior info)</span>
</span></span><span style=display:flex><span>    population <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(population_size):
</span></span><span style=display:flex><span>        params <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> param, value_range <span style=color:#f92672>in</span> param_ranges<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> isinstance(value_range, tuple):  <span style=color:#75715e># Init cont. parameters</span>
</span></span><span style=display:flex><span>                params[param] <span style=color:#f92672>=</span> random<span style=color:#f92672>.</span>uniform(value_range[<span style=color:#ae81ff>0</span>], value_range[<span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>elif</span> isinstance(value_range, list):  <span style=color:#75715e># Discrete parameters</span>
</span></span><span style=display:flex><span>                params[param] <span style=color:#f92672>=</span> random<span style=color:#f92672>.</span>choice(value_range)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>elif</span> isinstance(value_range, int):  <span style=color:#75715e># Integer parameters</span>
</span></span><span style=display:flex><span>                params[param] <span style=color:#f92672>=</span> random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, value_range)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>elif</span> isinstance(value_range, str):  <span style=color:#75715e># String parameters</span>
</span></span><span style=display:flex><span>                params[param] <span style=color:#f92672>=</span> value_range  <span style=color:#75715e># Set the string value directly</span>
</span></span><span style=display:flex><span>        population<span style=color:#f92672>.</span>append(params)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> population
</span></span></code></pre></div><p>Afterwards, the <code>fitness function</code> evaluates the performance of the DQN by considering various game-related metrics such as <code>score</code>, <code>record</code>, <code>steps</code>, <code>collisions</code>, and <code>same_positions_counter</code>. These metrics are utilized to compute a fitness score that quantifies the effectiveness of a parameter set within the DQN. Normalization of metrics like <code>score</code> and <code>steps</code> occurs next, ensuring that these metrics are on a comparable scale for fair evaluation. Normalization enables a coherent assessment where disparate metrics contribute equitably to the overall fitness score. Moreover, the function integrates conditional adjustments like penalties for certain conditions. For instance, it penalizes repeated visits to the same positions (<code>penalty_same_positions</code>) or inefficient utilization of steps (<code>penalty_efficiency_decay</code>), reflecting a meticulous consideration of nuanced gameplay elements.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fitness_function</span>(self, score, record, steps, collisions, same_positions_counter):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Metrics and weights</span>
</span></span><span style=display:flex><span>    weight_score <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.75</span>
</span></span><span style=display:flex><span>    weight_steps, MAX_POSSIBLE_STEPS <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.25</span>, <span style=color:#ae81ff>300</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Normalize metrics</span>
</span></span><span style=display:flex><span>    normalized_score <span style=color:#f92672>=</span> score <span style=color:#f92672>/</span> record <span style=color:#66d9ef>if</span> record <span style=color:#f92672>!=</span> <span style=color:#ae81ff>0</span> <span style=color:#66d9ef>else</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    normalized_steps <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> (steps <span style=color:#f92672>/</span> MAX_POSSIBLE_STEPS) <span style=color:#66d9ef>if</span> MAX_POSSIBLE_STEPS <span style=color:#f92672>!=</span> <span style=color:#ae81ff>0</span> <span style=color:#66d9ef>else</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Penalty for revisiting same positions &gt; 150 (5%)</span>
</span></span><span style=display:flex><span>    penalty_same_positions <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.05</span> <span style=color:#66d9ef>if</span> same_positions_counter <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>150</span> <span style=color:#66d9ef>else</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Efficiency decay (5%)</span>
</span></span><span style=display:flex><span>    efficiency_decay <span style=color:#f92672>=</span> max(<span style=color:#ae81ff>0</span>, (steps <span style=color:#f92672>-</span> score) <span style=color:#f92672>/</span> MAX_POSSIBLE_STEPS)
</span></span><span style=display:flex><span>    penalty_efficiency_decay <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.05</span> <span style=color:#f92672>*</span> efficiency_decay
</span></span></code></pre></div><p>Ultimately, the fitness score is computed by merging the normalized metrics, weighted according to their significance, and factoring in penalties or bonuses where applicable. The goal is to synthesize a comprehensive fitness score that encapsulates the effectiveness of a particular parameter set in improving the DQN&rsquo;s performance within the game environment</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Calculate fitness</span>
</span></span><span style=display:flex><span>fitness <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>    (normalized_score <span style=color:#f92672>*</span> weight_score) <span style=color:#f92672>+</span>
</span></span><span style=display:flex><span>    (normalized_steps <span style=color:#f92672>*</span> weight_steps) <span style=color:#f92672>-</span>
</span></span><span style=display:flex><span>    penalty_same_positions <span style=color:#f92672>-</span> penalty_efficiency_decay
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>return</span> max(<span style=color:#ae81ff>0</span>, fitness)  <span style=color:#75715e># Ensure non-negative fitness</span>
</span></span></code></pre></div><p>The fitness scores for the entirity of the population of parameter sets can be computed using <code>calculate_population_fitness</code>. It ensures the inclusion of at least 5 recent game metrics for evaluation or uses all available metrics if fewer than 5 are present. By iterating through these metrics, it extracts essential indicators like <code>score</code>, <code>record</code>, <code>steps</code>, <code>collisions</code>, and <code>same_positions_counter</code> for each individual set. Afterwards, there is selection: it determines which individuals, represented as parameter sets, proceed to the next generation based on their fitness scores. Initially, it normalizes the fitness scores received from the <code>fitness_function</code>, ensuring these scores reflect the effectiveness of parameter sets in enhancing the DQN&rsquo;s performance. Subsequently, it computes probabilities for each individual proportional to their fitness scores, favouring individuals with higher fitness. Employing a roulette wheel selection strategy, the method then selects individuals from the population according to these probabilities, allowing higher-scoring individuals a greater chance of being chosen. This selection process forms the basis for creating a new population consisting of the chosen individuals, facilitating the iterative evolution and refinement of parameter sets across successive generations within the GA framework.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>selection</span>(self, population, fitness_scores):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Normalize fitness scores to probabilities</span>
</span></span><span style=display:flex><span>    total_fitness <span style=color:#f92672>=</span> sum(fitness_scores)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> total_fitness <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>        probabilities <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> len(fitness_scores)] <span style=color:#f92672>*</span> len(fitness_scores)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        probabilities <span style=color:#f92672>=</span> [fitness <span style=color:#f92672>/</span> total_fitness <span style=color:#66d9ef>for</span> fitness <span style=color:#f92672>in</span> fitness_scores]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Ensure probabilities array size matches population size</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> len(probabilities) <span style=color:#f92672>&lt;</span> len(population):
</span></span><span style=display:flex><span>        probabilities<span style=color:#f92672>.</span>append(<span style=color:#ae81ff>0.0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Select based on fitness (roulette wheel selection) //</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># replace = True means one chromosome can be picked more than 1 time</span>
</span></span><span style=display:flex><span>    selected_indices <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>choice(
</span></span><span style=display:flex><span>        len(population), 
</span></span><span style=display:flex><span>        size<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>population_size, 
</span></span><span style=display:flex><span>        replace<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, 
</span></span><span style=display:flex><span>        p<span style=color:#f92672>=</span>probabilities <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>sum(probabilities)  
</span></span><span style=display:flex><span>        <span style=color:#75715e># Normalize probabilities to sum up to 1</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Create a new population based on the selected indices</span>
</span></span><span style=display:flex><span>    new_population <span style=color:#f92672>=</span> [population[idx] <span style=color:#66d9ef>for</span> idx <span style=color:#f92672>in</span> selected_indices] 
</span></span><span style=display:flex><span>    <span style=color:#75715e># List Comprehension - New population Array</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> new_population
</span></span></code></pre></div><p>Following the <code>selection</code> of individuals, the <code>crossover</code> function exemplifies genetic recombination between two parent individuals to produce offspring individuals as potential solutions within the GA. Initially, it ensures that both parents have the same length of genetic information, converting them into lists if they are initially dictionaries. Then, based on a randomnly determined crossover probability (<code>crossover_rate</code>), it either conducts the crossover process or maintains the parents as they are. When the crossover occurs (determined by a random probability check), it identifies a crossover point within the genetic information and generates two offspring by swapping the genetic information of the parents before and after this point. These newly created offspring individuals represent combinations of genetic material from both parents, potentially leading to diverse and potentially advantageous solutions within the population. If the crossover does not happen, it returns the original parent individuals as the output.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;Single-point crossover for two parent individuals. 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>Can explore two-point crossover, uniform crossover, elitist crossover, etc.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>crossover</span>(self, parent1, parent2, crossover_rate):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> isinstance(parent1, dict) <span style=color:#f92672>and</span> isinstance(parent2, dict):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Convert dictionary values to lists</span>
</span></span><span style=display:flex><span>        parent1 <span style=color:#f92672>=</span> list(parent1<span style=color:#f92672>.</span>values())
</span></span><span style=display:flex><span>        parent2 <span style=color:#f92672>=</span> list(parent2<span style=color:#f92672>.</span>values())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> len(parent1) <span style=color:#f92672>==</span> len(parent2) <span style=color:#75715e># Only if same len</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> random<span style=color:#f92672>.</span>random() <span style=color:#f92672>&lt;</span> crossover_rate:
</span></span><span style=display:flex><span>        <span style=color:#75715e># Crossover point</span>
</span></span><span style=display:flex><span>        crossover_point <span style=color:#f92672>=</span> random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>1</span>, len(parent1) <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Create offspring by combining parent genes</span>
</span></span><span style=display:flex><span>        offspring1 <span style=color:#f92672>=</span> parent1[:crossover_point] <span style=color:#f92672>+</span> parent2[crossover_point:]
</span></span><span style=display:flex><span>        offspring2 <span style=color:#f92672>=</span> parent2[:crossover_point] <span style=color:#f92672>+</span> parent1[crossover_point:]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> offspring1, offspring2
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>: <span style=color:#75715e># If crossover doesn&#39;t happen, return the parents</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> parent1, parent2
</span></span></code></pre></div><p>The <code>mutation</code> function operates on individuals — comprising genetic material representing potential solutions — and introduces small alterations to their genetic makeup based on a predefined <code>mutation rate</code>. This genetic variation mechanism enables exploration of novel solution spaces. The function iterates through the genetic information of an individual and, for each gene, checks if a randomly generated probability falls below the specified <code>mutation rate</code>. If so, it attempts to modify the gene: for numeric values, it converts the gene to an integer and performs a transformation (in this case, subtracting the value from 1), showcasing the alteration; for non-numeric values, it retains the original gene. The function aggregates these modified genes, generating a mutated individual with potential genetic diversity that might lead to the exploration of new and potentially beneficial solution areas within the GA&rsquo;s solution space.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;According to Genetic Algorithm, after crossover (breeding), we apply mutation 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>to the resulting offspring to introduce small changes to their genetic material 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>depending on the mutation rate, this helps explores new areas of solution space&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>mutation</span>(self, individual, mutation_rate):
</span></span><span style=display:flex><span>    mutated_individual <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> gene <span style=color:#f92672>in</span> individual:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> random<span style=color:#f92672>.</span>random() <span style=color:#f92672>&lt;</span> mutation_rate:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>try</span>: <span style=color:#75715e># Assuming &#39;gene&#39; is str to convert to a numerical</span>
</span></span><span style=display:flex><span>                gene <span style=color:#f92672>=</span> int(gene)  <span style=color:#75715e># Convert &#39;gene&#39; to an integer</span>
</span></span><span style=display:flex><span>                mutated_gene <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> gene
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>ValueError</span>:
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>&#34;&#39;gene&#39; might not be a numeric value.&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            mutated_gene <span style=color:#f92672>=</span> gene
</span></span><span style=display:flex><span>        mutated_individual<span style=color:#f92672>.</span>append(mutated_gene)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> mutated_individual
</span></span></code></pre></div><p>Lastly, it extends the existing offspring list with newly generated offspring, resulting from genetic recombination and mutation processes. Subsequently, it implements an elitism strategy, identifying the top-performing individuals within the population based on their fitness scores and replacing the least fit part of the population with these elite individuals. This preserves highly fit solutions from the current population for the next generation, ensuring the retention of successful traits. Additionally, it evaluates the fitness of each parameter set in the current population, identifying the best-performing parameters by comparing their fitness against a stored best fitness value, thereby capturing the best parameters encountered during the Genetic Algorithm&rsquo;s execution.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Replace the least fit part of the population with offspring</span>
</span></span><span style=display:flex><span>elite_count <span style=color:#f92672>=</span> int(self<span style=color:#f92672>.</span>population_size <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.1</span>)  <span style=color:#75715e># Keep top 10% as elite</span>
</span></span><span style=display:flex><span>elite_indices <span style=color:#f92672>=</span> sorted(range(len(fitness_scores)), key<span style=color:#f92672>=</span><span style=color:#66d9ef>lambda</span> i: fitness_scores[i], reverse<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)[:elite_count]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> idx <span style=color:#f92672>in</span> elite_indices:
</span></span><span style=display:flex><span>    offspring[idx] <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>population[idx]  <span style=color:#75715e># Preserve elite chromosomes</span>
</span></span></code></pre></div><h2 id=3-results>3 Results
<a class=anchor href=#3-results>#</a></h2><div class="book-columns flex flex-wrap"><div class="flex-even markdown-inner"><img src=https://s5.gifyu.com/images/SiDzT.gif alt=123></div><div class="flex-even markdown-inner"><img src=https://s5.gifyu.com/images/SiDzw.gif alt=123019></div></div><p><code>(to insert more images)</code></p><h2 id=4-outcomes>4 Outcomes
<a class=anchor href=#4-outcomes>#</a></h2><p>The GA can efficiently search through a defined parameter space to identify better sets of hyperparameters for the RL model. This can lead to enhanced performance, quicker convergence, and increased stability within the learning process. By fine-tuning the hyperparameters, the RL model might exhibit improved performance metrics such as higher rewards, more efficient learning, reduced training time, and potentially better generalization to unseen data, unlike using only the RL model, where it took approximately 150 games to converge to good scores.</p><blockquote class="book-hint2 important"><p class="hint-title important"><svg class="book-icon"><use href="/svg/hint-icons.svg#important-notice"/></svg><span>important</span></p>An interesting take on GA is the ability to explore diverse solution spaces, by doing so it might uncover parameter configurations that promote better exploration-exploitation trade-offs, addressing challenges like the exploration-exploitation dilemma common in RL.</blockquote><ul><li><a href="https://youtu.be/CY_LEa9xQtg?t=2467">Risto Miikkulainen and Lex Fridman discussing the importance of neuroevolution</a> in deep networks: for instance, how applying evolutionary computation is helpful in assessing architecture topology or the layer depth</li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/roaked/roaked.github.io/commit/55f7c155acc5572cf21873b8c44549f5e5558e83 title='Last modified by roaked | February 23, 2024' target=_blank rel=noopener><img src=/svg/calendar.svg class=book-icon alt=Calendar>
<span>February 23, 2024</span></a></div><div><a class="flex align-center" href=https://github.com/roaked/roaked.github.io/edit/main/content/content/docs/code/snake-game/deepqnetwork.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Ricardo Chin</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#1-reinforcement-deep-q-network-architecture>1 Reinforcement Deep Q-Network Architecture</a><ul><li><a href=#11-linearqnet-and-qtrainer-classes>1.1. LinearQNet and QTrainer Classes</a></li><li><a href=#12-deploying-a-reinforcement-learning-agent>1.2. Deploying a Reinforcement Learning Agent</a></li><li><a href=#13-max-replay-buffer-and-target-network>1.3. Max Replay Buffer and Target Network</a></li></ul></li><li><a href=#2-genetic-optimization-of-a-rl-deep-q-network>2 Genetic Optimization of a RL Deep-Q-Network</a><ul><li><a href=#21-parameter-space-definition>2.1. Parameter Space Definition</a></li><li><a href=#22-genetic-algorithm>2.2. Genetic Algorithm</a></li></ul></li><li><a href=#3-results>3 Results</a></li><li><a href=#4-outcomes>4 Outcomes</a></li></ul></nav></div></aside></main></body></html>