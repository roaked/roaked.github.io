<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Off Policy and DQN Parameter Optimization
  #


  
    
    First Snake.
  

This section documents the snake-q-learning-genetic-algorithm project: a Snake environment built with pygame and trained with an off-policy Deep Q-Network (DQN), with genetic search used to tune key learning parameters.
The implementation is organized around four core components:



game.py / game_user.py: environment dynamics, collisions, food placement, and rendering.




model.py: Q-network definition (LinearQNet) and training loop wrapper (QTrainer).




agent.py: epsilon-greedy action selection, replay-based training, and game-loop integration.




genetic.py: parameter optimization over learning-rate, discount, dropout, and exploration ranges.



The goal is to maximize score while keeping training stable, then compare baseline RL behaviour against GA-tuned configurations under the same Snake rules."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://xsleaks.dev/docs/code/snake-game/deepqnetwork/"><meta property="og:site_name" content="Ricardo Chin"><meta property="og:title" content="BaseRL, GA-RL & Hamiltonian"><meta property="og:description" content="Off Policy and DQN Parameter Optimization # First Snake.This section documents the snake-q-learning-genetic-algorithm project: a Snake environment built with pygame and trained with an off-policy Deep Q-Network (DQN), with genetic search used to tune key learning parameters.
The implementation is organized around four core components:
game.py / game_user.py: environment dynamics, collisions, food placement, and rendering. model.py: Q-network definition (LinearQNet) and training loop wrapper (QTrainer). agent.py: epsilon-greedy action selection, replay-based training, and game-loop integration. genetic.py: parameter optimization over learning-rate, discount, dropout, and exploration ranges. The goal is to maximize score while keeping training stable, then compare baseline RL behaviour against GA-tuned configurations under the same Snake rules."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2026-02-26T15:56:55+01:00"><title>BaseRL, GA-RL & Hamiltonian | Ricardo Chin</title><link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png><link rel=stylesheet href=/book.min.b1824eea1873de6fd466b2e1d8cc26a9f8e6e7aaf317af5779752ac8f255d4c4.css integrity="sha256-sYJO6hhz3m/UZrLh2Mwmqfjm56rzF69XeXUqyPJV1MQ=" crossorigin=anonymous><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>Ricardo Chin</span></a></h2><ul><li class=book-section-flat><a href=/docs/design/>Engineering Repository</a><ul><li><a href=/docs/design/finite-element-method-development/>FEM Package Development</a><ul></ul></li></ul></li><li class=book-section-flat><a href=/docs/code/>System Repository</a><ul><li><a href=/docs/code/agv/>AGV: Stochastic Identification</a><ul></ul></li><li><a href=/docs/code/uav/>UAV: Red Bull Air Racing</a><ul><li><a href=/docs/code/uav/dynamics/>Drone System Dynamics</a></li><li><a href=/docs/code/uav/continuous-controller-design/>Continuous Controller</a></li><li><a href=/docs/code/uav/discrete-controller-design/>Discrete Controller</a></li><li><a href=/docs/code/uav/computer-vision/>Gate Sense: Computer Vision</a></li></ul></li><li><a href=/docs/code/robotics/>Robotics: Kin, Dynamics & Control</a><ul></ul></li><li><a href=/docs/code/micromouse/>Micromouse: Flood Fill and A* (2D)</a><ul></ul></li><li><a href=/docs/code/bin-packing/>EC Optimization: Space & Time</a><ul><li><a href=/docs/code/bin-packing/genetic-algorithm/>Genetic Algorithm</a></li><li><a href=/docs/code/bin-packing/particle-swarm-optimization/>Particle Swarm Optimization</a></li></ul></li><li><a href=/docs/code/snake-game/>Snake: Reinforcement Learning</a><ul><li><a href=/docs/code/snake-game/deepqnetwork/ class=active>BaseRL, GA-RL & Hamiltonian</a></li><li><a href=/docs/code/snake-game/adversarial/>Adversarial Multi-Agent RL, BattleMode</a></li></ul></li><li><a href=/docs/code/deep-learning-fake-news/>Fake News: Inference & Clusters</a><ul></ul></li></ul></li><li class=book-section-flat><a href=/docs/lectures/>My Notes and Lectures</a><ul><li><a href=/docs/lectures/bayesian-machine-learning/bayes/>Bayesian Inference</a></li><li><a href=/docs/lectures/bayesian-optimization/bayesopt/>Bayesian Optimization</a></li><li><a href=/docs/lectures/nonlinear-programming/nonlinear/>Nonlinear Programming</a></li><li><a href=/docs/lectures/hamiltonian-graphs/hamiltonian/>Hamiltonian Graphs & Linked Lists</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>BaseRL, GA-RL & Hamiltonian</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><a href=#code-references-github-methods>Code References (GitHub Methods)</a></li></ul></li><li><a href=#1-reinforcement-deep-q-network-architecture>1 Reinforcement Deep Q-Network Architecture</a><ul><li><a href=#11-linearqnet-and-qtrainer-classes>1.1. LinearQNet and QTrainer Classes</a></li><li><a href=#12-deploying-a-reinforcement-learning-agent>1.2. Deploying a Reinforcement Learning Agent</a></li><li><a href=#13-max-replay-buffer-and-target-network>1.3. Max Replay Buffer and Target Network</a></li></ul></li><li><a href=#2-standard-rl>2 Standard RL</a><ul><li><a href=#21-learning-signal-and-data-flow>2.1. Learning signal and data flow</a></li><li><a href=#22-code-snippet-core-baseline-step>2.2. Code snippet (core baseline step)</a></li><li><a href=#23-episode-boundary-update>2.3. Episode boundary update</a></li></ul></li><li><a href=#3-ga-optimized-rl>3 GA-Optimized RL</a><ul><li><a href=#31-why-evolve-hyperparameters-during-training>3.1. Why evolve hyperparameters during training</a></li><li><a href=#32-metrics-and-optimization-loop>3.2. Metrics and optimization loop</a></li><li><a href=#33-runtime-integration-with-dqn>3.3. Runtime integration with DQN</a></li><li><a href=#34-is-rl-still-learning-if-it-is-not-winning>3.4. &ldquo;Is RL still learning if it is not winning?&rdquo;</a></li><li><a href=#35-why-ga-rl-can-still-fail-often>3.5. Why GA-RL can still fail often</a></li><li><a href=#36-what-was-improved-in-this-repository>3.6. What was improved in this repository</a></li><li><a href=#37-practical-interpretation-of-results>3.7. Practical interpretation of results</a></li></ul></li><li><a href=#4-hamiltonian>4 Hamiltonian</a><ul><li><a href=#41-algorithm-intuition>4.1. Algorithm intuition</a></li><li><a href=#42-cycle-driven-action-generation>4.2. Cycle-driven action generation</a></li><li><a href=#43-role-in-evaluation>4.3. Role in evaluation</a></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=off-policy-and-dqn-parameter-optimization><strong>Off Policy and DQN Parameter Optimization</strong>
<a class=anchor href=#off-policy-and-dqn-parameter-optimization>#</a></h1><div style=display:flex;justify-content:space-between;gap:10px;align-items:center><video width=100% height=auto controls autoplay loop muted playsinline>
<source src=/videos/first_snake.webm type=video/webm>First Snake.</video></div><p>This section documents the <a href=https://github.com/roaked/snake-q-learning-genetic-algorithm><code>snake-q-learning-genetic-algorithm</code></a> project: a Snake environment built with <code>pygame</code> and trained with an off-policy Deep Q-Network (DQN), with genetic search used to tune key learning parameters.</p><p>The implementation is organized around four core components:</p><ul><li><ol><li><code>game.py</code> / <code>game_user.py</code>: environment dynamics, collisions, food placement, and rendering.</li></ol></li><li><ol start=2><li><code>model.py</code>: Q-network definition (<code>LinearQNet</code>) and training loop wrapper (<code>QTrainer</code>).</li></ol></li><li><ol start=3><li><code>agent.py</code>: epsilon-greedy action selection, replay-based training, and game-loop integration.</li></ol></li><li><ol start=4><li><code>genetic.py</code>: parameter optimization over learning-rate, discount, dropout, and exploration ranges.</li></ol></li></ul><p>The goal is to maximize score while keeping training stable, then compare baseline RL behaviour against GA-tuned configurations under the same Snake rules.</p><h3 id=code-references-github-methods>Code References (GitHub Methods)
<a class=anchor href=#code-references-github-methods>#</a></h3><ul><li><a href="https://github.com/roaked/snake-q-learning-genetic-algorithm/search?q=def+__init__+LinearQNet&amp;type=code"><code>LinearQNet.__init__</code></a>, <a href="https://github.com/roaked/snake-q-learning-genetic-algorithm/search?q=def+forward+LinearQNet&amp;type=code"><code>LinearQNet.forward</code></a>, <a href="https://github.com/roaked/snake-q-learning-genetic-algorithm/search?q=def+save+LinearQNet&amp;type=code"><code>LinearQNet.save</code></a></li><li><a href="https://github.com/roaked/snake-q-learning-genetic-algorithm/search?q=def+train_step+QTrainer&amp;type=code"><code>QTrainer.train_step</code></a></li><li><a href="https://github.com/roaked/snake-q-learning-genetic-algorithm/search?q=def+get_state+QLearningAgent&amp;type=code"><code>QLearningAgent.get_state</code></a>, <a href="https://github.com/roaked/snake-q-learning-genetic-algorithm/search?q=def+get_action+QLearningAgent&amp;type=code"><code>QLearningAgent.get_action</code></a>, <a href="https://github.com/roaked/snake-q-learning-genetic-algorithm/search?q=def+remember+QLearningAgent&amp;type=code"><code>QLearningAgent.remember</code></a>, <a href="https://github.com/roaked/snake-q-learning-genetic-algorithm/search?q=def+train_long_memory+QLearningAgent&amp;type=code"><code>QLearningAgent.train_long_memory</code></a></li><li><a href="https://github.com/roaked/snake-q-learning-genetic-algorithm/search?q=def+train_RL&amp;type=code"><code>train_RL</code></a>, <a href="https://github.com/roaked/snake-q-learning-genetic-algorithm/search?q=def+train&amp;type=code"><code>train</code></a>, <a href="https://github.com/roaked/snake-q-learning-genetic-algorithm/search?q=def+genetic+genetic.py&amp;type=code"><code>genetic.genetic</code></a></li></ul><h2 id=1-reinforcement-deep-q-network-architecture>1 Reinforcement Deep Q-Network Architecture
<a class=anchor href=#1-reinforcement-deep-q-network-architecture>#</a></h2><p>DQN (Deep Q-Network) stands as a RL algorithm rooted in deep learning principles. It integrates a Q-Learning algorithm with a deep neural network to address RL challenges in expansive state and action spaces. In the DQN algorithm, a neural network is employed to approximate the Q function, where states and actions serve as inputs, yielding corresponding Q values as outputs. Essentially, the DQN algorithm can be broken down into the following steps:</p><ul><li><ol><li><strong>Initialization</strong>: Initialize a neural network with random weights θ to approximate the Q function Q(s,a,θ).</li></ol></li><li><ol start=2><li><strong>Experience Replay</strong>: Maintain a replay memory buffer
<link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\(\mathcal{D}\)
</span>to store experiences <span>\(e_t = (s_t, a_t, r_t, s_{t+1})\)
</span>consisting of state-action pairs at state t, rewards, and subsequent states encountered during interactions with the environment.</li></ol></li><li><ol start=3><li><strong>Action Selection</strong>: Utilize an epsilon-greedy policy to select actions based on the current state, balancing between exploration and exploitation.</li></ol></li></ul><span>\[
a_t = \begin{cases}
\text{random action} & \text{with probability } \epsilon \\
\text{argmax}_a Q(s_t, a, \theta) & \text{otherwise}
\end{cases}
\]</span><ul><li><ol start=4><li><strong>Training</strong>: Periodically sample batches of experiences from the replay memory buffer to update the neural network&rsquo;s parameters. The loss function used for training is typically derived from the Bellman equation, aiming to minimize the discrepancy between the predicted Q values and the target Q values.</li></ol></li></ul><span>\[
\mathcal{L}(\theta) = \mathbb{E}_{(s, a, r, s') \sim U(D)} \left[ \left( r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a, \theta) \right)^2 \right]
\]</span><ul><li><ol start=5><li><strong>Repeat</strong>: Iterate step 3 and 4 until convergence, continually updating the neural network to better approximate the Q function.</li></ol></li></ul><p>Overall, the DQN algorithm leverages deep learning techniques to efficiently handle RL problems characterized by large state and action spaces, enabling effective learning and decision-making in complex environments.</p><h3 id=11-linearqnet-and-qtrainer-classes>1.1. LinearQNet and QTrainer Classes
<a class=anchor href=#11-linearqnet-and-qtrainer-classes>#</a></h3><p>The <a href=https://github.com/roaked/snake-q-learning-genetic-algorithm/blob/main/model.py>LinearQNet</a> class represents a simple neural network architecture tailored for Q-value approximation in RL. It consists of two linear layers initialized during instantiation, with the first layer transforming input features to a hidden layer and the second layer producing Q-values for different actions. Additionally, it sets up other optional components, such as dropout regularization or weight initialization techniques, aiming to enhance the network&rsquo;s learning process and generalization ability.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, input_size, hidden_size, output_size):
</span></span><span style=display:flex><span>    super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>linear1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(input_size, hidden_size)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>linear2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(hidden_size, output_size)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>dropout <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dropout(<span style=color:#ae81ff>0.2</span>)  <span style=color:#75715e># Example: Adding dropout with p=0.2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Initialize weights using Xavier initialization</span>
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>xavier_uniform_(self<span style=color:#f92672>.</span>linear1<span style=color:#f92672>.</span>weight)
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>xavier_uniform_(self<span style=color:#f92672>.</span>linear2<span style=color:#f92672>.</span>weight)
</span></span></code></pre></div><p>The <a href="https://github.com/roaked/snake-q-learning-genetic-algorithm/search?q=def+forward+LinearQNet&amp;type=code"><code>forward</code></a> method defines the forward pass, applying a rectified linear unit (ReLU) activation to the hidden layer&rsquo;s output before generating the Q-values. Additionally, the <a href="https://github.com/roaked/snake-q-learning-genetic-algorithm/search?q=def+save+LinearQNet&amp;type=code"><code>save</code></a> method facilitates saving the model&rsquo;s state dictionary <code>self.state_dict()</code> to a specified file path using PyTorch&rsquo;s <code>torch.save</code> functionality, ensuring the preservation of trained model parameters.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>linear1(x))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>dropout(x)  <span style=color:#75715e># Applying dropout</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>linear2(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>save</span>(self, file_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;model.pth&#39;</span>):
</span></span><span style=display:flex><span>    model_folder_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;./model&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(model_folder_path):
</span></span><span style=display:flex><span>        os<span style=color:#f92672>.</span>makedirs(model_folder_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    file_name <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(model_folder_path, file_name)
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>save(self<span style=color:#f92672>.</span>state_dict(), file_name)    
</span></span></code></pre></div><p>Meanwhile, the QTrainer class manages the training process for the Q-network. Its initialization configures key parameters like learning rate and discount factor, along with setting up an Adam optimizer to update the model&rsquo;s parameters based on the provided learning rate.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>QTrainer</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, model, lr, gamma, target_update_freq<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>lr <span style=color:#f92672>=</span> lr
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>gamma <span style=color:#f92672>=</span> gamma
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>model <span style=color:#f92672>=</span> model
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>optimizer <span style=color:#f92672>=</span> optim<span style=color:#f92672>.</span>Adam(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>lr)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>criterion <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>MSELoss()
</span></span></code></pre></div><p>This can integrated in the <a href="https://github.com/roaked/snake-q-learning-genetic-algorithm/search?q=def+train_step+QTrainer&amp;type=code"><code>train_step</code></a> function which orchestrates the neural network training process by executing a single iteration of Q-learning updates. It receives an experience tuple containing information about a state, action, received reward, the resulting next state, and an indicator of whether the episode has ended. The function begins by converting these components into <code>PyTorch</code> tensors to facilitate computation within the neural network. Using the provided state, the neural network predicts Q-values for different actions, storing these predictions as <code>pred</code>. Now, we should take a look at how a new Q-value for a state-action pair based on the previous Q-value and the received reward, plus the discounted maximum Q-value achievable in the resulting state is computed. The process for the agent to adjust Q-values iteratively, gradually converging towards optimal action-selection strategies by learning from experiences obtained while interacting with the environment can be seen below for the Q-learning update rule.</p><span>\[
Q(s,a) = Q(s,a) + \alpha (r + \gamma \cdot max_{a'} Q(s',a')- Q(s,a))
\]</span><p>Q(s,a) represents the Q-value, indicating the expected cumulative reward by taking action <code>a</code> in state <code>s</code>. It quantifies the agent&rsquo;s understanding of the long-term desirability of choosing action <code>a</code> while in state <code>s</code>. The evaluation on whether to overwrite old information during upcoming Q-updates is given by the variable <span>\(\alpha\)
</span>. Higher learning rate (<span>
\(\alpha\)
</span>) gives more weight to recent experiences, influencing how much the Q-value changes based on the new piece of information. The agent gets an immediate reward <code>r</code> gained for the action after executing action <code>a</code> at this current state <code>s</code>. This immediate feedback guides the agent&rsquo;s learning, impacting the adjustment of Q-values based on the obtained rewards in each state-action pair. Moreover, the significance of future rewards compared to current rewards is given by <span>\(\gamma\)
</span>. Naturally higher discount factor <span>\(\gamma\)
</span>prioritizes long-term rewards more. Lastly, Q(s&rsquo;,a&rsquo;) indicates the maximum Q-value achievable in the subsequent state <code>s'</code> when taking an action <code>a'</code>. It influences agent&rsquo;s decision-making by taking into account potential future rewards based on the best action available from the next state, guiding him to more rewarding states.</p><p>Employing the Q-learning update rule, the function computes target Q-values based on the predicted values, updating them according to the received rewards and the next state&rsquo;s Q-values if the episode hasn&rsquo;t terminated. Subsequently, it calculates the Mean Squared Error (MSE) loss between the predicted and target Q-values and performs backpropagation to derive gradients for updating the neural network&rsquo;s parameters. Finally, the optimizer applies these gradients to adjust the model&rsquo;s weights through optimization, enhancing the network&rsquo;s ability to approximate accurate Q-values for efficient decision-making in RL scenarios.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_step</span>(self, state, action, reward, next_state, done):
</span></span><span style=display:flex><span>    state <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(state, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float)
</span></span><span style=display:flex><span>    next_state <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(next_state, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float)
</span></span><span style=display:flex><span>    action <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(action, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>long)
</span></span><span style=display:flex><span>    reward <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(reward, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    pred <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>model(state)
</span></span><span style=display:flex><span>    target <span style=color:#f92672>=</span> pred<span style=color:#f92672>.</span>clone()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Q-learning update rule</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Handling single-dimensional state and action tensors</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> state<span style=color:#f92672>.</span>dim() <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>:  <span style=color:#75715e># (1,x)</span>
</span></span><span style=display:flex><span>        state <span style=color:#f92672>=</span> state<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        next_state <span style=color:#f92672>=</span> next_state<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        action <span style=color:#f92672>=</span> action<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        reward <span style=color:#f92672>=</span> reward<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        done <span style=color:#f92672>=</span> (done,)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Predicting Q-values based on current state-action pair</span>
</span></span><span style=display:flex><span>    pred <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>model(state)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Clone the prediction for updating</span>
</span></span><span style=display:flex><span>    target <span style=color:#f92672>=</span> pred<span style=color:#f92672>.</span>clone()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> idx <span style=color:#f92672>in</span> range(len(done)):
</span></span><span style=display:flex><span>        Q_new <span style=color:#f92672>=</span> reward[idx]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> done[idx]:
</span></span><span style=display:flex><span>            Q_new <span style=color:#f92672>=</span> reward[idx] <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>gamma <span style=color:#f92672>*</span> torch<span style=color:#f92672>.</span>max(self<span style=color:#f92672>.</span>model(next_state[idx]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        action_idx <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>argmax(action[idx])<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>        target[idx][action_idx] <span style=color:#f92672>=</span> Q_new
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Zero the gradients, compute loss, backpropagate, and update weights</span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>criterion(target, pred)
</span></span><span style=display:flex><span>    loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>optimizer<span style=color:#f92672>.</span>step()
</span></span></code></pre></div><p>Together, these classes form the backbone of a Q-learning approach, where the LinearQNet acts as the neural network to estimate Q-values and the QTrainer orchestrates the training process by updating the network&rsquo;s parameters to improve Q-value predictions.</p><blockquote class="book-hint2 warning"><p class="hint-title warning"><svg class="book-icon"><use href="/svg/hint-icons.svg#warning-notice"/></svg><span>warning</span></p><p>Modifications:</p><ul><li>Experiment with deeper networks, different activation functions (Sigmoid, i.e), regularization L2/ Dropout to prevent overfitting (getting stuck) and use different optimizers (RMSprop or SGD) for <code>LinearQClass</code>.</li><li>It is possible to implement different loss functions or Double Q-learning to mitigate bias impact using separate networks to update Q-values.</li><li>Explore how to improve learning efficiency of the agent.</li></ul></blockquote><h3 id=12-deploying-a-reinforcement-learning-agent>1.2. Deploying a Reinforcement Learning Agent
<a class=anchor href=#12-deploying-a-reinforcement-learning-agent>#</a></h3><p>Let us start by applying the constructor method (<a href="https://github.com/roaked/snake-q-learning-genetic-algorithm/search?q=def+__init__+QLearningAgent&amp;type=code"><code>__init__</code></a>) of a class. This is where several fundamental attributes and objects are initialized for a RL agent.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>n_games <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span> <span style=color:#75715e># Number of games played</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>epsilon <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span> <span style=color:#75715e># Parameter for exploration-exploitation trade-off</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>gamma <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.9</span> <span style=color:#75715e># Discount factor for future rewards</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>memory <span style=color:#f92672>=</span> deque(maxlen<span style=color:#f92672>=</span>MAX_MEMORY) <span style=color:#75715e># Replay memory for storing experiences</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># QTrainer for model training </span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>trainer <span style=color:#f92672>=</span> QTrainer(self<span style=color:#f92672>.</span>model, lr<span style=color:#f92672>=</span>ALPHA, gamma<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>gamma) 
</span></span></code></pre></div><p><code>self.n_games</code> tracks the number of games the agent has played. <code>self.epsilon</code> represents a parameter essential for the <a href=https://en.wikipedia.org/wiki/Exploration-exploitation_dilemma>exploration-exploitation trade-off</a>, influencing the agent&rsquo;s decision-making process. The <code>self.gamma</code> variable signifies the discount factor applied to future rewards, impacting the agent&rsquo;s prioritization of immediate versus delayed rewards. The <code>deque</code> named <code>self.memory</code>, constrained by <code>MAX_MEMORY</code>, functions as a replay memory, storing previous experiences crucial for the agent&rsquo;s learning process. Additionally, the <code>self.trainer</code>, instantiated as <code>QTrainer</code>, facilitates model training using the <code>self.model</code>, employing the provided learning rate (<code>ALPHA</code>) and discount factor (<code>gamma</code>) within the <code>QTrainer</code> class.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Neural network model (input size, hidden layer no. neurons, output size)</span>
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>model <span style=color:#f92672>=</span> LinearQNet(<span style=color:#ae81ff>11</span>, <span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>3</span>) 
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>target_model <span style=color:#f92672>=</span> LinearQNet(<span style=color:#ae81ff>11</span>, <span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>3</span>)
</span></span></code></pre></div><p>The <code>self.model</code> is a neural network structure defined as <code>LinearQNet(11, 256, 3)</code> indicates specific architectural details tailored for a RL task. The choice of these parameters signifies the design of the neural network for this particular problem domain.</p><p>The &lsquo;11&rsquo; in the network signifies the input size, representing the number of features or variables characterizing the environment&rsquo;s state, which encompass details directions: vertical and horizontal distance, dangers based on moving forward, left or right and snake and food locations. The &lsquo;256&rsquo; hidden units denote the number of neurons in the hidden layer. This specific number, 256, is a heuristic or empirical choice commonly used across different domains: ranging from generative Convolutional Neural Networks (CNNs) architectures such as AlexNet or VGG to Natural Language Processing (NLPs) Recurrent Neural Network (RNNs) architectures. It provides a moderately sized layer that offers sufficient capacity for learning complex representations from the input data without overly increasing computational costs. Additionally, being a <a href=https://stackoverflow.com/questions/63515846/in-neural-networks-why-conventionally-set-number-of-neurons-to-2n>power of two, it aligns well with computational optimizations and hardware implementations</a>, making it computationally efficient for many systems. Lastly, the &lsquo;3&rsquo; as the output size corresponds to the number of actions the agent can undertake in the environment. In this case, having three outputs suggests that the agent has three distinct possible actions it can choose from in response to a given state in the environment. For instance, these actions represent movements such as &ldquo;left&rdquo;, &ldquo;right&rdquo; and &ldquo;forward&rdquo; seen previously in the development of <code>game.py</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_state</span>(self, game):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Extracting snake&#39;s head position and defining points in different directions</span>
</span></span><span style=display:flex><span>    head <span style=color:#f92672>=</span> game<span style=color:#f92672>.</span>snake[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    point_l <span style=color:#f92672>=</span> Point(head<span style=color:#f92672>.</span>x <span style=color:#f92672>-</span> <span style=color:#ae81ff>20</span>, head<span style=color:#f92672>.</span>y)
</span></span><span style=display:flex><span>    point_r <span style=color:#f92672>=</span> Point(head<span style=color:#f92672>.</span>x <span style=color:#f92672>+</span> <span style=color:#ae81ff>20</span>, head<span style=color:#f92672>.</span>y)
</span></span><span style=display:flex><span>    point_u <span style=color:#f92672>=</span> Point(head<span style=color:#f92672>.</span>x, head<span style=color:#f92672>.</span>y <span style=color:#f92672>-</span> <span style=color:#ae81ff>20</span>)
</span></span><span style=display:flex><span>    point_d <span style=color:#f92672>=</span> Point(head<span style=color:#f92672>.</span>x, head<span style=color:#f92672>.</span>y <span style=color:#f92672>+</span> <span style=color:#ae81ff>20</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Determining the snake&#39;s current direction</span>
</span></span><span style=display:flex><span>    dir_l <span style=color:#f92672>=</span> game<span style=color:#f92672>.</span>direction <span style=color:#f92672>==</span> Direction<span style=color:#f92672>.</span>LEFT
</span></span><span style=display:flex><span>    dir_r <span style=color:#f92672>=</span> game<span style=color:#f92672>.</span>direction <span style=color:#f92672>==</span> Direction<span style=color:#f92672>.</span>RIGHT
</span></span><span style=display:flex><span>    dir_u <span style=color:#f92672>=</span> game<span style=color:#f92672>.</span>direction <span style=color:#f92672>==</span> Direction<span style=color:#f92672>.</span>UP
</span></span><span style=display:flex><span>    dir_d <span style=color:#f92672>=</span> game<span style=color:#f92672>.</span>direction <span style=color:#f92672>==</span> Direction<span style=color:#f92672>.</span>DOWN
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Checking for potential dangers in different directions</span>
</span></span><span style=display:flex><span>    danger_straight <span style=color:#f92672>=</span> (dir_r <span style=color:#f92672>and</span> game<span style=color:#f92672>.</span>is_collision(point_r)) <span style=color:#f92672>or</span> \
</span></span><span style=display:flex><span>                      (dir_l <span style=color:#f92672>and</span> game<span style=color:#f92672>.</span>is_collision(point_l)) <span style=color:#f92672>or</span> \
</span></span><span style=display:flex><span>                      (dir_u <span style=color:#f92672>and</span> game<span style=color:#f92672>.</span>is_collision(point_u)) <span style=color:#f92672>or</span> \
</span></span><span style=display:flex><span>                      (dir_d <span style=color:#f92672>and</span> game<span style=color:#f92672>.</span>is_collision(point_d))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Indicating the snake&#39;s movement direction</span>
</span></span><span style=display:flex><span>    move_direction <span style=color:#f92672>=</span> [int(dir_l), int(dir_r), int(dir_u), int(dir_d)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Determining food&#39;s relative position compared to the snake&#39;s head</span>
</span></span><span style=display:flex><span>    food_position <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>        game<span style=color:#f92672>.</span>food<span style=color:#f92672>.</span>x <span style=color:#f92672>&lt;</span> game<span style=color:#f92672>.</span>head<span style=color:#f92672>.</span>x,  <span style=color:#75715e># food left</span>
</span></span><span style=display:flex><span>        game<span style=color:#f92672>.</span>food<span style=color:#f92672>.</span>x <span style=color:#f92672>&gt;</span> game<span style=color:#f92672>.</span>head<span style=color:#f92672>.</span>x,  <span style=color:#75715e># food right</span>
</span></span><span style=display:flex><span>        game<span style=color:#f92672>.</span>food<span style=color:#f92672>.</span>y <span style=color:#f92672>&lt;</span> game<span style=color:#f92672>.</span>head<span style=color:#f92672>.</span>y,  <span style=color:#75715e># food up</span>
</span></span><span style=display:flex><span>        game<span style=color:#f92672>.</span>food<span style=color:#f92672>.</span>y <span style=color:#f92672>&gt;</span> game<span style=color:#f92672>.</span>head<span style=color:#f92672>.</span>y   <span style=color:#75715e># food down</span>
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Constructing the state representation</span>
</span></span><span style=display:flex><span>    state <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>        int(danger_straight),  <span style=color:#75715e># Danger straight</span>
</span></span><span style=display:flex><span>        int(dir_r), int(dir_l), int(dir_u), int(dir_d),  <span style=color:#75715e># Move direction</span>
</span></span><span style=display:flex><span>    ] <span style=color:#f92672>+</span> food_position  <span style=color:#75715e># Food location</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>array(state, dtype<span style=color:#f92672>=</span>int)
</span></span></code></pre></div><p>The <a href="https://github.com/roaked/snake-q-learning-genetic-algorithm/search?q=def+get_state+QLearningAgent&amp;type=code"><code>get_state</code></a> function within the code constructs a comprehensive representation of the game state in the Snake environment. It begins by extracting vital information such as the snake&rsquo;s head position and defining points in multiple directions to detect potential dangers, which include positions 20 units away in various directions. The function then derives the snake&rsquo;s current direction by comparing it with predefined directional indicators (left, right, up, down) based on the game&rsquo;s orientation. It proceeds to assess the presence of <strong>potential dangers in the straight, right, and left directions</strong> by checking for collisions with specific points relative to the snake&rsquo;s current orientation. Furthermore, binary flags are employed to indicate the snake&rsquo;s movement direction, while the relative position of the food compared to the snake&rsquo;s head (left, right, up, down) is determined. Finally, all these features are flushed into an array that serves as a numeric representation of the game state.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_action</span>(self, state):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Select actions based on an epsilon-greedy strategy</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>epsilon <span style=color:#f92672>=</span> <span style=color:#ae81ff>80</span> <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>n_games
</span></span><span style=display:flex><span>        final_move <span style=color:#f92672>=</span> [<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>200</span>) <span style=color:#f92672>&lt;</span> self<span style=color:#f92672>.</span>epsilon:
</span></span><span style=display:flex><span>            move <span style=color:#f92672>=</span> random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>            final_move[move] <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            state0 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(state, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float)
</span></span><span style=display:flex><span>            prediction <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>model(state0)
</span></span><span style=display:flex><span>            move <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>argmax(prediction)<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>            final_move[move] <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> final_move
</span></span></code></pre></div><p>The <a href="https://github.com/roaked/snake-q-learning-genetic-algorithm/search?q=def+get_action+QLearningAgent&amp;type=code"><code>get_action</code></a> method operates as the decision-maker, employing an <a href=https://medium.com/@gridflowai/part-2-in-depth-exploration-on-epsilon-greedy-algorithm-2b19e59bbe22>epsilon-greedy strategy</a> to balance exploration and exploitation. This strategy dynamically adjusts the agent&rsquo;s behaviour by modifying the exploration rate (<code>epsilon</code>) based on the number of games played (<code>n_games</code>). If a randomnly generated value falls below the epsilon threshold, indicating exploration, the agent randomly selects an action from the available choices (<strong>move left, right, or straight</strong>). Conversely, in the exploitation phase, when the generated value surpasses the epsilon threshold, the agent exploits its learned knowledge. It leverages its neural network model (<code>self.model</code>) to predict Q-values for each potential action given the current state, selecting the action with the highest predicted Q-value. The resulting one-hot encoded representation (<code>final_move</code>) denotes the chosen action, guiding the agent&rsquo;s movement and decision-making process within the game.</p><h3 id=13-max-replay-buffer-and-target-network>1.3. Max Replay Buffer and Target Network
<a class=anchor href=#13-max-replay-buffer-and-target-network>#</a></h3><p>In RL, a replay buffer serves as a memory repository, capturing and retaining past experiences encountered by an agent during its interactions with an environment. This data structure enables the agent to reutilize and learn from diverse historical interactions by storing state-action-reward-next_state tuples. By decoupling the immediate use of experiences and instead sampling randomnly from this stored memory during training, the replay buffer breaks the temporal correlation between consecutive experiences, leading to more stable and efficient learning. Meanwhile, a target network, often employed in algorithms like Deep-Q-Networks (DQN), functions as a stabilized reference for target Q-values during training. This secondary neural network provides less frequently updated Q-value targets, addressing the issue of rapidly changing targets and enhancing training stability by decoupling the estimation of target Q-values from the primary network&rsquo;s parameters.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> collections <span style=color:#f92672>import</span> deque
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> random
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>MAX_MEMORY <span style=color:#f92672>=</span> <span style=color:#ae81ff>100_000</span>
</span></span><span style=display:flex><span>BATCH_SIZE <span style=color:#f92672>=</span> <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>QLearningAgent</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Initialize replay buffer</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>memory <span style=color:#f92672>=</span> deque(maxlen<span style=color:#f92672>=</span>MAX_MEMORY)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>remember</span>(self, state, action, reward, next_state, done):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Store experiences in memory</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>memory<span style=color:#f92672>.</span>append((state, action, reward, next_state, done))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_long_memory</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Perform training using experiences from the replay buffer</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> len(self<span style=color:#f92672>.</span>memory) <span style=color:#f92672>&lt;</span> BATCH_SIZE:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        mini_batch <span style=color:#f92672>=</span> random<span style=color:#f92672>.</span>sample(self<span style=color:#f92672>.</span>memory, BATCH_SIZE)
</span></span><span style=display:flex><span>        states, actions, rewards, next_states, dones <span style=color:#f92672>=</span> zip(<span style=color:#f92672>*</span>mini_batch)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>trainer<span style=color:#f92672>.</span>train_step(states, actions, rewards, next_states, dones)
</span></span></code></pre></div><p>The applied methodology enhances stability by mitigating overfitting to recent experiences and improves learning efficiency by allowing the agent to reuse and learn from its past interactions, contributing to more stable and effective training in RL algorithms. In the <code>agent.py</code>, particularly in the <code>train_and_record</code> function, the integration of the replay buffer involves augmenting the agent&rsquo;s interactions with the environment to store experiences and utilizing those experiences for training. As the agent interacts with the environment in each game step, the <code>remember</code> function within the <code>QLearningAgent</code> class captures the state-action-reward-next_state tuples and stores them in the replay buffer. They are essential for off-policy learning, and the <code>train_short_memory</code> and <code>train_long_memory</code> functions facilitate short-term and long-term learning, respectively. After each completed game, the agent leverages the stored experiences by calling <code>train_long_memory</code>, which samples a batch of experiences from the replay buffer and uses these experiences to update the agent&rsquo;s model via the <code>train_step</code> method in the <code>QTrainer</code> class. This integration facilitates learning from a diverse set of past interactions, contributing to more stable and efficient training by breaking temporal correlations between consecutive experiences. Adjusting memory replay features in the likes of <code>MAX_MEMORY</code> and <code>BATCH_SIZE</code> in addition to initialization variables allows not only for fine-tuning of the replay buffer&rsquo;s capacity and the size of experiences utilized for training, but also for studying the whole agent&rsquo;s learning process which can be studied using commonly known evolutionary algorithms such as: Genetic Algorithms (GAs).</p><h2 id=2-standard-rl>2 Standard RL
<a class=anchor href=#2-standard-rl>#</a></h2><p>The <strong>Standard RL</strong> mode is the baseline Deep Q-Learning pipeline: no evolutionary tuning, no handcrafted global planner, only the DQN agent learning from environment interaction plus replay-buffer updates. In this repository, <code>train_standard_rl()</code> calls <code>train_RL()</code> in <a href=https://github.com/roaked/snake-q-learning-genetic-algorithm/blob/main/agent_RL.py><code>agent_RL.py</code></a>.</p><h3 id=21-learning-signal-and-data-flow>2.1. Learning signal and data flow
<a class=anchor href=#21-learning-signal-and-data-flow>#</a></h3><p>At every frame, the agent builds a compact state vector (danger flags, direction, food relation), chooses one action with epsilon-greedy logic, receives reward feedback from <code>play_step</code>, and immediately trains on that transition (<code>short memory</code>). The same transition is also stored in replay memory, so episode-end updates can train on a broader historical sample (<code>long memory</code>).</p><p>This gives two complementary update scales:</p><ul><li><strong>Local adaptation</strong> from the newest transition (fast response).</li><li><strong>Global stabilization</strong> from replay sampling (less temporal bias).</li></ul><h3 id=22-code-snippet-core-baseline-step>2.2. Code snippet (core baseline step)
<a class=anchor href=#22-code-snippet-core-baseline-step>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>state_old <span style=color:#f92672>=</span> agent<span style=color:#f92672>.</span>get_state(game)
</span></span><span style=display:flex><span>final_move <span style=color:#f92672>=</span> agent<span style=color:#f92672>.</span>get_action(state_old)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>reward, done, score, collisions, steps <span style=color:#f92672>=</span> game<span style=color:#f92672>.</span>play_step(final_move)
</span></span><span style=display:flex><span>state_new <span style=color:#f92672>=</span> agent<span style=color:#f92672>.</span>get_state(game)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>agent<span style=color:#f92672>.</span>train_short_memory(state_old, final_move, reward, state_new, done)
</span></span><span style=display:flex><span>agent<span style=color:#f92672>.</span>remember(state_old, final_move, reward, state_new, done)
</span></span></code></pre></div><h3 id=23-episode-boundary-update>2.3. Episode boundary update
<a class=anchor href=#23-episode-boundary-update>#</a></h3><p>When an episode ends, the game resets and replay-based optimization is triggered:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>if</span> done:
</span></span><span style=display:flex><span>    game<span style=color:#f92672>.</span>_init_game()
</span></span><span style=display:flex><span>    agent<span style=color:#f92672>.</span>n_games <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    agent<span style=color:#f92672>.</span>train_long_memory()
</span></span></code></pre></div><p>This is the reference mode for all comparisons: if an advanced method does not beat this baseline in score trend, stability, or sample efficiency, it is not adding practical value.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>python agent_RL.py rl
</span></span></code></pre></div><h2 id=3-ga-optimized-rl>3 GA-Optimized RL
<a class=anchor href=#3-ga-optimized-rl>#</a></h2><p>The <strong>GA-Optimized RL</strong> mode combines DQN with a <strong>Genetic Algorithm (GA)</strong> that tunes training hyperparameters over time. In this repository, <code>train_ga_optimized()</code> calls <code>train()</code>, which runs DQN rollouts and periodically invokes <code>genetic.genetic(...)</code> to search stronger parameter settings.</p><ul><li><a href="https://youtu.be/CY_LEa9xQtg?t=2467">Risto Miikkulainen and Lex Fridman discussing the importance of neuroevolution</a> in deep networks: for instance, how applying evolutionary computation is helpful in assessing architecture topology or the layer depth</li></ul><h3 id=31-why-evolve-hyperparameters-during-training>3.1. Why evolve hyperparameters during training
<a class=anchor href=#31-why-evolve-hyperparameters-during-training>#</a></h3><p>A static hyperparameter set can be suboptimal across all phases of learning:</p><ul><li>Early phase usually benefits from more exploration.</li><li>Mid phase benefits from more stable optimization.</li><li>Later phase often requires stronger exploitation and less noise.</li></ul><p>GA addresses this by searching parameter candidates against real gameplay outcomes instead of relying on one fixed manual configuration.</p><h3 id=32-metrics-and-optimization-loop>3.2. Metrics and optimization loop
<a class=anchor href=#32-metrics-and-optimization-loop>#</a></h3><p>Candidate sets are evaluated using gameplay metrics such as:</p><ul><li><code>score</code></li><li><code>record</code></li><li><code>steps</code></li><li><code>collisions</code></li><li><code>same_positions</code></li></ul><p>Then selection/crossover/mutation generate the next candidate population.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>_, best_params, _ <span style=color:#f92672>=</span> genetic<span style=color:#f92672>.</span>genetic(
</span></span><span style=display:flex><span>    NUM_GENERATIONS,
</span></span><span style=display:flex><span>    score<span style=color:#f92672>=</span>score,
</span></span><span style=display:flex><span>    record<span style=color:#f92672>=</span>record,
</span></span><span style=display:flex><span>    steps<span style=color:#f92672>=</span>steps,
</span></span><span style=display:flex><span>    collisions<span style=color:#f92672>=</span>collisions,
</span></span><span style=display:flex><span>    same_positions_counter<span style=color:#f92672>=</span>same_positions_counter,
</span></span><span style=display:flex><span>    game_metrics_list<span style=color:#f92672>=</span>game_metrics_list
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><h3 id=33-runtime-integration-with-dqn>3.3. Runtime integration with DQN
<a class=anchor href=#33-runtime-integration-with-dqn>#</a></h3><p>The RL policy still trains with gradient updates, but the GA periodically adjusts optimization-relevant settings:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>if</span> isinstance(best_params, dict):
</span></span><span style=display:flex><span>    agent<span style=color:#f92672>.</span>update_hyperparameters(best_params)
</span></span></code></pre></div><p>This creates a hybrid system:</p><ul><li><strong>DQN</strong> learns action values.</li><li><strong>GA</strong> searches a better training regime for DQN itself.</li></ul><h3 id=34-is-rl-still-learning-if-it-is-not-winning>3.4. &ldquo;Is RL still learning if it is not winning?&rdquo;
<a class=anchor href=#34-is-rl-still-learning-if-it-is-not-winning>#</a></h3><p>Yes. In this codebase, learning updates happen even during losing episodes:</p><ul><li>Per-step update: <code>train_short_memory(...)</code> runs every transition.</li><li>Replay update: <code>train_long_memory()</code> runs at episode end.</li><li>Negative outcomes (collisions/death) are still training signals, not ignored data.</li></ul><p>So a GA-RL snake can lose many rounds and still update its Q-function. Losing does <strong>not</strong> mean &ldquo;no learning&rdquo;; it usually means &ldquo;learning is unstable, too slow, or misaligned with the objective&rdquo;.</p><h3 id=35-why-ga-rl-can-still-fail-often>3.5. Why GA-RL can still fail often
<a class=anchor href=#35-why-ga-rl-can-still-fail-often>#</a></h3><p>In adversarial or multi-agent-like settings (including same-board battle), the environment is effectively non-stationary: opponent behavior changes the transition distribution over time. This can degrade DQN stability even when updates are happening.</p><p>Common failure modes:</p><ol><li><strong>Noisy GA fitness signal</strong>: short evaluation windows overfit to luck-heavy episodes.</li><li><strong>Hyperparameter drift</strong>: frequent GA switching changes optimizer dynamics before DQN can consolidate.</li><li><strong>Objective mismatch</strong>: pure score-based fitness may underweight survival/win consistency.</li><li><strong>Opponent pressure</strong>: policy quality needed to survive is higher than solo-food optimization.</li></ol><h3 id=36-what-was-improved-in-this-repository>3.6. What was improved in this repository
<a class=anchor href=#36-what-was-improved-in-this-repository>#</a></h3><p>To reduce stagnation, the GA loop now uses:</p><ul><li>candidate-specific fitness tracking (instead of mismatched population-level reuse),</li><li>multi-game evaluation per candidate (<code>games_per_candidate=3</code>),</li><li>running-average fitness updates for smoother selection pressure,</li><li>trend-aware fitness bonus from recent score improvement,</li><li>elite retention + crossover + range-aware mutation,</li><li>rotating candidate schedule with periodic fallback to best-known parameters.</li></ul><p>These reduce collapse frequency, but they do not guarantee immediate win-rate dominance in battle mode; the setting is still hard and non-stationary.</p><h3 id=37-practical-interpretation-of-results>3.7. Practical interpretation of results
<a class=anchor href=#37-practical-interpretation-of-results>#</a></h3><p>When analyzing GA-RL runs, do not use only &ldquo;wins&rdquo;. Track:</p><ul><li>moving average score,</li><li>collision/death rate over windows,</li><li>win rate over windows,</li><li>stability across seeds/runs.</li></ul><p>If win-rate is flat but score trend and survival time improve, the agent is usually still learning useful behavior.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>python agent_RL.py ga
</span></span></code></pre></div><h2 id=4-hamiltonian>4 Hamiltonian
<a class=anchor href=#4-hamiltonian>#</a></h2><p>The <strong>Hamiltonian</strong> mode is a deterministic controller: it does not train a neural network and does not estimate Q-values. Instead, it follows a precomputed cycle that traverses the board safely. In code, <code>train_hamiltonian_cycle()</code> calls <code>train_hamiltonian()</code>.</p><h3 id=41-algorithm-intuition>4.1. Algorithm intuition
<a class=anchor href=#41-algorithm-intuition>#</a></h3><p>A Hamiltonian cycle visits each cell exactly once and returns to the start. If the snake follows this cycle consistently, it avoids the classic self-trap patterns that often end RL runs during long-body phases.</p><h3 id=42-cycle-driven-action-generation>4.2. Cycle-driven action generation
<a class=anchor href=#42-cycle-driven-action-generation>#</a></h3><p>The implementation creates neighbor lookups from the cycle and maps target cells to legal turn actions:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>cycle <span style=color:#f92672>=</span> _build_hamiltonian_cycle(game<span style=color:#f92672>.</span>width, game<span style=color:#f92672>.</span>height, block_size)
</span></span><span style=display:flex><span>next_lookup <span style=color:#f92672>=</span> {cycle[i]: cycle[(i <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>%</span> len(cycle)] <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(cycle))}
</span></span><span style=display:flex><span>prev_lookup <span style=color:#f92672>=</span> {cycle[(i <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>%</span> len(cycle)]: cycle[i] <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(cycle))}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>head <span style=color:#f92672>=</span> game<span style=color:#f92672>.</span>snake[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>next_candidates <span style=color:#f92672>=</span> [next_lookup<span style=color:#f92672>.</span>get(head), prev_lookup<span style=color:#f92672>.</span>get(head)]
</span></span></code></pre></div><p>Then each candidate is converted into the action encoding used by <code>play_step</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> candidate <span style=color:#f92672>in</span> next_candidates:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> candidate <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>    wanted_direction <span style=color:#f92672>=</span> _target_direction(head, candidate)
</span></span><span style=display:flex><span>    action <span style=color:#f92672>=</span> _direction_to_action(game<span style=color:#f92672>.</span>direction, wanted_direction)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> action <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        final_move <span style=color:#f92672>=</span> action
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>break</span>
</span></span></code></pre></div><h3 id=43-role-in-evaluation>4.3. Role in evaluation
<a class=anchor href=#43-role-in-evaluation>#</a></h3><p>Hamiltonian is valuable as a <strong>robustness baseline</strong>:</p><ul><li>no stochastic training variance,</li><li>deterministic and reproducible,</li><li>strong survival behavior on large-body states.</li></ul><p>Its trade-off is lower tactical flexibility; it may be slower than a strong RL policy at short-horizon food capture because it prioritizes cycle consistency over greedy local moves.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>python agent_RL.py hamiltonian
</span></span></code></pre></div></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/roaked/roaked.github.io/commit/ae2fb9b8ddc1a590c753b00101ec8fb95f1a3365 title='Last modified by roaked | February 26, 2026' target=_blank rel=noopener><img src=/svg/calendar.svg class=book-icon alt=Calendar>
<span>February 26, 2026</span></a></div><div><a class="flex align-center" href=https://github.com/roaked/roaked.github.io/edit/main/content/content/docs/code/snake-game/deepqnetwork.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Ricardo Chin</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#code-references-github-methods>Code References (GitHub Methods)</a></li></ul></li><li><a href=#1-reinforcement-deep-q-network-architecture>1 Reinforcement Deep Q-Network Architecture</a><ul><li><a href=#11-linearqnet-and-qtrainer-classes>1.1. LinearQNet and QTrainer Classes</a></li><li><a href=#12-deploying-a-reinforcement-learning-agent>1.2. Deploying a Reinforcement Learning Agent</a></li><li><a href=#13-max-replay-buffer-and-target-network>1.3. Max Replay Buffer and Target Network</a></li></ul></li><li><a href=#2-standard-rl>2 Standard RL</a><ul><li><a href=#21-learning-signal-and-data-flow>2.1. Learning signal and data flow</a></li><li><a href=#22-code-snippet-core-baseline-step>2.2. Code snippet (core baseline step)</a></li><li><a href=#23-episode-boundary-update>2.3. Episode boundary update</a></li></ul></li><li><a href=#3-ga-optimized-rl>3 GA-Optimized RL</a><ul><li><a href=#31-why-evolve-hyperparameters-during-training>3.1. Why evolve hyperparameters during training</a></li><li><a href=#32-metrics-and-optimization-loop>3.2. Metrics and optimization loop</a></li><li><a href=#33-runtime-integration-with-dqn>3.3. Runtime integration with DQN</a></li><li><a href=#34-is-rl-still-learning-if-it-is-not-winning>3.4. &ldquo;Is RL still learning if it is not winning?&rdquo;</a></li><li><a href=#35-why-ga-rl-can-still-fail-often>3.5. Why GA-RL can still fail often</a></li><li><a href=#36-what-was-improved-in-this-repository>3.6. What was improved in this repository</a></li><li><a href=#37-practical-interpretation-of-results>3.7. Practical interpretation of results</a></li></ul></li><li><a href=#4-hamiltonian>4 Hamiltonian</a><ul><li><a href=#41-algorithm-intuition>4.1. Algorithm intuition</a></li><li><a href=#42-cycle-driven-action-generation>4.2. Cycle-driven action generation</a></li><li><a href=#43-role-in-evaluation>4.3. Role in evaluation</a></li></ul></li></ul></nav></div></aside></main></body></html>