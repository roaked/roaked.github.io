<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Snake: Reinforcement Learning on Ricardo Chin</title><link>https://xsleaks.dev/docs/code/snake-game/</link><description>Recent content in Snake: Reinforcement Learning on Ricardo Chin</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://xsleaks.dev/docs/code/snake-game/index.xml" rel="self" type="application/rss+xml"/><item><title>BaseRL, GA-RL &amp; Hamiltonian</title><link>https://xsleaks.dev/docs/code/snake-game/deepqnetwork/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xsleaks.dev/docs/code/snake-game/deepqnetwork/</guid><description>&lt;h1 id="off-policy-and-dqn-parameter-optimization"&gt;
 &lt;strong&gt;Off Policy and DQN Parameter Optimization&lt;/strong&gt;
 &lt;a class="anchor" href="#off-policy-and-dqn-parameter-optimization"&gt;#&lt;/a&gt;
&lt;/h1&gt;
&lt;div style="display: flex; justify-content: space-between; gap: 10px; align-items: center;"&gt;
 &lt;video width="100%" height="auto" controls autoplay loop muted playsinline&gt;
 &lt;source src="https://xsleaks.dev/videos/first_snake.webm" type="video/webm"&gt;
 First Snake.
 &lt;/video&gt;
&lt;/div&gt;
&lt;p&gt;This section documents the &lt;a href="https://github.com/roaked/snake-q-learning-genetic-algorithm"&gt;&lt;code&gt;snake-q-learning-genetic-algorithm&lt;/code&gt;&lt;/a&gt; project: a Snake environment built with &lt;code&gt;pygame&lt;/code&gt; and trained with an off-policy Deep Q-Network (DQN), with genetic search used to tune key learning parameters.&lt;/p&gt;
&lt;p&gt;The implementation is organized around four core components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;game.py&lt;/code&gt; / &lt;code&gt;game_user.py&lt;/code&gt;: environment dynamics, collisions, food placement, and rendering.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;code&gt;model.py&lt;/code&gt;: Q-network definition (&lt;code&gt;LinearQNet&lt;/code&gt;) and training loop wrapper (&lt;code&gt;QTrainer&lt;/code&gt;).&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;code&gt;agent.py&lt;/code&gt;: epsilon-greedy action selection, replay-based training, and game-loop integration.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start="4"&gt;
&lt;li&gt;&lt;code&gt;genetic.py&lt;/code&gt;: parameter optimization over learning-rate, discount, dropout, and exploration ranges.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The goal is to maximize score while keeping training stable, then compare baseline RL behaviour against GA-tuned configurations under the same Snake rules.&lt;/p&gt;</description></item><item><title>Adversarial Multi-Agent RL, BattleMode</title><link>https://xsleaks.dev/docs/code/snake-game/adversarial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xsleaks.dev/docs/code/snake-game/adversarial/</guid><description>&lt;div style="display: flex; justify-content: space-between; gap: 10px; align-items: center;"&gt;
 &lt;video width="49%" height="auto" controls autoplay loop muted playsinline&gt;
 &lt;source src="https://xsleaks.dev/videos/first_competition.webm" type="video/webm"&gt;
 First Competition.
 &lt;/video&gt;
 &lt;video width="49%" height="auto" controls autoplay loop muted playsinline&gt;
 &lt;source src="https://xsleaks.dev/videos/third_competition.webm" type="video/webm"&gt;
 Second Competition.
 &lt;/video&gt;
&lt;/div&gt;
&lt;h1 id="adversarial-multi-agent-reinforcement-learning"&gt;
 &lt;strong&gt;Adversarial Multi-Agent Reinforcement Learning&lt;/strong&gt;
 &lt;a class="anchor" href="#adversarial-multi-agent-reinforcement-learning"&gt;#&lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;This page documents the &lt;strong&gt;battle mode&lt;/strong&gt; implemented in &lt;a href="https://github.com/roaked/snake-q-learning-genetic-algorithm/blob/main/agent_RL.py"&gt;&lt;code&gt;agent_RL.py&lt;/code&gt;&lt;/a&gt;, where three controllers are compared in the same experiment:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;GA-Optimized RL&lt;/strong&gt; (DQN + genetic hyperparameter updates)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;strong&gt;Standard RL&lt;/strong&gt; (baseline DQN)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;strong&gt;Hamiltonian&lt;/strong&gt; (deterministic cycle-following policy)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Unlike single-agent training, this setup emphasizes relative performance under shared constraints, direct competition, and robustness over many rounds.&lt;/p&gt;</description></item></channel></rss>