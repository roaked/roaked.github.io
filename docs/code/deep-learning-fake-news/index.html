<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="NLP: Language Features for Detection of Fake News # 1 A Growing Case # Fake news, a term emblematic of fabricated information intentionally disseminated across traditional news outlets or online social platforms, embodies deliberate disinformation strategies. These falsehoods aim to tarnish individuals, entities, or gain financial or political advantages, often employing misleading, attention-grabbing headlines. Some counterfeit news pieces disguise themselves as satirical content, sounding incredulous to the point of absurdity, yet managing to deceive unsuspecting audiences."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:title" content="Deep Learning on Fake News"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://xsleaks.dev/docs/code/deep-learning-fake-news/"><title>Deep Learning on Fake News | Ricardo Chin</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png><link rel=stylesheet href=/book.min.b74e85bd7803de00c09a320dcf09ae0d7e37702a9918995f5fe9d1c71c55a223.css integrity="sha256-t06FvXgD3gDAmjINzwmuDX43cCqZGJlfX+nRxxxVoiM=" crossorigin=anonymous><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-10WQY47KS2"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-10WQY47KS2",{anonymize_ip:!1})}</script><link rel=alternate type=application/rss+xml href=https://xsleaks.dev/docs/code/deep-learning-fake-news/index.xml title="Ricardo Chin"></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>Ricardo Chin</span></a></h2><ul><li class=book-section-flat><a href=/docs/design/>Design Repository</a><ul><li><a href=/docs/design/fea-beam-simulation/>FEA Beam Instability Modes</a><ul></ul></li><li><a href=/docs/design/industrial-crane-design/>Industrial Girder Crane</a><ul></ul></li><li><a href=/docs/design/finite-element-method-development/>FEM Package Development</a><ul></ul></li><li><a href=/docs/design/manual-transmission-design/>Gear Train Transmission</a><ul></ul></li><li><a href=/docs/design/electron-beam-tech/>Electron Beam Technology</a><ul></ul></li></ul></li><li class=book-section-flat><a href=/docs/code/>Coding Repository</a><ul><li><a href=/docs/code/uav/>UAV Red Bull Air Racing</a><ul><li><a href=/docs/code/uav/dynamics/>Drone Dynamics</a></li><li><a href=/docs/code/uav/continuous-controller-design/>Drone Continuous Controller</a></li><li><a href=/docs/code/uav/discrete-controller-design/>Drone Discrete Controller</a></li><li><a href=/docs/code/uav/computer-vision/>Drone Computer Vision</a></li></ul></li><li><a href=/docs/code/agv/>AGV: System Identification</a><ul></ul></li><li><a href=/docs/code/deep-learning-fake-news/ class=active>Deep Learning on Fake News</a><ul></ul></li><li><a href=/docs/code/hamiltonian-graphs/hamiltonian/>Hamiltonian Paths: Linked Lists</a></li><li><a href=/docs/code/bin-packing/>EC Optimization: Space & Time</a><ul><li><a href=/docs/code/bin-packing/genetic-algorithm/>Genetic Algorithm</a></li><li><a href=/docs/code/bin-packing/particle-swarm-optimization/>Particle Swarm Optimization</a></li></ul></li><li><a href=/docs/code/snake-game/>Snake Game: Genetic RL-DQN</a><ul></ul></li><li><a href=/docs/code/micromouse/>Micromouse: Flood Fill to A*</a><ul></ul></li><li><a href=/docs/code/batteries-ev/>PyBaMM-ML EV Battery Status</a><ul></ul></li></ul></li><li class=book-section-flat><a href=/docs/competitions/>Academic Competitions</a><ul><li><a href=/docs/competitions/fst/>Formula Student Lisbon</a></li><li><a href=/docs/competitions/thermocup/>ThermoCup</a></li></ul></li><li class=book-section-flat><a href=/docs/leet/>My LeetCode Solutions</a><ul></ul></li><li class=book-section-flat><a href=/docs/mod/>Website Modifications</a><ul></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Deep Learning on Fake News</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#1-a-growing-case>1 A Growing Case</a></li><li><a href=#2-approach-brainstorming>2 Approach Brainstorming</a></li><li><a href=#3-data-preprocessing>3 Data Preprocessing</a></li><li><a href=#4-methodology>4 Methodology</a><ul><li><a href=#41-clustering>4.1. Clustering</a></li><li><a href=#42-fuzzy-modelling>4.2. Fuzzy Modelling</a></li><li><a href=#43-artifficial-neural-networks>4.3. Artifficial Neural Networks</a></li></ul></li><li><a href=#5-outcomes>5 Outcomes</a><ul><li><a href=#51-results-interpretation>5.1. Results Interpretation</a></li><li><a href=#52-my-thoughts-on-applying-machine-learning-for-fake-news-detections>5.2. My Thoughts on Applying Machine Learning for Fake News Detections</a></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=nlp-language-features-for-detection-of-fake-news><strong>NLP: Language Features for Detection of Fake News</strong>
<a class=anchor href=#nlp-language-features-for-detection-of-fake-news>#</a></h1><h2 id=1-a-growing-case>1 A Growing Case
<a class=anchor href=#1-a-growing-case>#</a></h2><p>Fake news, a term emblematic of fabricated information intentionally disseminated across traditional news outlets or online social platforms, embodies deliberate disinformation strategies. These falsehoods aim to tarnish individuals, entities, or gain financial or political advantages, often employing misleading, attention-grabbing headlines. Some counterfeit news pieces disguise themselves as satirical content, sounding incredulous to the point of absurdity, yet managing to deceive unsuspecting audiences.</p><p>The proliferation of digital media and social networks has led to a rampant increase in fake news dissemination, presenting a contemporary societal challenge. Misinformation, with its potential to adversely impact lives, demands the crucial ability to differentiate between genuine and counterfeit news. This task is intricate; genuine news might appear implausible to the average reader, while fake news endeavours to appear credible.</p><p>Addressing this contemporary issue involves the automatic identification and prevention of fake news dissemination. Efforts by digital corporations and journalistic agencies have attempted to combat fake news, but these solutions have shown imperfections. Academic research has delved into understanding the propagation of fake news, recognizing language usage as a vital parameter in these investigations.</p><p><img src=https://media3.giphy.com/media/VDTOChMWX1BmFflzyr/giphy.gif alt=fakee></p><blockquote class="book-hint2 example"><p class="hint-title example"><svg class="book-icon"><use href="/svg/hint-icons.svg#example-notice"/></svg><span>example</span></p><p>Studies such as those by Mahyoob in his paper titled <a href=https://www.researchgate.net/publication/345997025_Linguistic-Based_Detection_of_Fake_News_in_Social_Media><em>&ldquo;Linguistic-Based Detection of Fake News in Social Media&rdquo;</em></a> and by Preston <a href=https://pubmed.ncbi.nlm.nih.gov/33705405/><em>&ldquo;Detecting fake news on Facebook: The role of emotional intelligence&rdquo;</em></a> shed light on the analysis of language characteristics in detecting fake news, providing insights, particularly for this application within the context of Portuguese news.</p><p>Additionally, reports by <a href=https://www.factcheck.org/fake-news/>Facebook and FactCheck.org</a> detail the challenges and strategies in combatting misinformation, emphasizing the significance of linguistic analysis in verifying news authenticity.</p></blockquote><p>In this project, drawing upon a meticulously curated corpus comprising <strong>3600 true and 3600 fake Portuguese news samples</strong>, collected from January 2016 to January 2018, I aimed to automatically identify fake news using aforementioned language characteristics. This endeavour relied on analyzing 21 specific language traits meticulously classified by the corpus&rsquo; authors to transform news articles into metadata, aligning with methodologies outlined by academic works and industry efforts in the field.</p><p>The goal of this project is to utilize established machine learning techniques, as previously outlined in research by Mahyoob and Preston, employing each language characteristic as a metadata feature, to effectively identify and mitigate the spread of fake news within Portuguese-language news sources.</p><h2 id=2-approach-brainstorming>2 Approach Brainstorming
<a class=anchor href=#2-approach-brainstorming>#</a></h2><p>Following the problem description, there are 21 features to be analyzed:</p><details><summary>Features (click to expand)</summary><div class=markdown-inner><table><thead><tr><th>Number</th><th>Feature</th></tr></thead><tbody><tr><td>1</td><td>number of tokens</td></tr><tr><td>2</td><td>number of words without punctuation</td></tr><tr><td>3</td><td>number of types</td></tr><tr><td>4</td><td>number of links inside the news</td></tr><tr><td>5</td><td>number of words in upper case</td></tr><tr><td>6</td><td>number of verbs</td></tr><tr><td>7</td><td>number of subjunctive and imperative verbs</td></tr><tr><td>8</td><td>number of nouns</td></tr><tr><td>9</td><td>number of adjectives</td></tr><tr><td>10</td><td>number of adverbs</td></tr><tr><td>11</td><td>number of modal verbs (mainly auxiliary verbs)</td></tr><tr><td>12</td><td>number of singular first and second personal pronouns</td></tr><tr><td>13</td><td>number of plural first personal pronouns</td></tr><tr><td>14</td><td>number of pronouns</td></tr><tr><td>15</td><td>pausality</td></tr><tr><td>16</td><td>number of characters</td></tr><tr><td>17</td><td>average sentence length</td></tr><tr><td>18</td><td>average word length</td></tr><tr><td>19</td><td>percentage of news with spelling errors</td></tr><tr><td>20</td><td>emotiveness</td></tr><tr><td>21</td><td>diversity</td></tr></tbody></table></div></details><p>The initial step towards obtaining meaningful results involves preprocessing the available data. Determining which features to utilize for further division into training and testing sets was a crucial decision point, expanded upon in the following section. <strong>How were these features selected?</strong></p><p>A thorough study was conducted employing statistical methods to assess the variability within each feature&rsquo;s dataset. Features demonstrating minimal variation, essentially stagnant in their values, were deemed non-contributory and subsequently excluded. Once these less informative features were removed, attention shifted to observing how these features varied between fake and true news samples. <strong>Four specific linguistic features</strong> were ultimately chosen, drawing from both statistical analysis and intuitive considerations.</p><blockquote class="book-hint2 important"><p class="hint-title important"><svg class="book-icon"><use href="/svg/hint-icons.svg#important-notice"/></svg><span>important</span></p>Two distinct datasets were created: one encompassing all <em>21 features</em> and another featuring <em>only the selected linguistic features</em>. Throughout the project, these sets were compared, and the resultant differences were discussed. It was anticipated that utilizing a mere four features, compared to the full 21, might yield inferior outcomes due to the reduced dataset facilitating the differentiation between fake and genuine news.</blockquote><p>Consideration for computational resources remained pivotal. The project emphasized optimizing computational efficiency, recognizing that certain models, such as clustering or neural networks, could demand substantial computation power. Maintaining a balance between model complexity and computational demand was crucial. Efficiency was prioritized without compromising noticeable accuracy outcomes.</p><p>Finally, acknowledging the variance in model results across simulations and the potential for parameter customization, efforts focused on identifying optimal parameter values for maximizing accuracy within each model. This iterative approach aimed to fine-tune model parameters for improved performance, considering the inherent variability in results across different simulations.</p><h2 id=3-data-preprocessing>3 Data Preprocessing
<a class=anchor href=#3-data-preprocessing>#</a></h2><blockquote class="book-hint2 warning"><p class="hint-title warning"><svg class="book-icon"><use href="/svg/hint-icons.svg#warning-notice"/></svg><span>warning</span></p>The dataset utilized in this project originated from the <a href=https://github.com/roneysco/Fake.br-Corpus>&ldquo;Fake.Br Corpus&rdquo; directly available at <em>Roney Santos&rsquo;</em> github page</a> specifically curated to encompass both true and false news in Brazilian Portuguese.</blockquote><p>This corpus originally contained complete news articles. However, the focus narrowed down to extract the essential features embedded within each news piece. All data was initially formatted in .txt files, necessitating the development of a <a href=https://github.com/roaked/fake-news-machine-learning/blob/main/Preprocessing.m>MATLAB script</a> to convert it into a more manageable .mat format. Alternatively, the following python can be used:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Number of news</span>
</span></span><span style=display:flex><span>N <span style=color:#f92672>=</span> list(range(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3603</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Remove news that don&#39;t exist for some reason</span>
</span></span><span style=display:flex><span>N<span style=color:#f92672>.</span>remove(<span style=color:#ae81ff>697</span>)
</span></span><span style=display:flex><span>N<span style=color:#f92672>.</span>remove(<span style=color:#ae81ff>1467</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>metaInputsTrue <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>metaInputsFake <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>metaTargetsTrue <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>metaTargetsFake <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> N:
</span></span><span style=display:flex><span>    <span style=color:#75715e># Assuming importfile function reads the data from a text file and returns a list or array</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># metaInputsTrue.append(importfile(f&#39;{i}-meta.txt&#39;))</span>
</span></span><span style=display:flex><span>    metaInputsFake<span style=color:#f92672>.</span>append(importfile(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>-meta.txt&#39;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># metaTargetsTrue.append([1, 0])</span>
</span></span><span style=display:flex><span>    metaTargetsFake<span style=color:#f92672>.</span>append([<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load metaInputs and metaTargets from the .mat files</span>
</span></span><span style=display:flex><span>metaInputs_data <span style=color:#f92672>=</span> scipy<span style=color:#f92672>.</span>io<span style=color:#f92672>.</span>loadmat(<span style=color:#e6db74>&#39;metaInputs.mat&#39;</span>)
</span></span><span style=display:flex><span>metaTargets_data <span style=color:#f92672>=</span> scipy<span style=color:#f92672>.</span>io<span style=color:#f92672>.</span>loadmat(<span style=color:#e6db74>&#39;metaTargets.mat&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># Access the loaded data</span>
</span></span><span style=display:flex><span>metaInputs <span style=color:#f92672>=</span> metaInputs_data[<span style=color:#e6db74>&#39;metaInputs&#39;</span>]
</span></span><span style=display:flex><span>metaTargets <span style=color:#f92672>=</span> metaTargets_data[<span style=color:#e6db74>&#39;metaTargets&#39;</span>]
</span></span><span style=display:flex><span><span style=color:#75715e># Combining the data</span>
</span></span><span style=display:flex><span>metaInputs <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>column_stack((metaInputsTrue, metaInputsFake))
</span></span><span style=display:flex><span>metaTargets <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>column_stack((metaTargetsTrue, metaTargetsFake))
</span></span><span style=display:flex><span><span style=color:#75715e># Convert lists to numpy arrays or Pandas DataFrames for further processing if needed</span>
</span></span><span style=display:flex><span>metaInputs <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(metaInputs)
</span></span><span style=display:flex><span>metaTargets <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(metaTargets)
</span></span></code></pre></div><p>As it is seen, this transformation yielded two primary files: &lsquo;metaInputs.mat,&rsquo; housing parameters for all news articles, and &lsquo;metaTargets.mat,&rsquo; distinguishing true news (indicated by a &lsquo;1&rsquo; in the first row) from false news (marked with a &lsquo;0&rsquo; in the second row). To simplify navigation, a structural layout was adopted: the first half of the parameter files consistently represented true news, while the subsequent half constituted false news. This deliberate arrangement facilitated easier comprehension through the interpretation of variables and generated plots.</p><blockquote class="book-hint2 important"><p class="hint-title important"><svg class="book-icon"><use href="/svg/hint-icons.svg#important-notice"/></svg><span>important</span></p>The dataset underwent a division into training and validation subsets. Employing a random selection method, <strong>75% of the dataset was allocated for model training, while the remaining 25% served as a validation set</strong>.</blockquote><p>Analyzing the pivotal features responsible for differentiating between authentic and deceptive news involved employing various statistical methods such as &lsquo;corrplot,&rsquo; &lsquo;matrixplot,&rsquo; and &lsquo;boxplot&rsquo;. However, the outcomes indicated that many features exhibited high non-linearity, posing a challenge in extracting meaningful correlations and insights.</p><p>The only meaningful contribution came from the <a href=%28https://github.com/roaked/fake-news-machine-learning/blob/main/Boxplot.py%29>Boxplot.py Python function</a> given its concise visualization using key statistics like the minimum, quartiles, median, and maximum values, providing insights into data distribution. It efficiently identifies outliers, assesses symmetry, measures data clustering, and detects potential skewness in the dataset.</p><h2 id=4-methodology>4 Methodology
<a class=anchor href=#4-methodology>#</a></h2><p>Previously, all methods were initially applied to the entire set of features, followed by a re-execution using only the linguistic features for comparison. This approach aimed to gauge the potential trade-off between accuracy and computational efficiency, as eliminating numerous features could expedite processing time. Moreover, the objective shifted from merely identifying blatantly obvious fake news (e.g., those with poor punctuation or grammar) to developing a model adept at detecting less instances of misinformation, as indicated by the selected linguistic features.</p><h3 id=41-clustering>4.1. Clustering
<a class=anchor href=#41-clustering>#</a></h3><p>The clustering classification method involves creating distinct clusters based on the available features and assigning each cluster a class label, distinguishing between true and fake news.</p><p>Two types of clustering techniques, fuzzy c-means and k-means clustering, were employed. Crisp clustering algorithms allocate each data point to a single cluster based on quantified similarity, while fuzzy clustering allows varying degrees of membership to multiple clusters, reflecting diverse similarities.</p><blockquote class="book-hint2 tip"><p class="hint-title tip"><svg class="book-icon"><use href="/svg/hint-icons.svg#tip-notice"/></svg><span>tip</span></p>Determining the optimal number of clusters was an initial consideration. Initially, there was a belief that higher cluster counts might yield better results, supported by a <a href=https://www.mathworks.com/help/stats/evalclusters.html>MATLAB function &rsquo;evalclusters&rsquo;</a>. However, a comprehensive study later revealed this wasn&rsquo;t always the case.</blockquote><p>Commencing with K-Means clustering, an algorithm using centroids and distance metrics, data points are associated with the nearest centroid, often calculated using squared Euclidean distances.</p><p>K-means clustering partitions observations into sets to minimize the within-cluster sum of squares. The <strong>objective function</strong> minimizes the variance by grouping observations into clusters.</p><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\[\text{Cost Function} = \text{argmin}_S k \sum_{i=1}^{k} \sum_{x \in S_i} \| x - \mu_i \|_2^2 = \text{argmin}_S k \sum_{i=1}^{k} |S_i| \text{Var}(S_i)\]</span><details><summary>Variables Description for K-means clustering (click to expand)</summary><div class=markdown-inner><ul><li>S denotes the set of clusters.</li><li>k represents the number of clusters.</li><li>x is a data point.</li><li>\mu_i signifies the centroid associated with cluster i.</li><li>S_i indicates the i^{th} cluster.</li><li>Var(S_i) represents the variance of cluster i.</li></ul></div></details><p>Identifying clusters containing fake news varied across simulations due to differing cluster numbering. To resolve this, the mode was employed to determine the cluster with the most data points, logically corresponding to fake news, given an equal split between true and fake data points. The following code was implemented:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-matlab data-lang=matlab><span style=display:flex><span><span style=color:#66d9ef>function</span> [kmeansTest,kmeansCluster] = <span style=color:#a6e22e>kmeansClustering</span>(trainingData,testingData,testingClass)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[pointsKM, <span style=color:#f92672>~</span>] = kmeans(trainingData<span style=color:#f92672>&#39;</span>, <span style=color:#ae81ff>6</span>);  <span style=color:#75715e>%6 clusters</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>MdlKDT = KDTreeSearcher(trainingData<span style=color:#f92672>&#39;</span>);
</span></span><span style=display:flex><span><span style=color:#75715e>% KDTreeSearcher object performs KNN (K-nearest-neighbor) search or</span>
</span></span><span style=display:flex><span><span style=color:#75715e>% radius search using a kd-tree. You can create a KDTreeSearcher object</span>
</span></span><span style=display:flex><span><span style=color:#75715e>% based on X</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i = <span style=color:#ae81ff>1</span>:length(testingData)
</span></span><span style=display:flex><span>obsNumber(i) = knnsearch(MdlKDT,testingData(:,i)<span style=color:#f92672>&#39;</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>% Knnsearch finds the nearest neighbor in X for each point in Y.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cluster(i) = pointsKM(obsNumber(i));
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fakeKMeans = mode(pointsKM);
</span></span><span style=display:flex><span>kmeansTest = testingClass(<span style=color:#ae81ff>1</span>,:);
</span></span><span style=display:flex><span>kmeansCluster = cluster;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i = <span style=color:#ae81ff>1</span>:length(kmeansCluster)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> kmeansCluster(i) <span style=color:#f92672>~=</span> fakeKMeans
</span></span><span style=display:flex><span>            kmeansCluster(i) = <span style=color:#ae81ff>1</span>;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>
</span></span><span style=display:flex><span>            kmeansCluster(i) = <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><p>Subsequently, fuzzy c-means clustering was executed, allowing data points to belong to multiple clusters with varying degrees of membership. Parameters such as the initial number of clusters &lsquo;c&rsquo; and the exponent controlling fuzzy overlap &rsquo;m&rsquo; were fine-tuned to optimize accuracy. The following code essentially loops through different parameter values, performs FCM, calculates accuracy, and identifies the best parameter value that maximizes accuracy. Then, it performs the clustering process again using the identified best parameter value.</p><p>The FCM algorithm partitions a collection of data into fuzzy clusters, returning cluster centers and a partition matrix indicating each data point&rsquo;s degree of belonging to clusters.</p><span>\[\text{Cost Function} = \text{argmin}_C \sum_{i=1}^{n} \sum_{i=1}^{c} w_{ij}^m \| x_i - c_j \|_2^2\]</span><details><summary>Variables Description for FCM clustering (click to expand)</summary><div class=markdown-inner><ul><li>C signifies the collection of clusters.</li><li>n represents the number of data elements.</li><li>c denotes the number of fuzzy clusters.</li><li>x_i represents a data point.</li><li>c_j signifies the j^{th} cluster center.</li><li>w_{ij} represents the degree to which x_i belongs to cluster j.</li><li>m represents the fuzzifier controlling cluster fuzziness.</li></ul></div></details><p>The following code aims to give a brief explanation how FCM was modeled. The code essentially loops through different parameter values, performs FCM, calculates accuracy, and identifies the best parameter value that maximizes accuracy. Then, it performs the clustering process again using the identified best parameter value. To start off, it iterates through differente &lsquo;p&rsquo; values and performs FCM for each single one of them.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-matlab data-lang=matlab><span style=display:flex><span><span style=color:#66d9ef>function</span> [cmeansTest,cmeansCluster,cmeansAcc,exponentValue] = <span style=color:#a6e22e>cmeansClustering</span>(trainingData,testingData,testingClass)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>p = <span style=color:#ae81ff>1.1</span>;
</span></span><span style=display:flex><span>u = <span style=color:#ae81ff>1</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> p=<span style=color:#ae81ff>1.1</span>:<span style=color:#ae81ff>0.1</span>:<span style=color:#ae81ff>3.5</span>
</span></span><span style=display:flex><span>    options = [p <span style=color:#ae81ff>150</span> <span style=color:#ae81ff>0.0000001</span> <span style=color:#ae81ff>0</span>];
</span></span><span style=display:flex><span>    exponentValue(u) = p;
</span></span><span style=display:flex><span>    [centersCM, <span style=color:#f92672>~</span>]= fcm(trainingData<span style=color:#f92672>&#39;</span>, <span style=color:#ae81ff>6</span>,options); <span style=color:#75715e>%6 clusters</span>
</span></span><span style=display:flex><span>    totalDelta = <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>    <span style=color:#75715e>%i = news index</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>%j = cluster index</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>%k = parameters index</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i = <span style=color:#ae81ff>1</span>:length(testingData)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> j = <span style=color:#ae81ff>1</span>:size(centersCM,<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>             <span style=color:#66d9ef>for</span> k = <span style=color:#ae81ff>1</span>:size(centersCM,<span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>                <span style=color:#75715e>%distance between each parameter k and each cluster j of news i</span>
</span></span><span style=display:flex><span>                delta = (testingData(k,i)<span style=color:#f92672>-</span>centersCM(j,k))^<span style=color:#ae81ff>2</span>;
</span></span><span style=display:flex><span>                totalDelta = totalDelta <span style=color:#f92672>+</span> delta;
</span></span><span style=display:flex><span>             <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e>%distance of news i to cluster j</span>
</span></span><span style=display:flex><span>            dist(j,i) = sqrt(totalDelta);
</span></span><span style=display:flex><span>            totalDelta = <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>        [<span style=color:#f92672>~</span>,ClusterIndex] = min(dist(:,i));
</span></span><span style=display:flex><span>        cluster(i) = ClusterIndex;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    fakeCMeans = mode(cluster);
</span></span><span style=display:flex><span>    cmeansTest = testingClass(<span style=color:#ae81ff>1</span>,:);
</span></span><span style=display:flex><span>    cmeansCluster = cluster;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i = <span style=color:#ae81ff>1</span>:length(cmeansCluster)
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>if</span> cmeansCluster(i) <span style=color:#f92672>~=</span> fakeCMeans
</span></span><span style=display:flex><span>              cmeansCluster(i) = <span style=color:#ae81ff>1</span>;
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>else</span>
</span></span><span style=display:flex><span>             cmeansCluster(i) = <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>u=u<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><p>Afterwards, the next part identifies the &lsquo;p&rsquo; value that gives the highest accuracy and performs clustering again using this value.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-matlab data-lang=matlab><span style=display:flex><span><span style=color:#66d9ef>function</span> [cmeansTest, cmeansCluster, cmeansAcc, exponentValue] = <span style=color:#a6e22e>cmeansClustering</span>(trainingData, testingData, testingClass)
</span></span><span style=display:flex><span>    p = <span style=color:#ae81ff>1.1</span>;
</span></span><span style=display:flex><span>    u = <span style=color:#ae81ff>1</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> p = <span style=color:#ae81ff>1.1</span>:<span style=color:#ae81ff>0.1</span>:<span style=color:#ae81ff>3.5</span>
</span></span><span style=display:flex><span>        options = [p <span style=color:#ae81ff>150</span> <span style=color:#ae81ff>0.0000001</span> <span style=color:#ae81ff>0</span>];
</span></span><span style=display:flex><span>        exponentValue(u) = p;
</span></span><span style=display:flex><span>        [centersCM, <span style=color:#f92672>~</span>] = fcm(trainingData<span style=color:#f92672>&#39;</span>, <span style=color:#ae81ff>6</span>, options); <span style=color:#75715e>% Fuzzy C-means with 6 clusters</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>% Clustering process, assigning clusters, and computing accuracy</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>% Same as before</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>% ...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        cmeansAcc(u) = stats.accuracy;
</span></span><span style=display:flex><span>        u = u <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><p>In the end, both FCM and k-means aim to minimize objective functions; however, the addition of membership values and the fuzzifier parameter in FCM allows for fuzzier clustering. The fuzzifier &rsquo;m&rsquo; determines the level of cluster fuzziness, with larger &rsquo;m&rsquo; values resulting in fuzzier clusters, while &rsquo;m=1&rsquo; implies crisp partitioning.</p><details><summary><strong>Study:</strong> Evaluation of exponent &rsquo;m&rsquo; for all features and linguistic features (click to expand)</summary><div class=markdown-inner><p>The observed trend indicates that the peak accuracy aligns with the lowest exponent value of &rsquo;m,&rsquo; typically slightly above one unit. Moreover, as the value of &rsquo;m&rsquo; increases, there is an observable exponential decrease in accuracy.</p><p><img src=https://live.staticflickr.com/65535/53342293266_18477c93e5_c.jpg alt="Max Accuracy vs. Fuzzy Partition Exponent (m) for all features"></p><p>Applying clustering to linguistic features followed a similar process. The analysis revealed a maximum accuracy of 87.51% concerning the exponent value.</p><p><img src=https://live.staticflickr.com/65535/53342617799_f2443f4c18_c.jpg alt="Max Accuracy vs. Fuzzy Partition Exponent (m) for linguistic features"></p></div></details><h3 id=42-fuzzy-modelling>4.2. Fuzzy Modelling
<a class=anchor href=#42-fuzzy-modelling>#</a></h3><p>Fuzzy modeling uses rules that are like &ldquo;if-then&rdquo; statements in everyday language. These rules connect input to output in systems that work with vague or uncertain information. There are two main types of rules: one that&rsquo;s easier for people to understand and gives fuzzy (not exact) outputs, and another that&rsquo;s more mathematical, precise, and better for complex systems.</p><p>To make a fuzzy system, you start by grouping similar data together, allowing for some overlap between the groups. There are different ways to do this grouping. The number of groups usually matches the number of rules used in the system.</p><p>In these systems, the choice of rules affects how well they work for different tasks. Considering these differences, one type of rule, the Takagi-Sugeno model, was chosen for a specific case because it&rsquo;s better suited for complex systems and provides precise outputs.</p><details><summary><strong>Example:</strong> Car&rsquo;s adaptive cruise control system with fuzzy logic - (click to expand)</summary><div class=markdown-inner><ul><li>If the distance to the car in front is relatively close and the speed is moderately high, then reduce acceleration slightly.</li><li>If the distance to the car in front is quite far and the speed is low, then increase acceleration moderately.</li></ul><p>In this scenario, fuzzy logic allows the system to interpret vague terms like &ldquo;relatively close&rdquo; or &ldquo;quite far&rdquo; regarding the distance to the car ahead. If the system were based on the Takagi-Sugeno model, it would precisely adjust acceleration based on these conditions, ensuring smoother driving and safer distance management.</p></div></details><p>In setting up the fuzzy model, the threshold for the membership function was carefully selected to prioritize minimizing false negatives over false positives. This choice aimed to err on the side of categorizing genuine news as potentially fake rather than labeling false news as true. To pinpoint the most effective threshold value, a dedicated function was designed to identify the optimal point that maximizes the model&rsquo;s accuracy. Across various simulations, this optimal threshold typically fell between 0.45 and 0.55.</p><p>Let&rsquo;s assume an ideal threshold of 0.51: a visual representation illustrates this point. News pieces with a membership value above 0.51 were classified as true (shown above the black lines), while those below were categorized as fake.</p><p><img src=https://live.staticflickr.com/65535/53342552453_91d5f2cc05_c.jpg alt="Optimal Threshold"></p><p>It was previously mentioned that increasing the number of clusters might not always lead to higher accuracy in Fuzzy Modeling. To confirm this, a study was conducted to explore the relationship between accuracy and the number of clusters. The following script implemented where initially a fuzzy model was trained using different cluster numbers to obtain the optimal cluster number.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-matlab data-lang=matlab><span style=display:flex><span>Input = trainingData<span style=color:#f92672>&#39;</span>;
</span></span><span style=display:flex><span>Output = trainingClass<span style=color:#f92672>&#39;</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>DAT.U = Input;
</span></span><span style=display:flex><span>DAT.Y = Output(:, <span style=color:#ae81ff>1</span>);
</span></span><span style=display:flex><span>clusterNumber(<span style=color:#ae81ff>1</span>) = <span style=color:#ae81ff>2</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> j = <span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>20</span>
</span></span><span style=display:flex><span>    clusterNumber(j) = clusterNumber(<span style=color:#ae81ff>1</span>) <span style=color:#f92672>+</span> j <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>;
</span></span><span style=display:flex><span>    STR.c = clusterNumber(j);
</span></span><span style=display:flex><span>    [FM, <span style=color:#f92672>~</span>] = fmclust(DAT, STR);
</span></span><span style=display:flex><span>    [Ym,<span style=color:#f92672>~</span>,<span style=color:#f92672>~</span>,<span style=color:#f92672>~</span>,<span style=color:#f92672>~</span>] = fmsim(testingData<span style=color:#f92672>&#39;</span>,testingClass<span style=color:#f92672>&#39;</span>,FM); 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>%Threshold to define if fake or true</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    Threshold = <span style=color:#ae81ff>0.00</span>;
</span></span><span style=display:flex><span>    MaxAccuracy(j) = <span style=color:#ae81ff>0.00</span>;
</span></span><span style=display:flex><span>    MaxThreshold = <span style=color:#ae81ff>0.00</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span>(Threshold <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>1.00</span>) 
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i = <span style=color:#ae81ff>1</span>:size(Ym)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> Ym(i) <span style=color:#f92672>&gt;</span> Threshold
</span></span><span style=display:flex><span>                YClass(i) = <span style=color:#ae81ff>1</span>;
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>else</span>
</span></span><span style=display:flex><span>              YClass(i) = <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>        stats = confusionmatStats(testingClass(<span style=color:#ae81ff>1</span>,:),YClass);
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> MaxAccuracy(j) <span style=color:#f92672>&lt;</span> stats.accuracy       
</span></span><span style=display:flex><span>            MaxAccuracy(j) = stats.accuracy;
</span></span><span style=display:flex><span>            MaxThreshold = Threshold;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>end</span>   
</span></span><span style=display:flex><span>        Threshold = Threshold <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.01</span>;    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>end</span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i = <span style=color:#ae81ff>1</span>:size(Ym)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> Ym(i) <span style=color:#f92672>&gt;</span> MaxThreshold
</span></span><span style=display:flex><span>            YClassOptimal(i) = <span style=color:#ae81ff>1</span>;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>
</span></span><span style=display:flex><span>            YClassOptimal(i) = <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>end</span>  
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><p>Afterwards, the cluster number that provided the highest accuracy is selected and optimal model using this cluster number is computed.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-matlab data-lang=matlab><span style=display:flex><span>maximum = max(MaxAccuracy);
</span></span><span style=display:flex><span>[<span style=color:#f92672>~</span>, highestCluster] = find(MaxAccuracy <span style=color:#f92672>==</span> maximum);
</span></span><span style=display:flex><span>STR.c = clusterNumber(highestCluster(<span style=color:#ae81ff>1</span>));
</span></span><span style=display:flex><span>[FM, <span style=color:#f92672>~</span>] = fmclust(DAT, STR);
</span></span><span style=display:flex><span>[Ym,<span style=color:#f92672>~</span>,<span style=color:#f92672>~</span>,<span style=color:#f92672>~</span>,<span style=color:#f92672>~</span>] = fmsim(testingData<span style=color:#f92672>&#39;</span>,testingClass<span style=color:#f92672>&#39;</span>,FM); 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>%Threshold to define if fake or true</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    Threshold = <span style=color:#ae81ff>0.00</span>;
</span></span><span style=display:flex><span>    MaxAccuracy(j) = <span style=color:#ae81ff>0.00</span>;
</span></span><span style=display:flex><span>    MaxThreshold = <span style=color:#ae81ff>0.00</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span>(Threshold <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>1.00</span>) 
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i = <span style=color:#ae81ff>1</span>:size(Ym)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> Ym(i) <span style=color:#f92672>&gt;</span> Threshold
</span></span><span style=display:flex><span>                YClass(i) = <span style=color:#ae81ff>1</span>;
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>else</span>
</span></span><span style=display:flex><span>              YClass(i) = <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>     <span style=color:#75715e>%plotconfusion(testingClass(1,:),YClass)</span>
</span></span><span style=display:flex><span>    stats = confusionmatStats(testingClass(<span style=color:#ae81ff>1</span>,:),YClass);
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> MaxAccuracy(j) <span style=color:#f92672>&lt;</span> stats.accuracy     
</span></span><span style=display:flex><span>            MaxAccuracy(j) = stats.accuracy;
</span></span><span style=display:flex><span>            MaxThreshold = Threshold;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>    Threshold = Threshold <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.01</span>;    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i = <span style=color:#ae81ff>1</span>:size(Ym)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> Ym(i) <span style=color:#f92672>&gt;</span> MaxThreshold
</span></span><span style=display:flex><span>            YClassOptimal(i) = <span style=color:#ae81ff>1</span>;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>
</span></span><span style=display:flex><span>            YClassOptimal(i) = <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><details><summary><strong>Study:</strong> Impact of numbers of clusters on the model&rsquo;s accuracy - (click to expand)</summary><div class=markdown-inner><p>For all features:</p><p><img src=https://live.staticflickr.com/65535/53342663434_5eb9800f9f_c.jpg alt="21 features"></p><p>For linguistic features:</p><p><img src=https://live.staticflickr.com/65535/53341450212_925f6179e7.jpg alt="4 features"></p><p>While an increase in clusters appears to enhance the overall consistency of average maximum accuracy, an interesting observation arises. In the simulation involving the highest number of clusters (represented as the last point), the resulting accuracy doesn&rsquo;t perfectly align with the maximum accuracy achieved. Considering that all features were utilized to attain this result, and the accuracy was already quite satisfactory, the differences among the increasing clusters aren&rsquo;t distinctly noticeable.</p><p>For in-depth study, extracting the membership functions for each features could be done by consulting <a href="https://de.mathworks.com/matlabcentral/fileexchange/47171-constrained-fuzzy-model-identification-files-for-fuzzy-modeling-and-identification-toolbox?s_tid=srchtitle">Fuzzy Modeling and Identification Toolbox fm2tex function</a></p></div></details><h3 id=43-artifficial-neural-networks>4.3. Artifficial Neural Networks
<a class=anchor href=#43-artifficial-neural-networks>#</a></h3><details><summary><strong>Background:</strong> What are Artifficial Neural Networks and how do they work? - (click to expand)</summary><div class=markdown-inner><p>Artificial Neural Networks (ANNs) were initially inspired by the human brain&rsquo;s structure to handle tasks where traditional algorithms struggled. To implement an effective ANN, understanding how it predicts various inputs, its associated phases, and its adaptability to new, unknown inputs is crucial.</p><p>A Multilayer Perceptron (MLP) is a type of feedforward ANN composed of an input layer, a hidden layer, and an output layer. The hidden and output layers consist of nonlinear activation function-equipped neurons. Unlike linear perceptrons, MLPs employ supervised learning, specifically the backpropagation technique, allowing them to handle non-linear data by adjusting node biases and connection weights.</p><p>During training, the network adjusts biases and weights via backpropagation to compute output values using input weights and activation functions. To compute weight adjustments <strong>(δw_ij)</strong>, the network calculates the gradient of the Mean Squared Error (MSE) cost function and multiplies it by a learning rate <strong>(α)</strong>.</p><p>Validation ensures the network&rsquo;s performance by comparing computed and known outputs, detecting overfitting. Testing evaluates the fully trained network using a separate set of examples.</p><p>The pattern recognition neural network chosen aims to classify news as true or fake based on news parameters. Data partitioned for training (70%), validation (5%), and testing (25%) using dividerand function. Mean Squared Error as the cost function was preferred over Mean Absolute Error due to its suitability for situations where large errors are undesirable.</p></div></details><p>Various training methods exist, with factors like problem type, network size, and memory influencing selection. For this project, Levenberg-Marquardt (LM) algorithm was chosen due to its robustness and standard usage for pattern recognition problems.</p><p>The activation function &rsquo;tansig&rsquo; (hyperbolic tangent sigmoid) was utilized for neurons to ensure smooth activation, less computation demand, and easier weight learning, aligning well with the LM algorithm.</p><p>The number of neurons in the hidden layer was also left to the user’s choice, it was varied between 5, 10 and 15 neurons.</p><table><thead><tr><th>Layers & Neurons</th><th>All Features</th><th>Linguistic Features</th></tr></thead><tbody><tr><td>[ 5 ]</td><td>96.7%</td><td>89.1%</td></tr><tr><td>[ 10 ]</td><td>96.5%</td><td>89.1%</td></tr><tr><td>[ 15 ]</td><td>96.5%</td><td>89.3%</td></tr><tr><td>[2 2]</td><td>96.8%</td><td>88.7%</td></tr><tr><td>[5 5]</td><td>96.6%</td><td>89.0%</td></tr><tr><td>[10 10]</td><td>96.8%</td><td>89.0%</td></tr><tr><td>[2 2 2]</td><td>96.7%</td><td>88.7%</td></tr><tr><td>[4 4 4]</td><td>96.7%</td><td>88.6%</td></tr><tr><td>[6 6 6]</td><td>96.6%</td><td>88.6%</td></tr><tr><td>[2 2 2 2]</td><td>96.8%</td><td>88.8%</td></tr><tr><td>[4 4 4 4]</td><td>96.8%</td><td>88.7%</td></tr></tbody></table><h2 id=5-outcomes>5 Outcomes
<a class=anchor href=#5-outcomes>#</a></h2><p>Using the available function in MATLAB and <a href=https://github.com/roaked/fake-news-machine-learning/blob/main/confusionmatStats.py>the developed Python script</a> (see below.) to retrieve the confusion matrices and the remaining metrics, it was possible to study certain parameters such as accuracy. For simplicity sake; I will just list down the results.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>confusionmatStats</span>(group, grouphat<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> grouphat <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        value1 <span style=color:#f92672>=</span> group
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        value1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[np<span style=color:#f92672>.</span>sum((group <span style=color:#f92672>==</span> i) <span style=color:#f92672>&amp;</span> (grouphat <span style=color:#f92672>==</span> j)) <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>unique(grouphat)] <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>unique(group)])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    numOfClasses <span style=color:#f92672>=</span> value1<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    totalSamples <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sum(value1)
</span></span><span style=display:flex><span>    accuracy <span style=color:#f92672>=</span> (<span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>trace(value1) <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> value1)) <span style=color:#f92672>/</span> (numOfClasses <span style=color:#f92672>*</span> totalSamples) <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    TP <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>diag(value1)
</span></span><span style=display:flex><span>    FP <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sum(value1, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>) <span style=color:#f92672>-</span> TP
</span></span><span style=display:flex><span>    FN <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sum(value1, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>) <span style=color:#f92672>-</span> TP
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Calculate TN without deletions</span>
</span></span><span style=display:flex><span>    TN <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sum(value1) <span style=color:#f92672>-</span> (FP <span style=color:#f92672>+</span> FN <span style=color:#f92672>+</span> TP)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    sensitivity <span style=color:#f92672>=</span> TP <span style=color:#f92672>/</span> (TP <span style=color:#f92672>+</span> FN)
</span></span><span style=display:flex><span>    specificity <span style=color:#f92672>=</span> TN <span style=color:#f92672>/</span> (FP <span style=color:#f92672>+</span> TN)
</span></span><span style=display:flex><span>    precision <span style=color:#f92672>=</span> TP <span style=color:#f92672>/</span> (TP <span style=color:#f92672>+</span> FP)
</span></span><span style=display:flex><span>    f_score <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> TP <span style=color:#f92672>/</span> (<span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> TP <span style=color:#f92672>+</span> FP <span style=color:#f92672>+</span> FN)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    stats <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;confusionMat&#39;</span>: value1,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;accuracy&#39;</span>: accuracy,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;sensitivity&#39;</span>: sensitivity,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;specificity&#39;</span>: specificity,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;precision&#39;</span>: precision,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;recall&#39;</span>: sensitivity,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;Fscore&#39;</span>: f_score
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> stats
</span></span></code></pre></div><h3 id=51-results-interpretation>5.1. Results Interpretation
<a class=anchor href=#51-results-interpretation>#</a></h3><p>After generating the confusion matrix plots, a comprehensive table was compiled to encompass all available accuracy metrics.</p><table><thead><tr><th>Model Methods</th><th>ANN</th><th>T-S FIS</th><th>FCM</th><th>KM</th></tr></thead><tbody><tr><td>All Features Accuracy</td><td>97.3%</td><td>95.9%</td><td>94.1%</td><td>94.1%</td></tr><tr><td>All Features True Positive</td><td>96.6%</td><td>93.8%</td><td>93.3%</td><td>93.3%</td></tr><tr><td>All Features False Positive</td><td>2.9%</td><td>1.9%</td><td>5.2%</td><td>5.2%</td></tr><tr><td>Linguistic Features Accuracy</td><td>89.0%</td><td>87.6%</td><td>87.1%</td><td>87.0%</td></tr><tr><td>Linguistic Features True Positive</td><td>86.7%</td><td>85.8%</td><td>89.2%</td><td>89.1%</td></tr><tr><td>Linguistic Features False Positive</td><td>8.8%</td><td>10.5%</td><td>15.1%</td><td>15.2%</td></tr></tbody></table><p>Upon reviewing the results from the testing set, the anticipated hierarchy of performance among models held true: neural networks outperformed fuzzy models, which, in turn, surpassed clustering methods. The accuracies obtained for all features were as follows: Artificial Neural Network (ANN) at 97.3%, Takagi-Sugeno Fuzzy Inference System (T-S FIS) at 95.9%, Fuzzy Clustering Means (FCM) at 94.1%, and K-Means (KM) also at 94.1%. As expected, the use of only linguistic features resulted in lower accuracies due to fewer comparison terms.</p><blockquote class="book-hint2 tip"><p class="hint-title tip"><svg class="book-icon"><use href="/svg/hint-icons.svg#tip-notice"/></svg><span>tip</span></p>Observing the performance on training versus testing sets revealed slight overfitting in the NN and T-S FIS models, showcasing approximately 1% higher accuracy in the training set. To address this issue, augmenting the dataset and employing regularization techniques could enhance model generalization, ensuring better learning of patterns from the training data.</blockquote><p>Interestingly, FCM and KM showed closely aligned results, especially with linguistic features where they were identical. Altering the exponent for the fuzzy partition matrix could prompt FCM to converge towards KM values.</p><p>All methods exhibited accuracies exceeding 94% when using all features, indicating proficient performance in categorizing news. This implies that out of 7200 news pieces, more than 6760 were accurately identified based on their features, with the highest accuracy of 97.3% signaling misclassification of only about 200 news articles.</p><p>Using solely linguistic features yielded satisfactory results, with all methods achieving accuracy equal to or greater than 87%. This indicates that, among 7200 news items, more than 6264 were correctly categorized based on their features. However, the highest accuracy of 89.1% suggested misidentification of 784 news articles.</p><blockquote class="book-hint2 warning"><p class="hint-title warning"><svg class="book-icon"><use href="/svg/hint-icons.svg#warning-notice"/></svg><span>warning</span></p>It&rsquo;s noteworthy that employing a vast number of clusters significantly escalates computational demands. Balancing computational efficiency against marginal performance improvements is crucial, as extended computation time might not necessarily yield substantial enhancements in results.</blockquote><p><img src=https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExNmZ0Z3U0MG94a3hpOWY1ZjVuNGFtc2ltZXg5MTlobmhvbWY3YXN6diZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/l3q2E6XD7P7Q0n184/giphy.gif alt=fake></p><h3 id=52-my-thoughts-on-applying-machine-learning-for-fake-news-detections>5.2. My Thoughts on Applying Machine Learning for Fake News Detections
<a class=anchor href=#52-my-thoughts-on-applying-machine-learning-for-fake-news-detections>#</a></h3><p>Wrapping things up, the ANN, T-S FIS, C-M, and K-M methods all delivered pretty solid outcomes, with the ANN standing out as the star performer here.</p><p>The accuracy obtained was good enough for the work performed, but in the real world, aiming for near-perfect accuracy — like nudging towards that 100% mark—holds serious weight. Think about it, labeling something as fake when it&rsquo;s not, or the other way around, carries hefty ethical, legal, and economic implications.</p><p>Boosting accuracy is a puzzle. Tweaking model parameters or even experimenting with more clusters might help, but there&rsquo;s a catch — those simulations could drag on forever. Another trick is amping up the features, like diving into the writing style or digging into the website URL to assess if the source is trustworthy. But again, piling up features might lead to overfitting headaches.</p><p>I could&rsquo;ve explored different models too; they might&rsquo;ve bumped up the accuracy. Or, splitting news by their subject could&rsquo;ve been a game-changer. I mean, the language in political news can be totally different from religious or society-related stuff.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/roaked/roaked.github.io/commit/c59df2bd3e92ddd63c53603062281293e5245497 title='Last modified by roaked | December 15, 2023' target=_blank rel=noopener><img src=/svg/calendar.svg class=book-icon alt=Calendar>
<span>December 15, 2023</span></a></div><div><a class="flex align-center" href=https://github.com/roaked/roaked.github.io/edit/main/content/content/docs/code/deep-learning-fake-news/_index.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Ricardo Chin</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#1-a-growing-case>1 A Growing Case</a></li><li><a href=#2-approach-brainstorming>2 Approach Brainstorming</a></li><li><a href=#3-data-preprocessing>3 Data Preprocessing</a></li><li><a href=#4-methodology>4 Methodology</a><ul><li><a href=#41-clustering>4.1. Clustering</a></li><li><a href=#42-fuzzy-modelling>4.2. Fuzzy Modelling</a></li><li><a href=#43-artifficial-neural-networks>4.3. Artifficial Neural Networks</a></li></ul></li><li><a href=#5-outcomes>5 Outcomes</a><ul><li><a href=#51-results-interpretation>5.1. Results Interpretation</a></li><li><a href=#52-my-thoughts-on-applying-machine-learning-for-fake-news-detections>5.2. My Thoughts on Applying Machine Learning for Fake News Detections</a></li></ul></li></ul></nav></div></aside></main></body></html>