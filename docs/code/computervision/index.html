<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  1 Abstract
  #

This research concerns the structure from motion from two views, which is the process of estimating the 3D structure of a scene from a set of 2D views. This requires proper camera calibration to ensure that the 2D images can be used for metric information extraction. Afterwards, the poses of the calibrated camera can be estimated from two images, then the 3D structure can be reconstructed up to an unknown scale factor, which can be found by detecting an object of known size."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://xsleaks.dev/docs/code/computervision/"><meta property="og:site_name" content="Ricardo Chin"><meta property="og:title" content="Structure From Motion: Camera"><meta property="og:description" content="1 Abstract # This research concerns the structure from motion from two views, which is the process of estimating the 3D structure of a scene from a set of 2D views. This requires proper camera calibration to ensure that the 2D images can be used for metric information extraction. Afterwards, the poses of the calibrated camera can be estimated from two images, then the 3D structure can be reconstructed up to an unknown scale factor, which can be found by detecting an object of known size."><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><title>Structure From Motion: Camera | Ricardo Chin</title><link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png><link rel=stylesheet href=/book.min.6e4d08d120eef807bed2ef1c891443c8cac81abb054e9afbe962c4da325c5c2b.css integrity="sha256-bk0I0SDu+Ae+0u8ciRRDyMrIGrsFTpr76WLE2jJcXCs=" crossorigin=anonymous><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script><link rel=alternate type=application/rss+xml href=https://xsleaks.dev/docs/code/computervision/index.xml title="Ricardo Chin"></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>Ricardo Chin</span></a></h2><ul><li class=book-section-flat><a href=/docs/design/>Engineering Repository</a><ul><li><a href=/docs/design/fea-beam-simulation/>FEA Beam Instability Modes</a><ul></ul></li><li><a href=/docs/design/finite-element-method-development/>FEM Package Development</a><ul></ul></li></ul></li><li class=book-section-flat><a href=/docs/code/>System Repository</a><ul><li><a href=/docs/code/agv/>AGV: Stochastic Identification</a><ul></ul></li><li><a href=/docs/code/uav/>UAV: Red Bull Air Racing</a><ul><li><a href=/docs/code/uav/dynamics/>Drone System Dynamics</a></li><li><a href=/docs/code/uav/continuous-controller-design/>Continuous Controller</a></li><li><a href=/docs/code/uav/discrete-controller-design/>Discrete Controller</a></li><li><a href=/docs/code/uav/computer-vision/>Gate Sense: Computer Vision</a></li></ul></li><li><a href=/docs/code/robotics/>Robotics: Kin, Dynamics & Control</a><ul></ul></li><li><a href=/docs/code/deep-learning-fake-news/>Fake News: Inference & Clusters</a><ul></ul></li><li><a href=/docs/code/bin-packing/>EC Optimization: Space & Time</a><ul><li><a href=/docs/code/bin-packing/genetic-algorithm/>Genetic Algorithm</a></li><li><a href=/docs/code/bin-packing/particle-swarm-optimization/>Particle Swarm Optimization</a></li></ul></li><li><a href=/docs/code/snake-game/>Snake: Reinforcement Learning</a><ul><li><a href=/docs/code/snake-game/deepqnetwork/>BaseRL, GA-RL & Hamiltonian</a></li><li><a href=/docs/code/snake-game/adversarial/>Adversarial MultiAgent-RL, BattleMode</a></li></ul></li></ul></li><li class=book-section-flat><a href=/docs/lectures/>My Notes and Lectures</a><ul><li><a href=/docs/lectures/bayesian-machine-learning/bayes/>Bayesian Inference</a></li><li><a href=/docs/lectures/bayesian-optimization/bayesopt/>Bayesian Optimization</a></li><li><a href=/docs/lectures/nonlinear-pro/nonlinear/>Nonlinear Programming</a></li><li><a href=/docs/lectures/hamiltonian-graphs/hamiltonian/>Hamiltonian Graphs & Linked Lists</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Structure From Motion: Camera</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#1-abstract>1 Abstract</a></li><li><a href=#2-theoretical-framework>2 Theoretical Framework</a><ul><li><a href=#21-pinhole-camera-model>2.1. Pinhole Camera Model</a></li><li><a href=#22-epipolar-geometry>2.2. Epipolar Geometry</a></li><li><a href=#23-essential-matrix>2.3. Essential Matrix</a></li><li><a href=#24-structure-from-motion-3d-reconstruction>2.4. Structure From Motion: 3D Reconstruction</a></li></ul></li><li><a href=#3-experimentation>3 Experimentation</a><ul><li><a href=#31-first-camera>3.1. First Camera</a></li><li><a href=#32-second-camera>3.2. Second Camera</a></li></ul></li></ul></nav></aside></header><article class=markdown><h2 id=1-abstract>1 Abstract
<a class=anchor href=#1-abstract>#</a></h2><p>This research concerns the structure from motion from two views, which is the process of estimating the 3D structure of a scene from a set of 2D views. This requires proper camera calibration to ensure that the 2D images can be used for metric information extraction. Afterwards, the poses of the calibrated camera can be estimated from two images, then the 3D structure can be reconstructed up to an unknown scale factor, which can be found by detecting an object of known size.</p><blockquote class="book-hint2 info"><p class="hint-title info"><svg class="book-icon"><use href="/svg/hint-icons.svg#info-notice"/></svg><span>info</span></p>Geometric camera calibration, also known as camera intrinsic calibration or multi-camera calibration, is the process of estimating the parameters required to relate (2D) points in a camera’s image plane with (3D) points in the world scene the camera is viewing. With a calibrated camera, each point on a sensor translates into a ray emanating into the scene, providing directional information about points in a scene.</blockquote><p>With a pair of calibrated cameras (typically called a stereo pair), it is possible to triangulate distances and make estimations of the distance to points in the world. Determining geometry from multiple views (either from multiple cameras or multiple frames from one) is at the heart of classical computer vision.</p><p>Generally, camera calibration means the process of computing a camera&rsquo;s physical parameters, like image center, focal length, position and orientation, etc. This is called explicit calibration. Some intermediate parameters can also be calibrated for either making 3D measurements (back-projection) or prediction image coordinates from known world coordinates (projection). This type of calibration is called implicit calibration. A camera&rsquo;s intrinsic parameters do not change if you move the camera, that is what separates the intrinsic parameters from the extrinsic ones. Despite this the intrinsic parameters may change if the lens is adjusted.</p><blockquote class="book-hint2 important"><p class="hint-title important"><svg class="book-icon"><use href="/svg/hint-icons.svg#important-notice"/></svg><span>important</span></p>A camera&rsquo;s intrinsic parameters do not change if you move the camera, that is what separates the intrinsic parameters from the extrinsic ones. Despite this the intrinsic parameters may change if the lens is adjusted. There may be small influences on the intrinsic parameters from moving the camera (as the camera is not perfectly rigid) or from changing surroundings (e.g. temperature), but they are small enough to be disregarded for most use cases.</blockquote><h2 id=2-theoretical-framework>2 Theoretical Framework
<a class=anchor href=#2-theoretical-framework>#</a></h2><h3 id=21-pinhole-camera-model>2.1. Pinhole Camera Model
<a class=anchor href=#21-pinhole-camera-model>#</a></h3><p>Two views can be obtained simultaneously as in a stereo rig, or sequentially, by moving the camera relative to the scene. Both situations are geometrically equivalent, but if two cameras were used simultaneously they would obviously need to be calibrated separately. A camera matrix is a 3x4 matrix which describes the mapping of a pinhole camera from 3D points in the world to 2D points in an image. The main premise of the pinhole camera model is that the camera aperture is described as a single point. Hence, it is based on the principle of co-linearity, where each point in the object space is projected by a straight line through the projection centerinto the image plane.</p><p><img src=https://csundergrad.science.uoit.ca/courses/cv-notes/notebooks/images/camera-geometry.png alt=21312></p><h3 id=22-epipolar-geometry>2.2. Epipolar Geometry
<a class=anchor href=#22-epipolar-geometry>#</a></h3><h3 id=23-essential-matrix>2.3. Essential Matrix
<a class=anchor href=#23-essential-matrix>#</a></h3><h3 id=24-structure-from-motion-3d-reconstruction>2.4. Structure From Motion: 3D Reconstruction
<a class=anchor href=#24-structure-from-motion-3d-reconstruction>#</a></h3><h2 id=3-experimentation>3 Experimentation
<a class=anchor href=#3-experimentation>#</a></h2><p>Multiple cameras were used for further result discussion, but only the cameras that provided best results were closely studied. It is also important to note that despite having applied similar procedures to analyse results, both experiments differ in many ways (ie: camera, objects’ geometries, distance at which objects were captured).</p><h3 id=31-first-camera>3.1. First Camera
<a class=anchor href=#31-first-camera>#</a></h3><p>In order to calibrate the camera the Camera Calibrator app from MATLAB Computer Vision System Toolbox was used. To ensure that the focal length didn’t affect the intrinsic parameters, an app without automatic focus was installed on the phones that took the pictures.</p><p><img src=https://live.staticflickr.com/65535/53852030294_fdc7abc114_w.jpg alt=123901></p><p>The photos were then calibrated using the Camera Calibrator app from MATLAB. Results for calibration were:</p><p><img src=https://live.staticflickr.com/65535/53850775922_4d87ff997b_z.jpg alt=123123></p><p>For this calibration, a mean error of 0.33 pixels was obtained, indicating a good calibration. After the calibration, the CameraParameters variable is saved to a data file.</p><p><img src=https://live.staticflickr.com/65535/53852030259_0b3f5e2c19.jpg alt=1231></p><p>On this project camera motion was recovered and the 3-D structure of a scene was reconstructed from two images taken with a calibrated camera. With the camera parameters obtained, two photos of the same set of objects were taken with one camera, at different positions. These photos were used to construct a &ldquo;structure from motion from two views&rdquo;.</p><p><img src=https://live.staticflickr.com/65535/53852104940_065a09d8c7_z.jpg alt=401349></p><p>The two images are then compared, in order to find point correspondences between them. In order to detect the features to track, detecMinEigenFeatures is used. This function finds corners using the minimum eigenvalue algorithm. The ’MinQuality’ parameter specifies the minimum accepted quality of corners with a scalar value in the range [0,1]. If this parameter is reduced, more points will be detected and they’ll be more uniformly distributed throughout the image. Since the motion of the camera is not very large, the Kanda-Luscas-Tomasi (KLT) feature tracker is suited to establish point correspondences. The detected points (150 strongest corners) are displayed:</p><p><img src=https://live.staticflickr.com/65535/53851927528_5c0b16ebb4_w.jpg alt=2134></p><p>The features are tracked on image 2 using vision.PointTracker function. The correspondences can be visualized as follows:</p><p><img src=https://live.staticflickr.com/65535/53852104935_437eb5aedf_w.jpg alt=1300></p><p>The essential matrix can then be computed with the estimateEssentialMatrix function. The function also allows to find the inlier points that meet the epipolar constraint. The epipolar inliers are shown as:</p><p><img src=https://live.staticflickr.com/65535/53852032714_5b49837734_w.jpg alt=239193></p><p>After this step, the location and orientation of the second camera relative to the first one are computed with the relativeCameraPose. To reconstruct the 3-D locations of matched points, a re-detection on the first image is performed using a lower ’MinQuality’ to get more points. This points are once again tracked into the second image. Estimation of the 3-D locations corresponding to the matched points is performed with the triangulate function, which implements the Direct Linear Transformation (DLT) algorithm. To visualize the location and orientations of the camera, plotCamera function is used. And to visualize the 3-D point cloud, pcshow is used.</p><p>One extra step that can be performed is to fit a sphere to the point cloud. This step is useful if the image contains an object with the shape of a sphere. For our set of images, a ball with know diameter was used for testing. To locate the sphere, MATLAB function pcfitsphere is used.</p><p><img src=https://live.staticflickr.com/65535/53850775872_338b864c80_c.jpg alt=139933></p><p>The figure shows that the program was able to successfully locate an object that resembles a sphere, even though the ball isn’t fully defined in the 3-D point cloud. Adding the lateral view of the cloud we can see the objects from the original images have depth in the structure.</p><p><img src=https://live.staticflickr.com/65535/53851669366_ae32349d6f_c.jpg alt=121></p><p>Since the radius of the ball is known, it can be used to determine the scale factor of the reconstructed image. With this step, the coordinates of the 3-D points in centimeters can be obtained. This means, a metric reconstruction of the scene is achieved. Final point cloud can be seen as follows:</p><p><img src=https://live.staticflickr.com/65535/53851927518_3cbd04f222_w.jpg alt=210319></p><p><img src=https://live.staticflickr.com/65535/53852030204_0d928da300_z.jpg alt=21049124></p><p>Thus, camera motion was recovered and the 3-D structure of a scene was reconstructed from two images taken with a calibrated camera. The program was able to successfully extract features from the objects and add depth to them, while showing the estimated position of the cameras from where the photos were taken. Some features of the ball were more easily extracted than others. The ball is only partially found on the
point cloud because it is too close to the edge of the frame which distorts.</p><p>In MATLAB, one aspect of the program is that it doesn’t give the same exact results 100% of the time. For this test, the program works almost every time, but sometimes the zoom in the last two figures isn’t correct.</p><h3 id=32-second-camera>3.2. Second Camera
<a class=anchor href=#32-second-camera>#</a></h3></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/roaked/roaked.github.io/commit/4aca1b7a7ce8cb937b4bb2b9220c8a23f3b1cd47 title='Last modified by roaked | February 26, 2026' target=_blank rel=noopener><img src=/svg/calendar.svg class=book-icon alt=Calendar>
<span>February 26, 2026</span></a></div><div><a class="flex align-center" href=https://github.com/roaked/roaked.github.io/edit/main/content/content/docs/code/computervision/_index.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Ricardo Chin</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#1-abstract>1 Abstract</a></li><li><a href=#2-theoretical-framework>2 Theoretical Framework</a><ul><li><a href=#21-pinhole-camera-model>2.1. Pinhole Camera Model</a></li><li><a href=#22-epipolar-geometry>2.2. Epipolar Geometry</a></li><li><a href=#23-essential-matrix>2.3. Essential Matrix</a></li><li><a href=#24-structure-from-motion-3d-reconstruction>2.4. Structure From Motion: 3D Reconstruction</a></li></ul></li><li><a href=#3-experimentation>3 Experimentation</a><ul><li><a href=#31-first-camera>3.1. First Camera</a></li><li><a href=#32-second-camera>3.2. Second Camera</a></li></ul></li></ul></nav></div></aside></main></body></html>